"Good morning, good afternoon, good evening and good night!",NULL,"Yes, my blog has a new home, and by some weird twist of fate, you've stumbled upon it.  It could also be that you followed the link from my old blog.
<!--break-->
A little ironically, this is actually more of a homecoming for my blog; see, for now, it's hosted by myself on my Linux box, which is where my Soapbox first started back in the summer of 2005.  I lost that in a hard disk crash (probably; I can't quite remember, and they seem to happen to me quite a bit.  I'm unlucky that way.) and from the flames, the phoenix of the appropriately-titled Soapbox 2.0 arose, generously hosted by the University of Warwick.  Falling in and out of use at various points over a period of about two years, I finally gave up on it in January this year (well, that's when the last post was, at least), wanting to do something more with it than I was able to on the Warwick Blog system.  

What is that I want to do that I couldn't do previously?  Well, I have a large number of interests, such as music, coding (which also happens to be my job), and films to name but a few, and I wanted to be able to blog about these in a more separate manner than I was able previously; to be able to maintain separate blogs for each, almost.  I also wanted to put together a personal website, something more than just a blog.  We'll see how well that turns out.  

I'm also hoping that this will serve to improve my skills as a developer.  In his recent <a href=""http://www.codinghorror.com/"">Coding Horror</a> post on <a href=""http://www.codinghorror.com/blog/archives/001138.html"">Code Kata</a>, Jeff Atwood lists as one of his own two Kata steps maintaining a blog, so you, dear reader, are part of a grand experiment!  To this end, I expect the coding section to expand the most rapidly, and so I apologise to all my non-technical readers who may be turned off by this.  I'll do my best to keep the other sections updated frequently too."
Message Passing in a Plug-in Framework,NULL,"I've been running a couple of coding projects at home over the last couple of months, one of which is a personal finance program.  I've used this as an experiment in new technologies (mainly .Net 3.0/3.5 stuff, including LINQ and WPF/XAML), as well as one in architecture and design patterns.  The program is based around a plug-in architecture and the time has come to consider message-passing between plug-ins; for example, I need non-storage plug-ins to inform storage plug-ins when their data has changed, and what data has changed, so that the storage plug-ins can schedule these changes for serialisation to their storage format (be it an SQLite database, an XML file, or whatever).
<!--break-->
The plug-in framework enables the loading of different classes of plug-ins, each with their own interface; for example, there is a class of plug-in for accounts, one for reports, two for file type support (import and export), etc.  The framework provides the environment for loading and interacting with plug-ins, the necessary interfaces etc. for coding plug-ins against, managing of preferences and logging, etc., etc., etc., ad nauseam.  It is a mediator and arbitrator between the plug-ins which provide the real meat of the application.  

The project has now got to the point where I need to be able to pass messages around the framework, between plug-ins.  The immediate solution that sprung to mind was the classic publish/subscribe model, so I duly looked into the <a href=""http://www.wikipedia.org/wiki/Observer_Pattern"" title=""Wikipedia article on the Observer design pattern"">Observer pattern</a>, a manual implementation of publish/subscribe.  This feels wrong in that it doesn't seem to provide a way for plug-ins to register new notification types (e.g., BeforeDataChange =&gt; ""if you need to do something before you update your data, now is your chance"") under defined categories of events, let alone define new categories.  Solution #1 scrapped.  

I thought I'd cracked it with my next possible solution, the Blackboard pattern, which is based around the idea of cops solving a crime; the policemen post different bits of information and evidence on a central blackboard and infer further information in the form of links and relationships between the items.  This approach has historically been used to great effect in AI systems, such as OCR, speech recognition, etc., but didn't feel quite right for this scenario.  However, I didn't like the idea of posting up all these changes for the entire application to see, when the message was intended for a particular set of recipients.  Solution #2 scrapped.  

So I revisited publish/subscribe, this time with .Net events.  A .Net event is just a special case of the .Net idea of a delegate, or callback, method; it is an analogue to a function pointer for those of you in the C++ world, although it has to be said that this metaphor quickly breaks down as delegates also have an object representation that makes the more than just pointers.  They give you multi-casting for free, as callback methods are assigned to delegates using the ""+="" operator, building a list of methods invoked by calling the delegate.  I thought maybe I could leverage the object-ness of delegates to implement categories via inheritance, but it turned out this wasn't possible (probably A Good Thing™).

Events don't seem to play nicely with my proposed model of using a broker class to mediate the publications and subscriptions.  The broker should store a list of published actions, and plug-ins should be able to retrieve the list of published actions in order to subscribe to a subset of them.  I may need to go down the path of a custom solution (such as porting the Message Manager from my fourth-year project), which is not very <abbr title=""Don't Repeat Yourself"">DRY</abbr>, but should give me a greater level of decoupling in the architecture than I think an events-based model will give me."
Lesson #2,NULL,"I learned my second lesson from this blog today.  What was the first, you might ask?  <a href=""http://www.alastairsmith.me.uk/random-stuff/2008/06/24/good-morning-good-afternoon-good-evening-and-good-night.html#comment-5"" title=""&quot;Good morning, good afternoon, good evening and good night&quot;, comment no. 4"">The importance of peer code reviews</a>, and I learnt this pretty much as soon as I'd started.  

The second lesson was to improve my writing style!  After a comment or two on <a href=""http://www.alastairsmith.me.uk/coding/2008/06/25/message-passing-a-plug-framework.html"" title=""Message Passing in a Plug-in Framework"">my last post</a> (in conversation rather than via the blog itself), it appears that I didn't explain the situation particularly clearly.  I will try not not be so arrogant as to assume that my readers know what I'm talking about all the time!
<!--break-->
This is actually an interesting discussion topic.  I've found at work, and in team projects at University, that technical communication is easily hampered by not clearing the path and providing sufficient context to the discussion.  Particularly with intra-team communications, it's very easy to assume that everyone is familiar with the topic about which you are conversing.  In <i>The Pragmatic Programmer: From Journeyman to Master</i>, Andy Hunt and David Thomas define the following ""acrostic"" as a useful framework for pitching your conversations:
<pre>
                    <span class=""key"">W</span>hat do you want them to learn?
               What <span class=""key"">I</span>s their interest in what you have to say?
                How <span class=""key"">S</span>ophisticated are they?
           How much <span class=""key"">D</span>etail do they want?
Whom do you want to <span class=""key"">O</span>wn the information?
        How can you <span class=""key"">M</span>otivate them to listen to you?
</pre>

There are &mdash; count them &mdash; only six questions here to ask yourself, and they take only a couple of moments to run through mentally whilst preparing for some form of communication.  With a bit of practice, it can even be done in conversation (and should certainly be done before starting a conversation).  The steps, like much of the advice Hunt and Thomas impart, are simple, logical and sensible: identify your objective, identify your audience and their level of knowledge, and identify the key stakeholders.  

I'll post a full review of <i>The Pragmatic Programmer</i> when I've finished reading it, hopefully in the next couple of weeks or so, but I can say now that this is one book that every fresh graduate programmer should read.  I'll also form a recommended reading list, but for now it looks suspiciously like <a href=""http://www.codinghorror.com/blog/archives/000020.html"" title=""Coding Horror: Recommended Reading for Developers"">Jeff Atwood's</a>."
Chicago,NULL,"[4/5]
2002's <i>Chicago</i>, directed by Bob Marshall, is an excellent and faithful adaptation of Kander and Ebb's musical, and seemingly kick-started a small, slow revival in musical cinema (such as 2005's <i>The Producers</i> and 2007's <i>Sweeney Tood</i>).

Set in 1920's Chicago, the plot follows the lives of Velma Kelly (Catherine Zeta-Jones) and Roxie Hart (Renée Zellweger) after they are charged with murder, Velma for a double homicide after finding her husband in bed with her sister (with whom she performed in a Vaudeville act) and Roxie for the murder of her lover (after she discovers that he wasn't going to make her the Vaudeville star he'd promised).
<!--break-->
As Roxie arrives on Murder Row in Cook County, Illinois, Matron ""Mama"" Morton (Queen Latifah) is helping Velma re-build her career in advance of her acquittal thanks to lawyer Billy Flynn (Richard Gere).  Roxie, a wanna-be Vaudeville star, seizes her chance, and Velma's lawyer, and starts to make a name for herself as the latest celebrity murderess.  It is on this journey that Roxie's true colours really shine, as we follow her into a deeper web of lies, corruption and greed until we reach the conclusion that crime really does pay.  

The murders, therefore, are rather incidental to the rest of the plot.  <i>Chicago</i> is a satire on the concept of the celebrity criminal and corruption in Chicago in the 1920s, and works brilliantly as such.  I'm yet to see the show on stage, but Marshall has transported it well across media, with more than a few numbers feeling as though they are on Broadway rather than the big screen; however, the vastly larger budget has been well spent on authentic costumes, sets and even computer graphics to really bring the characters and prohibition-era Chicago to life.  The media of film also allows Marshall to carry off some of the numbers perhaps more effectively than is possible on stage: for example, Mama's entry (<i>When You're Good to Mama</i>) neatly cuts between a Cabaret-style stage show and the prison where she is welcoming her new charges.  

As with <i>Sweeney Todd</i>, a number of cuts had to be made from the original score, and, seemingly, some new music was added too.  The story unfolds in a manner that is faithful to the original plot with no re-telling, but it is paraphrased here and there, so fans of the musical may be mildly disappointed by this.  The singing, too, is of a good standard: convincing, but not Broadway.  

I do have one minor criticism of the film, and it's one that will never really be addressed, due to the logistics of shooting such a picture.  Whilst the cast do all their own singing, I was disappointed to see that the scenes were mimed with the soundtrack over-dubbed.  I should really have come to expect this as it's rather become accepted technique given the complexities of recording an <i>n</i>-piece jazz orchestra and singer(s) as well as all the visual recording too.  All in all, it doesn't detract from the quality of the film in any meaningful sense unless you're an ultra-purist, and the technique does have the added advantage of providing a uniform sound/mix throughout the film such as you would get seeing this show at the West End or on Broadway.  

In summary, then, this is an excellent adaptation, with some good performances and innovative direction.  Purists may be a little disappointed, but it is an enjoyable picture with fabulous sets and costumes, not to mention a toe-tapping soundtrack and a dry wit."
PTP/IP,NULL,"[img_assist|nid=8|link=node|align=left|width=190|height=200]...or Pigeon Transport Protocol/Internet Protocol.  <a href=""http://www.faqs.org/rfcs/rfc1149.html"" title=""RFC 1149: A Standard for the Transmission of IP Datagrams on Avian Carriers""><abbr title=""Request for Comments"">RFC</abbr> 1149</a> defines a standard for the transmission of IP datagrams on avian carriers.  That is, computer networks run by carrier pigeon.

From the overview,

<blockquote>Avian carriers can provide high delay, low throughput, and low altitude service. &hellip; The carriers have an intrinsic collision avoidance system, which increases availability.  </blockquote>
<!--break-->
The <abbr title=""Request for Comments"">RFC</abbr> goes on to describe the frame format, which consists of hexadecimal printed in octets, ""separated by whitestuff and blackstuff"", on a small scroll of paper which is then wrapped around one leg of the avian carrier.  Unfortunately, the bandwidth of the protocol is limited by the length of the carrier's leg, but as IP is best-effort only the loss of a carrier is tolerable.  

Whilst security is not generally a problem (with the exception of military implementations), the respected security expert Bruce Schneier appears to have <a href=""http://www.schneier.com/blog/archives/2008/06/carrier_pigeons.html"" title=""Schneier on Security: Carrier Pigeons Bringing Contraband into Prisons"">found the first vulnerability in the protocol</a>.

Many thanks to David Waitzman for this bit of silliness, who has even gone so far as to amend the original <abbr title=""Request for Comments"">RFC</abbr> to include <a href=""http://www.faqs.org/rfcs/rfc2549.html"" title=""IP over Avian Carriers with Quality of Service""><abbr title=""Quality of Service"">QoS</abbr></a>.  And yes, RFC 1149 was an April Fool."
Comments Problem,NULL,"So it seems there's currently a problem with the comments' system.  First of all, <a href=""http://en.wikipedia.org/wiki/DON%27T_PANIC!"" title=""Wikipedia Article on The Hitchhiker's Guide to the Galaxy"">DON'T PANIC!</a>  Your awesome comments are being posted correctly, and are making it through to the moderation queue fine (or being posted if you've got an account).
<!--break-->
The bad news is the Akismet anti-spam module I've installed is in beta for Drupal 6 at the moment, and an error in the module is causing the screen to go blank when you hit ""Save"" to publish your awesome comment.  

If you want the technical detail, a function that this module called has been removed from the Drupal API in Drupal 6.  There's more information available on the <a href=""http://drupal.org/node/240894"" title=""Call to undefined function drupal_submit_form in Drupal 6 when user submits a comment"">bug report</a> over at <a href=""http://drupal.org"">Drupal.org</a>."
Multiplicity in Computing Environments,NULL,"Multiplicity.  The Merriam-Webster Online Dictionary defines multiplicity thus:

<blockquote cite=""http://www.merriam-webster.com/dictionary/multiplicity"">
<p>Multiplicity, noun</p>
<ol>
  <li><b>a</b>: the quality or state of being multiple or various <b>b</b>: the number of components in a system (as a multiplet or a group of energy levels)</li>
  <li>a great number</li>
</ol></blockquote>

Multiplicity is not something new to computer science or computing environments; indeed, parallel computing (processing with multiple, simultaneously-executing processes) <a href=""http://en.wikipedia.org/wiki/Parallel_computing#History"" title=""Wikipedia article on Parallel Computing"">seems to date back to Babbage's Analytic Engine</a>.  And every time it's introduced, it benefits the end user in some way.
<!--break-->
Some classes of user, such as financial traders, have long had multiple displays attached to a single computer to allow them to view multiple pieces of information at the same time. I have two displays attached to my computer at work for this very reason: I can have a piece of research or the current build of the website open in a web browser on one screen and the code on which I'm working on the other.  I'd like a similar set-up at home, when I can afford it.  Ideally, I'd have three monitors in both situations; not because I'm greedy, but because this model more neatly matches the mental model of a ""grid"" of displays.  A 3x1 grid of monitors provides much more flexibility than a 2x1 grid.  As Jeff Atwood puts it in his Coding Horror blog post, <a href=""http://www.codinghorror.com/blog/archives/000740.html"" title=""Joining the Prestigious Three Monitor Club"">Joining the Prestigious Three Monitor Club</a>:
<blockquote cite=""http://www.codinghorror.com/blog/archives/000740.html"">
As good as two monitors is, three monitors is even better. With three monitors, there's a ""center"" to focus on. And 50% more display area. While there's certainly a point of diminishing returns for additional monitors, I think three is the sweet spot&hellip;I don't care how large a single display can be; you can <em>never</em> have enough desktop space. 
</blockquote>

Multiple-cored processors have broken into the mainstream in the last few years.  Previously only limited to servers requiring high availability or processing power, or high-performance computing applications, it's now easier to pick up a dual-core computer than it is a single-core computer.  Quad-core (and tri-core too) processors are starting to get a decent foothold, and <a href=""http://www.reghardware.co.uk/2008/06/03/intel_nehalem_roadmap_update/"" title=""Intel lengthens first-gen 'Nehalem' CPU shelf life?"">in about 18 months we should be seeing six-core chips from Intel</a>.  Multi-core processors are not the biggest of innovations ever, but they have <a href=""http://www.hanselman.com/blog/FasterBuildsWithMSBuildUsingParallelBuildsAndMulticoreCPUs.aspx"" title=""Faster Builds with MSBuild using Parallel Builds and Multicore CPUs"">vastly improved the user's capacity for multi-tasking</a>.  Well-written software that properly takes advantage of the many-core architectures now prevalent in the marketplace is starting to come into being, such as Adobe's Creative Suite product (I believe this has had decent multi-core support since CS3) and Microsoft's MSBuild (MSBuild 3.5 processes your command-line builds in parallel where it can, providing a fair bit of speed-up unless your solution's projects form a linear dependency).

Tabbed browsing, first introduced into Opera, adopted more widely in Firefox 1.0 and then, eventually, Internet Explorer 7, brought multiplicity to the web browser.  I started using Firefox in 2003/2004 when it was still pre-release and called ""Firebird"", and I've never gone back to Internet Explorer, partly for the enhanced security and wealth of features Firefox provides over Internet Explorer, but the killer feature that stopped me straying was the tabbed browsing.  I can have multiple web pages open without filling up my task bar with multiple windows!

Early versions of Office, amongst a herd of other applications, utilised the <abbr title=""Multiple Document Interface"">MDI</abbr> model, which hosted multiple documents in one application window.  I believe usability studies found that most end users expected to be able to switch between documents using the more standard Alt+Tab key combination, so <abbr title=""Multiple Document Interface"">MDI</abbr> model was replaced with the <abbr title=""Single Document Interface"">SDI</abbr> model in Office 2000, but I firmly believe that had there been a quick and easy-to-remember shortcut for switching between documents, we would still be using the <abbr>MDI</abbr> model today.  

This week I purchased a copy of <a href=""http://www.cubedesktop.com/"" title=""CubeDesktop - 3D realtime virtual desktop manager and task switcher"">Cube Desktop</a> for use at work.  This natty piece of software allows me to run multiple desktop environments within Windows; it creates up to six desktops (one of each face of the cube), does some funky semi-customisable <a href=""http://www.cubedesktop.com/images/screenshots/1.jpg"">3</a>-<a href=""http://www.cubedesktop.com/images/screenshots/2.jpg"">d</a> <a href=""http://www.cubedesktop.com/images/screenshots/3.jpg"">transitions</a> between the desktops, and provides an <a href=""http://www.cubedesktop.com/images/screenshots/5.jpg"">overview</a> of all your running applications, on all desktops.  

Mac and Linux users can feel free to snigger at this point: Windows Vista is a modern operating system (whatever the views of its critics and detractors) and it <em>still</em> doesn't have support for multiple workspaces.  Linux has had this feature <strong>forever</strong> (albeit without the funky 3-d transitions and application overview, although the Beryl project resolves this), and Apple has had the overview in the form of Exposé since Panther (Mac OS X 10.3), and the multiple desktops were introduced under the moniker Spaces into Leopard (Mac OS X 10.5).  What I really love about Exposé, and I haven't tested this on CubeDesktop yet, is the ability to interact (albeit to a limited extent) with the applications in preview mode.  For example, I can drag-and-drop a file from one folder to another whilst Exposé is running.  

The Pro edition of CubeDesktop is less than €20 (~£16), and it's really very good.  I now have separate workspaces to organise my daily activities at work; this tiny little change has vastly improved my productivity through the simple separation of concerns.  I'd like one desktop for admin (checking emails, writing my status report, etc.), please.  Check.  I'd like another one for development (running Perforce, Visual Studio, Firefox/IE, PowerShell).  Sure thing.  Maybe a separate one for running my debug environments.  No problem; have four!

This made me think about what other multiplicity I'd like to see, and I hit on the idea of multiple clipboards; in fact, Microsoft Office has supported this since Office 2000, and for me it was one of the best new features of that release.  Presumably, though, it broke the user's mental model of the clipboard as it appears to have been quietly hidden in Office 2007: to enable it, you now have to click that tiny little pop-out icon on the Clipboard area of the Home tab of the ribbon.  There's a mouthful for you; let's try a screenshot:

[img_assist|nid=10|title=|desc=|link=none|align=none|width=494|height=640]

Subtle, huh?  This is application specific, though (although I believe it's supported across Office applications); what I'd like is for something like this to hijack the global clipboard and be available for all applications.  It would need a small, unobtrusive window to display the items on the clipboard, and the clipboard could hold either a configurable maximum number of items, or as much information as the computer's memory would allow.  A handy short-cut key combination &mdash; [Flag] + V, perhaps &mdash; would display this window and give it focus so that I could select items with the arrow keys on the keyboard, or using the number keys as an index to the list of items.  The window should display a small preview of the items on the clipboard (e.g., a thumbnail of an image, or the first few words of a bit of text), with a larger preview displayed to the side of the item when it is selected.  The clipboard window would be closed by selecting an item (and thereby pasting it into the application you're using), or by hitting Escape to avoid inserting an item.  

I think the trickiest bit of this implementation would be keeping a handle on the previously-focused application window.  That being said, I might have found project number 3...

I'd be interested to hear what people would like to see multiple things of in their computing environment that they don't already have."
Curriculum Vitae,NULL,
Bassoon Curriculum Vitae,,"I have been playing bassoon since 1997.  Within a year I had obtained Grade 4 with distinction, and I achieved Grade 8 in 2001.  I was awarded a DipABRSM in Performance in the summer of 2009.  I am currently studying with Tom Hardy in London.  

My sight-reading is of a high standard, and when required I am able to play performances at sight.  For example, I have performed Debussy's <em>Pr&eacute;lude &agrave; l'Apr&egrave;s-Midi d'un Faune</em> and Elgar's 'cello concerto at sight, and Mozart's clarinet concerto and Stravinsky's <em>The Rite of Spring</em> on a single rehearsal.  

I have performed in both orchestras and operatic productions.  My orchestral experience encompasses the following ensembles, in which I have played first, second and contrabassoon (although please note that I do not own a contrabassoon).  With the exception of the Cambridge Graduate Orchestra, I held the position of principal bassoonist in each orchestra for at least three years.  
<dl>
  <dt>2012-date</dt> <dd><a href=""http://www.huntsphil.org.uk/"" title=""Huntingdonshire Philharmonic"">Huntingdonshire Philharmonic Orchestra</a> (not contrabassoon)</dd>
  <dt>2010-date</dt> <dd><a href=""http://homepage.ntlworld.com/shug0131/"" title=""Cambridge Symphonic Winds"">Cambridge Symphonic Winds</a></dd>
  <dt>2009-date</dt> <dd><a href=""http://www.cambridgegraduateorchestra.com/"" title=""Cambridge Graduate Orchestra (CGO)"">Cambridge Graduate Orchestra</a> (not contrabassoon)</dd>
  <dt>2003-2007</dt> <dd><a href=""http://www.uwso.co.uk/"" title=""University of Warwick Symphony Orchestra (UWSO)"">University of Warwick Symphony Orchestra (UWSO)</a></dd>
  <dt>2001-2004</dt> <dd><a href=""http://www.popsincorporated.co.uk/"" title=""Pops Incorporated UK (formerly Southampton Pops Orchestra)"">Southampton Pops Orchestra</dd>
  <dt>2000-2003</dt> <dd><a href=""http://www.waso.hampshire.org.uk/orchestra.html"">Winchester Youth Orchestra</a></dd>
  <dt>1999-2003</dt> <dd><a href=""http://www.southamptonmusic.org.uk/ensembles/soton_youth_orc.asp"" title=""Southampton Youth Orchestra"">Southampton Youth Orchestra</a></dd>
</dl>

I am generally looking for freelance-type performance opportunities on a reasonable expenses-contribution basis, but would gladly take on longer-term tenures with an ensemble or production.  I am usually able to dep at relatively short notice, and am a competent and reliable deputy.  I can travel easily within the South Cambridgeshire area; travel over greater distances would normally be subject to negotiation of expenses.  

Please send me an email via the <a href=""/contact"" title=""Contact Form"">contact form</a> if you are short of a bassoonist for your concert or production.  You can also check out my profiles on <a href=""http://www.freelance-musicians.info/musicians/231/?"" title=""Freelance Musicians"">freelance-musicians.info</a> and <a href=""http://www.musiconnections.co.uk/enhanced_musician.php?id=146"" title=""Musiconnections"">Musiconnections</a>.  I'm also contactable through the <a href=""http://www.classicalmusichomepage.com/diaryservice"" title=""CMH Diary Service"">Classical Music Homepage's Diary Service</a>.
<!--break-->
My experience of the orchestral repertoire is broad.  Parts I have played include:
<ul>
  <li>First bassoon in Beethoven's <em>Symphony No. 9</em>; second bassoon in <em>Symphony No. 3</em> and <em>Symphony No. 7</em></li>
  <li>First bassoon in the World Premiere of Jonathan Dove's <em>Airport Scenes</em></li>
  <li>First and second bassoon in each of Tchaikovsky's <em>Symphonies No. 5</em> and <em>6</em>, and <em>Romeo and Juliet</em></li>
  <li>First bassoon in Shostakovich's <em>Jazz Suite No. 2</em>; Holst's <em>The Planets</em>; Vaughn Williams' <em>Hodie</em> and <em>Symphony No. 5</em>; Gershwin's <em>An American in Paris</em></li>
  <li>First bassoon in various film/jazz/TV/pop music including much of John Williams' repertoire, music by Queen, The Beatles, Leroy Anderson and others</li>
  <li>First bassoon in Sibelius' <em>Symphony No. 5</em>; second bassoon in <em>Symphony No. 2</em></li>
  <li>Second bassoon in Stravinsky's <em>Symphony of Winds</em> and <em>The Rite of Spring</em>; Verdi's <em>Requiem</em></li>
  <li>Second bassoon in Strauss' <em>Overture to Die Fledermaus</em>; Glinka's <em>Overture to Ruslan and Ludmilla</em></li>
  <li>Third bassoon in Nielsen's <em>Symphony No. 4 ""The Inextinguishable""</em></li>
  <li>Contrabassoon in Mahler's <em>Symphonies No. 1</em> and <em>2</em>; Shostakovich's <em>Symphony No. 5</em> and <em>Festival Overture</em>; Bernstein's <em>Symphonic Dances</em> from <em>West Side Story</em></li>
</ul>

I have also performed in the following operatic productions:
<dl>
  <dt><a href=""http://www.societies.cam.ac.uk/cube/"" title=""Cambridge University Baroque Ensemble (CUBE)"">Cambridge University Baroque Ensemble (CUBE)</a></dt><dd>Jun. 2008: <em>Orfeo e Euridice</em> (Gluck)</dd>
  <dt>Unaffiliated Productions</dt>
<dd>Mar. 2008: <a href=""https://www.srcf.ucam.org/davidandgoliath/biogs.html"" title=""David and Goliath""><em>David and Goliath</em></a> (Samuel Hogarth)</dd>
  <dt><a href=""http://www.warwickstudentopera.co.uk/"" title=""Warwick Student Opera (WSO"")>Warwick Student Opera (WSO)</a></dt><dd>Feb. 2007: <em>The Merry Widow</em> (Léhar)</dd>
  <dd>Nov. 2006: <em>Trial by Jury</em> (Gilbert &amp; Sullivan)</dd>
  <dd>Jun. 2006: <em>Pirates of Penzance</em> (Gilbert &amp; Sullivan)</dd>
  <dd>Feb. 2006: <em>Orpheus in the Underworld</em> (Offenbach)</dd>
</dl>"
Freelance Musicians' Database,NULL,"I came across a useful directory of freelance musicians the other day.  Cunningly, it's called Freelance Musicians, and can be found at http://www.freelance-musicians.info.  It's UK only (currently; that may or may not change), and provides facilities for listing individuals and groups/ensembles.  You can check out my own listing at http://www.freelance-musicians.info/musicians/231/.  

It's quite a new service, and there's only 152 musicians currently listed, but go sign up and swell their numbers if you've got a musical talent to flaunt!"
About Me,NULL,"<h2>Who am I?</h2>
[img_assist|nid=43|title=|desc=|link=node|align=left|width=198|height=274]I'm Alastair Smith, a full-time Software Developer.  I live in Cambridge, UK, and work for <a href=""http://www.grantadesign.com"">Granta Design Ltd.</a>, a medium-sized software company producing solutions for managing materials' information.  I work on their <a href=""http://www.grantadesign.com/products/mi/"">Granta MI product</a>.  I play the <a href=""http://en.wikipedia.org/wiki/Bassoon"">bassoon</a> to a high amateur standard, and take gigs as they come.  See my <a href=""/bassoon-cv"">Bassoon CV</a> for more information.
<!--break-->
<h2>Why do I blog?</h2>
I blogged on and off for a couple of years, but started doing so more seriously in June 2008 when I read Jeff Atwood's <a href=""http://www.codinghorror.com/blog/archives/001138.html"" title=""Coding Horror: The Ultimate Code Kata"">post on code kata</a>.  Kata originates in the martial arts, and is a form of effortful study.  Jeff's top tip is to maintain a blog in this manner, and so I began.  

<h2>What do I blog about?</h2>
I blog on a range of topics, that are loosely grouped into the categories of: programming (the main focus of the blog); music; cinema; science and technology; and ""random stuff"" (i.e., a bucket for everything else that doesn't fit one of the other categories).

<h2>What is ""CodeBork""?</h2>
""Bork"" is an expletive almost exclusively used by programmers.  Generally, it's used to refer to something that's broken; e.g., ""Try the other printer, this one's borked"".  ""CodeBork"" therefore refers to broken code, roughly equivalent to the more common term ""headf**k"".    

All that is a long way round of saying ""I made it up for my blog""."
Shure SE210 Earphones,"These headphones provide superior sound quality and immerse you in your music thanks to the noise-cancelling attachments.  A little bit of time should be spent with the headphones to find the right attachments for your ears, but it is ultimately time (and money) well spent for a quality listening experience.  

[4/5]
","
[4/5]
At £65 (+ delivery), the <a href=""http://www.amazon.co.uk/Shure-Se210-Sound-Isolating-Earphones/dp/B000POFA1A/ref=sr_1_1?ie=UTF8&s=electronics&qid=1215630379&sr=8-1"" title=""Amazon listing for Shure SE210 Earphones"">Shure SE210 earphones</a> are not the cheapest headphones ever, but neither are they the most expensive.  In comparison with, for example, the Sony MDR-EX71SLB Fontopia earphones that I owned previously, the sound is clearer, the noise cancellation greater, and the build quality higher.  All this is to be expected, however, when comparing with a set of headphones that were purchased for £35, and have since been reduced to just £17.  

The Shure SE210 earphones are a class above the Fontopias, with an impressively-sized driver unit on each earphone that produces crystal clear sound with a good balance.  Whilst the Fontopias are bassy &mdash; overly so, almost &mdash; the SE210s provide a more even frequency response over their range, which causes the listener to notice the higher frequencies more than with the Fontopias.  Plugging my SE210s directly into my computer at work reveals a load of interference from the CPU that I hadn't noticed with the Fontopias (this is particularly fun when scrolling with the mouse wheel!).

The increase in sound quality may make you realised that your encoded MP3s aren't quite as CD-quality as you originally thought.  You may well want to take the time re-rip your CDs using a higher bit-rate if you've used 128kbps MP3 encoding for example.  Whilst it largely depends on the encoder you've used for your music, 192kbps MP3 seems to be a good trade-off between sound quality and file-size, but the 256kbps AAC that iTunes Plus tracks come in would be a better choice; the difference between this bit rate and a lossless encoder should be only barely perceptible.  However, like me, you may already have set your encoding bit rate to be higher than usual; I have because I listen to a lot of classical music which degrades quickly at lower bit rates.  It is also worth ensuring you're using the right equaliser pre-set for the music type you're listening to; this doesn't seem to make a huge difference for me between certain types of music on my iPod (e.g., the ""Rock"" and ""Classical"" settings seem quite similar), but you may find otherwise.  My own issues may be because of ripping quality, the fact my iPod is a couple of years old, or something else entirely.  <a href=""http://homepage.mac.com/marc.heijligers/index.html"" title=""HifiVoice website"">HifiVoice</a> has some <a href=""http://homepage.mac.com/marc.heijligers/audio/ipod/index.html"" title=""iPod sound quality tips"">information on improving the sound quality from your iPod</a>, and in fact a lot of the advice will carry over to other players too.  

The most important thing to do after receiving the SE210s is to sit down with them and the large array of weird and wonderful fittings to find the set that suit you best.  There are three types included: foam fittings in four different sizes; rubber fittings in three different sizes; and a pair of three-tiered rubber fittings that look oddly like a rounded Christmas tree.  The foam fittings are rather like earplugs: you have to compress them, insert them, and then hold them in place whilst they expand to fit your ear.  Choosing the wrong set <em>will</em> affect the noise cancellation and will likely affect the perceived sound quality too, as the buds need to fit right to the bottom of the ear canal for the best results.  The weird Christmas-tree-type fittings are a good one to start with, as they help you find <em>just how far in these 'phones go</em>.  I found the foam attachments to be the best, although the largest do get a bit uncomfortable after a while; as such, I'm intending to find the time to try the next size down, or try the Christmas-tree-type fittings again.  You can also buy custom-made fittings from recommended suppliers.  

Sitting so deep in your ear as these earphones do, there is a potential wax issue.  Luckily, Shure have foreseen this, and include a handy cleaning tool for the phones themselves.  There's no substitute for better aural hygiene, though ;-)  All the fittings are washable in warm soapy water and should be left to drip dry; the foam ear-plug-style fittings will therefore need longer to dry.  Also included is a handy case for storing the earphones when not in use.  This is sturdy and well-made, although I find the zip a bit of an annoyance: it has a tendency to get stuck when unzipping.  

Because of the need to find the right size attachment for your ear &mdash; and the wax issue &mdash; these are not good earphones for sharing your music or otherwise lending to people.  Equally, though, you probably don't want to be lending these earphones to people (except maybe to try out for themselves), as they're something a little bit special.  These are all-round excellent earphones for music lovers of all kinds on a constrained budget; they're not wildly expensive like Shure's SE420s, for example, but they do provide an excellent quality of sound at a relatively modest price.  If you're looking to upgrade the 'phones you currently have, give these (or the moderately cheaper E2Cs or SE110s if you're more strapped for cash) a serious thought."
Rating filter module for Drupal,NULL,"One of the things I do with my blog is to review stuff that I've bought or seen or read or listened to.  As a result, I wanted to be able to rate this stuff as a concise summary of my view of it.  

There are a couple of Drupal modules available that could help me out with this, such as review, bookreview and moviereview, but these are all in various stages of death, and I didn't necessarily want all the extra, sometimes stuff-specific, functionality of those modules, so I sat down last night and wrote myself a little filter module that nicely converts ratings of the form [x/y] to stars.  It handles half stars and everything.

The following implementation is a <em>very</em> simple one: it deliberately doesn't expose any settings to the administrator or the user.  As such, the default out-of-the-box behaviour is all that is available.  No, you can't customise or change the images.  No, you can't customise the ratings input formats (e.g., using percentages).  No, you can't do anything but replace the [x/y] with x whole stars and y-x empty stars.  However, it's a pretty good introduction to developing for Drupal, so here goes.
<!--break-->
I've included the code below, and I'll dissect it along the way for our mutual benefit.  Before I do that, though, I should probably explain a couple of important Drupal concepts.  Drupal uses ""nodes"" for displaying content; nodes can be images, ""stories"" (posts like this blog entry), pages (for static content, like my <a href=""/about"" title=""About Me"">About Me</a> page), comments, forum posts, etc., etc., etc.  Any bit of content that is the main focus of the current page is a node.  Filters search the node content for patterns it recognises and does something with those patterns (e.g., stripping out disallowed HTML tags, syntax highlighting code, etc.).  Input formats are a collection of filters and control how a user can post content to the site in relation to the user's permissions and the node type.  For example, I have two input formats defined ""Filtered HTML"" and ""Full HTML"".  The names are fairly self-explanatory, and they both have all filters enabled (although ""Full HTML"" has the HTML filter turned off).  I could define a more restrictive ""Filtered HTML"" input format (or disable HTML entirely) for comments and enable this filter for anonymous users.  I could install a <abbr title=""What You See Is What You Get"">WYSIWYG</abbr> editor and enable it only for my own use when writing image descriptions (although that would be fairly useless).  

Anyway, on to the code.  First of all, I defined some constants representing the star images (full star, half star and ""empty"" star to give the total number of stars available for the rating), and one for the regular expression used to match the textual ratings.  This is fairly simple, standard PHP stuff:
<?php
define('STAR', '<img src=""/sites/default/files/ratingfilter/images/star.gif"" alt=""Star"" />');
define('HALF_STAR', '<img src=""/sites/default/files/ratingfilter/images/half_star.gif"" alt=""Half-star"" />');
define('EMPTY_STAR', '<img src=""/sites/default/files/ratingfilter/images/empty_star.gif"" />');
define('RATING_REGEX', ""#\[(\d+)(\.5)?/(\d+)\]#"");
?>

The regular expression is hopefully fairly self-explanatory, but in the interests of completeness, it matches the following sequence of characters:
<ol>
  <li>an opening square bracket</li>
  <li>one or more digits (\d+)</li>
  <li>an optional decimal point and number five for half stars (\.5)?</li>
  <li>a forward slash</li>
  <li>one or more digits</li>
  <li>a closing square bracket</li>
</ol>

Next, I implemented the <a href=""http://api.drupal.org/api/function/hook_filter"" title=""Drupal API documentation for hook_filter"">hook_filter()</a> function exposed by the Drupal API.  The $op parameter specifies what the filter should do; the $delta parameter specifies the ""offset"" of the filter (so multiple filters can be specified in one module; I ignored this); the $format parameter specifies the input format in use (I ignored this too); the $text parameter specifies the input text, e.g., the node body text that you're currently reading.  
<?php
function ratingfilter_filter($op, $delta = 0, $format = -1, $text = """") {
  switch($op) {
    // Expose the rating filter to the filter system
    case ""list"":
      return array('Rating Filter');
    break;
  
    // Provide a description of the filter, used on the Input Formats administration page
    case ""description"":
      return t(""Allows the insertion of graphical ratings (e.g. 4 stars out of 5) for the subject of the node.  Ratings are inserted as [x/y], i.e., x stars out of y.  "");
    break;

    // Prepare the text for filtering.  Usually used for escaping characters.  
    case ""prepare"":
      return ratingfilter_filter_prepare($text);
    break;

    // Apply the filter to the text.  
    case ""process"":
      return ratingfilter_filter_process($text);
    break;
  }
}
?>

As you can see, I chose to pass the grunt work out to a pair of helper methods.  There's no particular reason to do so &mdash; indeed, the example filter in the Drupal API documentation doesn't bother &mdash; but I prefer the modularity of this approach.  Judging by the example, this should help me should I decide to add further filters to the module (although this is unlikely).  I also think that keeping the grunt work out of this method improves the readability of the code.  This particular hook seems a little too general-purpose; personally I think I would have provided separate hooks for each filter operation, or for the prepare and process operations at the very least.  

The next function is an implementation of <a href=""http://api.drupal.org/api/function/hook_filter_tips"" title=""Drupal API documentation for hook_filter_tips"">hook_filter_tips()</a>.  This method is used on the node creation pages for the input formats for which the filter is enabled, and provides hints to the user on how the filter works.  Again, $delta refers to the ""offset"" of the filter in the module, $format refers to the input format in use, and $long specifies whether a summary or full description should be displayed.  This is another very simple function.  

<?php
function ratingfilter_filter_tips($delta, $format, $long = false) {
  if ($long) {
    return t('Ratings of the form [x/y] will be replaced with an image-based representation of the score.');
  } else {
    return t('Use [x/y] to display a score.');
  }
}
?>

On to the meat of the module.  I've grouped the prepare and process function together here, as the prepare function does nothing at all.  According to the <a href=""http://api.drupal.org/api/function/hook_filter"" title=""Drupal API documentation for hook_filter"">API documentation</a>, the prepare step should <em>only</em> do escaping of text.  As my filter has no need for this ability (currently, at least), the method simply returns the input as-is.  

I'll let you have a read over the process function before dissecting it.

<?php
function ratingfilter_filter_prepare($input) {
  // This method should only do escaping, which isn't required for this simple filter.  
  return $input;
}

// Replaces a rating of the form [x/y] with the appropriate number of stars.
function ratingfilter_filter_process($input) {
  $output = $input;

  while (preg_match(RATING_REGEX, $output, $matches) > 0) {
    $rating = """";
    $numStars = (int) $matches[1];

    // Create the numerator stars
    for ($i = 0; $i < $numStars; $i++) {
      $rating .= STAR;
    }

    // Add a half star if required, and count this as an extra whole star for the sake of the overall total.
    if ($matches[2] == "".5"") {
      $rating .= HALF_STAR;
      $numStars += 1;
    }

    // Create the (remaining) denominator stars
    for ($i = 0; $i < (int)$matches[3] - $numStars; $i++) {
      $rating .= EMPTY_STAR;
    }

    // Replace the textual ratings one at a time
    $output = preg_replace(RATING_REGEX, $rating, $output, 1);
  }

  return $output;
}
?>

First of all, I copy the input to an output variable.  This may seem a bit odd, based on the rest of the function's code: surely we could just use $input throughout?  This is, in many respects, a matter of style, but I prefer the fact that the parameter is left unchanged, thereby preserving the data that was passed into the function.  Yes, it increases memory usage, but we're rather beyond the days where memory usage was a real concern (unless you're running Windows Vista with 1GB RAM).  

We then run the regular expression defined above over the text until it no longer matches (i.e., all scores have been replaced).  The $matches array stores a number of items: $matches[0] is the entire text that the regex matched (i.e., the full textual rating, square brackets and all), and each subsequent element in $matches represents the text matching one of the parenthesised sub-expressions in RATING_REGEX.  That is to say, for a rating of 3.5 out of 5, $matches[1] = 3, $matches[2] = .5 and $matches[3] = 5.  I then cast the numerator ($matches[1]) to an integer to ensure that I've got the right datatype for the following <tt>for</tt> loop which then creates the right number of ""whole"" stars.  The cast isn't strictly necessary, but I like being type-safe, and I need the figure later anyway.  If a half star is required, I create the half star and increment the number of stars by one so that I don't end up with the wrong total number of stars (effectively rounding up the number of stars).  Finally the required number of empty stars are created to bring the total number of stars to equal the denominator ($matches[3]).  The <tt>preg_match</tt> function runs the RATING_REGEX again, this time replacing the matched text with the image-based rating.  The final parameter, $limit, specifies the number of occurrences that should be replaced; passing in 1 here ensures that the ratings are processed individually and in turn.  <tt>str_replace</tt> can also be used, passing in $matches[0] rather than the regex.  

The code is entirely public domain; I'm not even fussed about attribution in this instance!  Please feel free to copy it off this page and put it to use in your own site, but also note that I offer absolutely no warranty on it whatsoever, not even that it is fit for purpose.  

I hope this has been a useful exercise, and, as always, please leave me comments below as you see fit."
NAnt Starter Series,"<a href=""http://blog.jpboodhoo.com/"" title=""Develop with Passion"">Jean-Paul Boodhoo</a> put together a very clear and concise introduction to NAnt back in 2006.  It's helped me no end converting a couple of my projects from Visual Studio Ctrl+Shift+B/MSBuild to NAnt today; I heartily recommend it as an introduction to NAnt.  You can find the index to the series at http://blog.jpboodhoo.com/NAntStarterSeries.aspx.  
","<a href=""http://blog.jpboodhoo.com/"" title=""Develop with Passion"">Jean-Paul Boodhoo</a> put together a very clear and concise introduction to NAnt back in 2006.  It's helped me no end converting a couple of my projects from Visual Studio Ctrl+Shift+B/MSBuild to NAnt today; I heartily recommend it as an introduction to NAnt.  You can find the index to the series at http://blog.jpboodhoo.com/NAntStarterSeries.aspx.  

Also worth checking out is <a href=""http://www.nantbuilder.com/"">NAnt Builder</a>.  I've got the trial version for now, but I'm seriously considering buying a copy; it makes working with NAnt's XML-based makefiles much more pleasant.  The only thing holding me back is a couple of usability issues that I will report to the developers soon (e.g., Ctrl+Space brings up the Intellisense for the current tag, but also inserts a space character into the file).  At £25 (it's currently on special offer at $50), though, it's quite the bargain.

Next up is to convert the projects' unit tests to NUnit and work those into my build files...  Oh, and I should have an update on my <a href=""/coding/2008/06/25/message-passing-a-plug-framework.html"" title=""Message Passing in a Plug-in Framework"">message passing problem</a> relatively soon, too :-)  That, and a question about file encodings that's been troubling me for about a month now."
.NET Format Strings,NULL,".NET format strings rock.  These are roughly equivalent to the old-school C-style sprintf() functions, with their <code>%d</code>, etc., symbols.  There's some serious power to these strings, however; think PHP's date() function on acid, and for more than just dates.

None of this is likely to be new to all but the greenest of .NET developers (like me), but it's always worth reminding yourself of how great these things are.

[img_assist|nid=36|title=|desc=|link=node|align=none|width=400|height=407]
<!--break-->
You can pass format strings to the ToString() method of any object; .NET has some built-in logic that will help decipher certain format string.  For example, parity with PHP's date() function is achieved by passing a format string to the ToString() method on DateTime:

<blockcode language=""csharp"">
Console.WriteLine(DateTime.Now.ToString(""dd/MM/yy h:mm""));
</blockcode>

outputs

<blockcode>20/08/08 6:40</blockcode>

So far, so PHP date().  The power of format strings comes in when used with the String.Format() method.  This is the bit that's similar to the sprintf() class of functions in C, etc.  For example, we can insert a representation of an object into a string:

<blockcode language=""csharp"">
Console.WriteLine(String.Format(""Hello {0}"", ""world!""));
</blockcode>

outputs the classic

<blockcode>Hello world!</blockcode>

The beauty of that <code>{0}</code> place-holder is that <em>it directly indexes the remaining parameters passed to String.Format()</em>.  So we can do things like

<blockcode language=""csharp"">
Console.WriteLine(String.Format(""This string was output at {3}:{4} on {0}/{1}/{2}"",
    DateTime.Now.ToString(""dd""), DateTime.Now.ToString(""MM""), 
    DateTime.Now.ToString(""yy""), DateTime.Now.ToString(""hh""), 
    DateTime.Now.ToString(""mm""));    
</blockcode>

It's a somewhat-contrived example (unless you're writing a logging system), but it illustrates the point.  It's hugely more programmer-friendly than having to do ten or so string concatenations.  In addition, String.Format() handily accepts an array as its second parameter.  If you pass in an array (and no further parameters), the place-holders then index the array!

The place-holders can include further formatting information, such as padding characters (e.g., leading zeros, ensuring currency figures are displayed to two decimal places, etc.).  The correct format here is as follows:
<blockcode lang=""csharp"">
Console.WriteLine(String.Format(""{0:00000}"", 50));      //Outputs ""00050""
</blockcode>
The zero before the colon is the parameter indexer as before, and the bit after the colon is what does the actual formatting, padding the string with leading zeros.  Therefore, you can apply different formats to different parameters!

The flexibility of the latter part of the format strings is impressive too; you can use special characters to specify different built-in representations, such as ""C"" for currency (automatically inserts the currency symbol for the current locale and uses parentheses for negative numbers), ""P"" for percent (automatically inserts the % character), ""X"" specifies hex formatting&hellip;  And then there are <em>custom</em> format strings!

Kathy Kam at Microsoft published <a href=""http://blogs.msdn.com/kathykam/archive/2006/03/29/564426.aspx"" title="".NET Format String 101"">an excellent summary of their capabilities</a> a couple of years ago that's become my go-to reference for these things.  There's also a <a href=""http://blogs.msdn.com/kathykam/archive/2006/09/29/.NET-Format-String-102_3A00_-DateTime-Format-String.aspx"" title="".NET Format String 102: DateTime Format String"">follow-up post</a> describing further format strings specific to the DateTime type.  Check them out and see what other formatting wizardry you can accomplish!"
NUnit Headaches,NULL,"So my promised updates on other things haven't quite come to fruition...  This week's been kinda busy.  

But this evening I sat down with NAnt and NUnit to try and get my Unit Tests working under NUnit and running from the automated NAnt build, and, six hours later, I'm still here.

[img_assist|nid=21|title=|desc=|link=none|align=center|width=427|height=640]
<!--break-->
The first headache was trying to get the <nunit2> task in NAnt to work with my tests.  NAnt ships with NUnit 2.2.8, whereas I've been using the <em>much</em> more up-to-date 2.4.7 version.  Very early, I ran into the dreaded ""could not load file or assembly"" error, and looking again at the docs for the <a href=""http://nant.sourceforge.net/release/0.86-beta1/help/tasks/nunit2.html"" title=""&lt;nunit2&gt; task"">&lt;nunit2&gt; NAnt task</a> I discovered I needed to redirect the assembly bindings via a test.config file, but it took me a <strong>very</strong> long time to work out that I'd got my oldVersion and newVersion the wrong way round :-(
<blockcode language=""xml"">
<?xml version=""1.0"" encoding=""utf-8"" ?>
<configuration>
    <runtime>
        <assemblyBinding xmlns=""urn:schemas-microsoft-com:asm.v1"">
            <dependentAssembly>
                <assemblyIdentity name=""nunit.framework"" publicKeyToken=""96d09a1eb7f44a77"" culture=""Neutral"" /> 
                <bindingRedirect oldVersion=""2.2.8.0"" newVersion=""2.4.7.0"" /> 
            </dependentAssembly>
        </assemblyBinding>
    </runtime>
</configuration>
</blockcode>

Line 7 should actually read:
<blockcode language=""xml"">
                <bindingRedirect oldVersion=""2.4.7.0"" newVersion=""2.2.8.0"" /> 
</blockcode>
so that my 2.4.7-bound tests can be run with the 2.2.8 binaries provided with NAnt.  Something in <a href=""http://www.hanselman.com/blog/CommentView.aspx?guid=a9e7d7d7-027c-44de-b873-4d6c6e3255b2"" title=""Calling NUnit from NAnt Pragmatically"">Scott Hanselman's post on the subject</a> helped me grasp this fact in a flash of inspiration.  

However, Scott recommends invoking nunit-console.exe with the &lt;exec&gt; task &mdash; a more generic call-an-external-program sort of task.  For example,
<blockcode language=""xml"">
<exec program=""path\to\nunit-console.exe"">					
    <arg value=""My.Tests.dll"" />
    <arg value=""/xml=results\My.Tests.xml"" />
    <arg value=""/nologo"" />
</exec>
</blockcode>

Unfortunately, I'm back to square one with this method.  Running nunit-console.exe directly from the command line gives me the exact same ""could not load file or assembly"" error that I was suffering earlier, and, unsurprisingly, I get the same results through my NAnt build:
<blockcode>
NUnit version 2.4.7
Copyright (C) 2002-2007 Charlie Poole.
Copyright (C) 2002-2004 James W. Newkirk, Michael C. Two, Alexei A. Vorontsov.
Copyright (C) 2000-2002 Philip Craig.
All Rights Reserved.

Runtime Environment -
   OS Version: Microsoft Windows NT 5.1.2600 Service Pack 2
  CLR Version: 2.0.50727.1433 ( Net 2.0.50727.1433 )

Could not load file or assembly 'nunit.framework, Version=2.4.7.0, Culture=neutral, PublicKeyToken=96d09a1eb7f44a77' or
one of its dependencies. The system cannot find the file specified.
</blockcode>

Note there that <strong>NUnit version 2.4.7 is unable to load the nunit.framework assembly at version 2.4.7</strong>.  I've trashed my test.config file, so I'm not doing any assembly re-binding any more, and NUnit is housed within it's own directory in my working copy.  Goodness only knows what's going on here &mdash; maybe I'll find out tomorrow..."
NUnit Headaches 2,NULL,"Following up on last night's <a href=""http://www.alastairsmith.me.uk/coding/2008/07/19/nunit-headaches.html"" title=""NUnit Headaches"">NUnit Headaches</a>, I managed to solve the problem in a relatively short amount of time.  

Closer inspection of the error message received revealed that the nunit.framework DLL could not be located.  Adding nunit.framework.dll and nunit.core.dll to the <abbr title=""Global Assembly Cache"">GAC</abbr> fixed this.  My impression was that the installutil .NET tool did this for me; turns out that's not the case, and I needed the gacutil tool in the Windows SDK instead.

Now I just have to work out how to resolve the same error with the DLL under test without adding <em>that</em> to the GAC as well.  Maybe then I can fix NUnit in the same way and uninstall NUnit from the GAC.
<!--break-->
"
NUnit Aspirin,NULL,"I finally resolved my NUnit headaches.  Turns out the aspirin I needed to was to <abbr title=""Read the Fucking Manual"">RTFM</abbr> and continue to <a href=""http://blog.jpboodhoo.com/AutomatingYourBuildsWithNAntPart5.aspx"" title=""Automating your builds with NAnt &mdash; Part 5"">the next step of Jean-Paul Boodhoo's guide to NAnt</a>.  

[img_assist|nid=24|title=|desc=|link=url|url=http://en.wikipedia.org/wiki/Aspirin|align=center|width=500|height=400]

Had I done this before, I would have realised that in order to get NUnit working in the manner I wished, I needed to copy the NUnit DLLs either to the <abbr title=""Global Assembly Cache"">GAC</abbr> or to the build staging area so that they resided in the same directory as my Unit Tests' DLL.  The latter is the preferred method.  I would also have learned that I needed to copy any other dependent assemblies, such as <a href=""http://logging.apache.org/log4net/index.html"" title=""Apache log4net"">log4net</a> and <a href=""http://www.nmock.org/"" title=""NMock"">NMock</a> as part of the build process.
<!--break-->
So why is it necessary to copy these DLLs to the same directory?  Because whilst the .NET dependency resolution is fairly bright (the GAC is pretty cool), it's not all <em>that</em> bright, and most developers don't both extending its behaviour.  I guess it's not really worth the bother.  

What would be more useful, however, is for the compile-time reference information to be shoved into the .NET assembly, so that it can try that location before any others (it could additionally resolve the relative path provided at compile to an absolute path and so try both options).  I guess, however, that most build environments don't reflect the final production environment post-install, so maybe that's why this functionality isn't there.  

As I mentioned before, this option is preferred over installing NUnit to the GAC.  This is because this would tie me to the installed version of NUnit.  If I upgraded my tools depot to NUnit 2.5, and built my unit tests with NUnit 2.5, I'd have to go messing around with <a href=""http://www.alastairsmith.me.uk/coding/2008/07/19/nunit-headaches.html"" title=""NUnit Headaches"">that assembly redirection/rebinding I mentioned in my previous post</a>.  Either that, or upgrade NUnit in the GAC too.  This way, I have more independence in choosing the version of NUnit I use and flexibility in using it, which is the exact same reason I dropped the use of the <nunit2> task in my build files.  

Maybe now I can get on with unit testing my Message Manager!  :-)"
Message Passing 2,NULL,"As a follow-up to my previous post on <a href=""http://www.alastairsmith.me.uk/coding/2008/06/25/message-passing-a-plug-framework.html"">message passing in a plug-in framework</a>, I thought I'd post my solution to the problem.  Now that I've finally reached a solution, that is...!

[<strong>Note</strong>: It would be worth reading the first post to get an idea of what I was trying to do.  ]

[img_assist|nid=26|title=|desc=|link=node|align=none|width=480|height=318]
<!--break-->
As you may recall, I was investigating a number of ways of passing messages in my plug-in framework, and was trying to settle on the built-in .NET event system as the underlying mechanism.  Whilst discussing this with my friend and colleague <a href=""http://www.analysisuk.com/blog/"" title=""Steve's personal blog"">Steve Harrison</a>, he suggested that I look into the <a href=""http://msdn.microsoft.com/en-us/library/system.componentmodel.inotifypropertychanged.aspx"">INotifyPropertyChanged interface</a> provided as part of the .NET System.ComponentModel namespace.  

Despite my earlier misgivings about using events in this way, this turned out to be very useful!  Rather than having to declare my own event types, etc., I simply implement the interface as follows:

<blockcode language=""csharp"">
public class Plugin : IPlugin, INotifyPropertyChanged
{
    // Implement IPlugin to identify this as a plugin
    ...

    // Implement INotifyPropertyChanged to publish events
    public event PropertyChangedEventHandler PropertyChanged;
}
</blockcode>

Easy-peasy &mdash; one line of code!

Now, ideally, listening plug-ins would then subscribe directly to the events published by other plug-ins, but there's no way of doing this in a de-coupled framework.  How does a listening plug-in know what events are available for subscription, or which plug-ins are publishing events?  

To get around this, I introduced IMessageBroker (and an associated implementation):
<blockcode language=""csharp"">
public interface IMessageBroker
{
    IDictionary<IPlugin, IList<EventInfo>> Events { get; }
    void Initialise(ILog logger);
    void RegisterEvents(IPlugin publisher, EventInfo[] publishedEvents);
    void DeregisterEvents(IPlugin publisher, EventInfo[] publishedEvents);
    void Subscribe(IPlugin publisher, EventInfo e, Delegate handler);
    void Unsubscribe(IPlugin publisher, EventInfo e, Delegate handler);
    IList<EventInfo> GetSubscribedEvents(IPlugin subscriber);
}
</blockcode>

As you can see, the message broker serves registration-type requests (from plug-ins publishing events) and subscription-type requests (from plug-ins listening to events).  The <a href=""http://msdn.microsoft.com/en-us/library/system.reflection.eventinfo.aspx"" title=""EventInfo Class (System.Reflection)"">EventInfo class</a> is provided as part of the Reflection feature in C#/.NET, and does exactly what it says on the tin: it provides access to metadata on the event, and can mediate subscriptions.  Using Reflection, a plug-in passes an array of the events it publishes to the message broker, which then stores it in a dictionary (or map for the Java-types out there) of events, indexed by publisher.  Subscribe takes a publisher, an event and an event handler and creates the publish/subscribe relationship between the two plug-ins using the <a href=""http://msdn.microsoft.com/en-us/library/system.reflection.eventinfo.addeventhandler.aspx"" title=""EventInfo.AddEventHandler Method (System.Reflection)"">AddEventHandler method</a>.  

Handily, the broker is flexible enough to be able to subscribe any PropertyChangedEventHandler delegate to registered events &mdash; it doesn't have to be part of a plug-in.  This was doubly useful when testing:
<blockcode language=""csharp"">
public void SendMessageTest()
{
    // Subscribe to the event with a test handler
    PropertyChangedEventHandler propertyChanged = 
        new PropertyChangedEventHandler(PropertyChangedHandler);
    mb.Subscribe(eventPlugin, mb.Events[eventPlugin][0], propertyChanged);

    // Fire the event
    ...

    if (!handlerRun)
    {
        Assert.Fail(""PropertyChangedHandler not run"");
    }
}

public void PropertyChangedHandler(object sender, PropertyChangedEventArgs e)
{
    Trace.WriteLine(""PropertyChangedHandler was successfully called"");
    Trace.WriteLine(String.Format(""Arguments passed were: {0}, {1}"", sender, e));
    handlerRun = true;
}
</blockcode>

So, to summarise and conclude.  The approach I took was a little more complex than might usually be the case in a publish/subscribe model, but this was dictated by the nature of the system.  The use of the INotifyPropertyChanged interface greatly simplified the implementation of the events, and Reflection allowed me to dynamically pass around information about events rather than having to explicitly state it.  The INotifyPropertyChanged pattern also gives me a model on which to base any extension to the pattern that I might need later on.

How easy it will be to introduce a hierarchy or taxonomy of events at a later stage remains to be seen; I don't think it will be particularly easy.  However, my needs are fairly simple at this stage, and although the plug-in framework is intended to be generic, it's still tied to the personal finance program I'm trying to build.  One of my goals is to completely decouple the framework from the program, but we shall see how easy this is.  With the message passing problem solved, I now need to catch up on <a href=""http://trac.alastairsmith.me.uk/FinanceProg/roadmap"" title=""FinanceProg Roadmap"">lost time</a>."
The Dark Knight,"[5/5]
This is a truly excellent movie, and a fitting sequel to 2005's <em>Batman Begins</em>.  It is much darker, more raw and more visceral than the first four Batman films, as we have come to expect from Christopher Nolan, and you will be reflecting on the movie for days afterwards.  The superhero movie has this year come of age, and <em>The Dark Knight</em> is the best example yet of how a superhero movie can be a damn good piece of cinema too.
","
[5/5]
Simply put, this is the best film that I have seen in a long time, and I've seen a lot of excellent films over the last nine months or so.  <em>Gone Baby Gone</em>, <em>Charlie Wilson's War</em> and <em>There Will Be Blood</em> all spring to mind.  <em>The Happening</em> does not.  These are all truly excellent films in their own way (with the exception of the <em>The Happening</em>), whether it be challenging your morals and ethics, the sharply written script, or the sheer force of the delivery.  

None of them scratch <em>The Dark Knight</em>.

[img_assist|nid=29|title=|desc=|link=none|align=right|width=300|height=410]Here is a film which is so dark in theme and plot (something that is reflected in the cinematography of a lot of the crucial scenes) that you're desperately searching for a faint glimmer of hope.  Halfway through, you find yourself wishing &mdash; begging, demanding &mdash; that <em>that</em> particular plot twist hadn't happened.  Heath Ledger's Joker brims with insanity, infecting even the seemingly incorruptible; the portrayal of the character is a long, <em>long</em> way from Jack Nicholson's 1989 outing in <em>Batman</em>.  As always, the devil is in the detail, and Ledger has it nailed right down to the lascivious licking of his lips.  Whoever thought that Tim Burton's legendarily dark interpretations could be made to look like a Disney film?  The Joker's plots are inconceivably twisted and devious, and executed with just the right mix of intellect and psychosis.  For a while, it seems like the anarchy The Joker seeks to unleash will reign.  This is the most accurate and faithful rendition of The Joker yet, and personally I would argue that it is the best we are ever likely to see.  

<em>That</em> plot twist is the culmination of a terrible choice, and the consequences are unbelievably grave.  Nolan fairly assaults his audience, leaving them questioning Batman's decision and putting a similar choice in the hands of ordinary people towards the end of the film.  Two boats &mdash; one full of civilians, one full of prisoners &mdash; have been wired with explosives, and the detonator is on the other boat.  Which one will blow?  The result is surprising and one of the glimmers of light in the onslaught of the final half hour.  

<em>The Dark Knight</em> has been hailed as <a href=""http://www.reelviews.net/php_review_template.php?identifier=1235"" title=""Review of Batman: The Dark Knight on ReelViews"">this series' <em>The Empire Strikes Back</em></a>, hailed by some to be the best sequel of all time.  There is now a strong contender to that title.  Breaking box office records, <a href=""http://www.guardian.co.uk/global/2008/jul/28/actionandadventure.christianbale"">the race is on to see whether <em>The Dark Knight</em> will take more than <em>Titanic</em></a>.  

Superhero movies are finally growing up and taking themselves seriously, arguably following on from <em>Batman Begins</em>, and as a result are becoming even more mainstream than previously; they're no longer just entertaining action films, but good cinema in their own right.  <em>Iron Man</em> wouldn't have been the film it was (and, for me, wouldn't have been as good) had <em>Batman Begins</em> not come before to show everyone what superhero movies can be.  <em>The Dark Knight</em> is the most grown-up and serious of all of them, and I can't wait to see where Nolan takes it next."
Lookalikes,NULL,"I've been told a few times in the past (and, surprisingly, a couple of times fairly recently) that I look like Leonardo DiCaprio.  I don't see it myself.
[img_assist|nid=30|title=Me, Alastair Smith|desc=|link=node|align=left|width=150|height=200][img_assist|nid=31|title=Leonardo DiCaprio|desc=|link=node|align=left|width=122|height=200]"
My experiences with Server 2008,NULL,"I've recently been reconfiguring my home network to utilise a Windows domain.  As I've improved my home development environment, I've started needing to make greater use of network shares and things, and running a mixed-OS environment as I do, I have inevitably run into some problems with credentials.  <strong>This is the first post in a two-part series</strong>; this post will describe my initial experiences configuring my domain, whilst <a href=""http://www.alastairsmith.me.uk/science-and-technology/2008/08/20/linux-and-server-2008-active-directory.html"" title=""Linux and Server 2008 Active Directory"">the second post</a> will focus on getting Ubuntu and Fedora Core 7 talking to my domain.
<!--break-->
After solving some rather annoying hardware issues and (thankfully, in hindsight) dropping my previous plan to deploy Xen as my hypervisor<sup><a href=""#footnote1"" title=""Footnote 1"" id=""footnote1link"">1</a></sup>, I started installing Windows Server 2008.  I chose this most recent version of Windows Server for a couple of reasons:<ol><li>I've used it at work and quite like it</li>
<li>There are some changes from Server 2003 that I wanted to get to know better</li>
<li>It would be a useful learning experience.</li></ol>

Running a four-core AMD rig with 4GB RAM as I am, I opted for the x64 version.  Installation was painless (Microsoft have done wonders with their Operating System installers for both Vista and Server 2008) and very soon I had a basic installation running.  A few tweaks later and BAM BAM BAM BAM BAM! I had <abbr title=""Active Directory Domain Services"">AD DS</abbr>, <abbr title=""Active Directory Certificate Services"">AD CS</abbr>, <abbr title=""Internet Information Services"">IIS</abbr> 7, File Services and <abbr title=""Domain Name System"">DNS</abbr> installed and configured.  The server roles really are an effective way of simplifying the configuration of some complex network infrastructure, like Active Directory.  In Windows 2000 Server, Active Directory was a scary, monolithic beast; in Windows Server 2008, it's smooth, streamlined and well tamed.  

Later on, I realised that I'd missed a role: <abbr title=""Dynamic Host Configuration Protocol"">DHCP</abbr>.  This is something with which I've never had any experience configuring, but the role installation wizard walked me through the minefield with a clear map of the way forward.  Very quickly, I had Server 2008 dishing out IP addresses to the rest of my network left, right and centre, and confirmed this by turning off the DHCP on my router.  Copying over the address reservations from the router to Server 2008 also proved very easy, with the only small hiccough being that the MAC address box in Server 2008 doesn't accept any byte delimiters (so colons are right out).  

The next step was to install VMware Server for running virtual machines.  I quickly found that VMware Server 1.0.x is not supported on Vista and Server 2008, so my only option was to download the beta of Server 2.0.  This is very different from the 1.0.x versions that I'm used to, with management now handled through a web interface rather than a console program.  Accessing the web interface proved to be a bit of a bitch for no good reason, but after playing around with the Windows firewall and ensuring that all the VMware services were running, it started to play ball<sup><a href=""#footnote2"" title=""Footnote 2"" id=""footnote2link"">2</a></sup>.

The next step was to move my existing Fedora Core 7 VM (which hosts this blog, and most of my development tools, such as my <a href=""http://subversion.tigris.org/"" title=""Subversion homepage"">Subversion</a> repositories, <a href=""http://trac.edgewall.org/"" title=""Trac homepage"">Trac environments</a> and <a href=""http://ccnet.thoughtworks.com/"" title=""Cruise Control.NET homepage"">Cruise Control</a> log files) to the new VMware server.  This was accomplished really very painlessly, as would be expected, despite the fact that I was moving the VM from a 1.0.x to a 2.0b host.  I simply copied over the VM's files (~15 mins for over 40GB thanks to my Gigabit Ethernet subnet), imported the VM into the VMware Server inventory and hit ""power on"".  I <em>love</em> server virtualisation<sup><a href=""#footnote3"" title=""Footnote 3"" id=""footnote3link"">3</a></sup> :-)

Finally, I got around to creating a set of new VMs.  These are running Vista and Ubuntu (both x86 due to the small amount of memory I can allocate them), and Windows Server is still responsive.  I guess that's because I've still got 1 core and ~2GB RAM to throw at running it.  It definitely took a hit when I was running two further VMs (Vista and Ubuntu x64), and VMware Server kept suspending one or more of the VMs due to lack of resources.  This was mildly disappointing given the grunt I've thrown at the machine, but not hugely surprising given that the Vista VMs were consuming 2-3GB RAM between them.  

The <a href=""http://www.alastairsmith.me.uk/science-and-technology/2008/08/20/linux-and-server-2008-active-directory.html"" title=""Linux and Server 2008 Active Directory"">next post</a> will focus on setting up my Ubuntu and Fedora VMs to work with the Active Directory Domain.  
<hr />
<p id=""footnote1""><sup>1</sup> Thankfully, as I realised soon after starting to install Windows Server 2008 that if I hadn't done this, all my data would have been tied up in a VHD file.  Not good.  <a href=""#footnote1link"">Back to article.</a></p>
<p id=""footnote2""><sup>2</sup> You'd be right to wonder what on Earth I'm doing running a VMware server on an Active Directory Primary Domain Controller.  Indeed, VMware warn against this in their installer.  However, this mostly applies to environments where the Domain Controller is going to be under load and need it to remain responsive.  With the VMs shut down, the Domain Controller runs effectively idle with very little in the way of CPU load, and relatively standard RAM load.  <a href=""#footnote2link"">Back to article.</a></p>
<p id=""footnote3""><sup>3</sup> And even working for the second biggest player in the server virtualisation field, exposed to the technology on a daily basis, I'm still impressed by it!  <a href=""#footnote3link"">Back to article.</a></p>"
Linux and Server 2008 Active Directory ,NULL,"I've recently reconfigured my home network to utilise a Windows domain. As I've improved my home development environment, I've needed to make greater use of network shares and things, and running a mixed-OS environment as I do, I have inevitably run into some problems with credentials. <strong>This is the second post in a two-part series</strong>; <a href=""http://www.alastairsmith.me.uk/science-and-technology/2008/08/20/my-experiences-server-2008.html"" title=""My experiences with Server 2008"">the first post</a> describes my initial experiences configuring my domain, whilst this post will focus on getting Ubuntu and Fedora Core 7 talking to my domain.
<!--break-->
There are two subsystems that need to be configured to get Linux to connect to an Active Directory: Samba and Kerberos.  Most tutorials will teach you how to do this manually, and this method is fraught with semi-meaningful errors if you make a mistake (or seem to not have made a mistake).  These tutorials are ten-a-penny if you <a href=""http://www.google.co.uk/search?q=Join+Linux+to+active+directory"" title=""Google search for &quot;Join Linux to active directory&quot;"">search Google</a>, so I'm not going to replicate that information here, and not least because I didn't manage to get it working this way.  

What I'd like to do instead is draw your attention to a less well-known method that applies to all of the major distros and Mac OS X too: using <a href=""http://www.likewisesoftware.com/products/likewise_open/"">Likewise Open</a>.  I tried this on my Ubuntu server and it worked like a charm; ditto Fedora 7.  At this point, I'd like to credit <a href=""http://bobbyallen.wordpress.com/"" title=""Bobby Allen's blog"">Bobby Allen</a> for the invaluable help his <a href=""http://bobbyallen.wordpress.com/2008/05/23/how-to-join-ubuntu-804-to-windows-active-directory-domain/"" title=""How to join Ubuntu 8.04 to Windows Active Directory (Domain)"">post</a> provided.

The first step is to install Likewise Open.  This is simple enough on Ubuntu, Debian and other distros that use the apt-get update system; distros using RPM and yum (such as Fedora) will need to follow the link above to download, which requires registration.  It may be available from fink on Mac OS X; I don't know as I haven't tried.  

Once installation is complete (which is very easy), you then just need to run the following command:
<code>
sudo domainjoin-cli join fqdn.of.your.domain Administrator
</code>

On Fedora 7, the installer specified I had to supply the path to domainjoin-cli too:
<code>
sudo /usr/centeris/bin/domainjoin-cli join fqdn.of.your.domain Administrator
</code>

Note that <code>fqdn.of.your.domain</code> is your domain itself, not the domain controller.  So for me, where the domain controller is called gandalf.middle-earth.co.uk, I typed middle-earth.co.uk here.  All being well, you should get a message stating that the domain was joined successfully.  If not, check the errors, tweak as necessary, and try again.  I had no errors on either system; it worked first time.  

The next think you have to do (on Ubuntu) is to register Likewise Open with the daemon host and start the daemon:
<code>sudo update-rc.d likewise-open defaults
sudo /etc/init.d/likewise-open start</code>

This should be completed as part of the installer on Fedora, etc.  Unfortunately, I haven't found a way to confirm this yet.  Either way, you should be able to log on to your Linux host with the username DOMAIN\username, just as you would on Vista.  Note that you <em>really do have to add the DOMAIN</em>\, otherwise the Linux host will try to authenticate you against /etc/passwd or whatever other mechanism you were using before.  DOMAIN is the ""pre-Windows 2000 domain"" that you entered when setting up Active Directory, and is usually your domain name without the top-level domain suffixes (such as .com, .co.uk, etc.).  

Finally, in order to get Windows Server 2008 to pick up the DNS suffix of the Linux host in the DHCP manager, I had to tweak the DHCP client configuration.  This was simple enough; simply add or edit the following line in your dhclient.conf file:
<code>
send ""linux-hostname.fqdn.of.your.domain"";
</code>

Don't forget to restart your network interface after making this change.  

The location and name of the dhclient.conf file differs across distros: in Ubuntu it was at /etc/dhcp3/dhclient.conf; in Fedora 7 it was at /etc/dhclient-eth<em>n</em>.conf, where <em>n</em> matches the number of the network interface the config applies to (most likely 0)."
NMock Framework,NULL,"To aid the testing of my personal finance program, I've been making heavy use of the <a href=""http://www.nmock.org/"">NMock</a> mocking framework for .NET.  This is a cool utility that allows you replace actual code calls with mock code calls, factoring out a lot of complexity of setting up some tests.  For example, I first started using it to mock the <a href=""http://logging.apache.org/log4net/"" title=""Apache log4net"">log4net</a> logging framework rather than creating a new logger for each test/suite and passing that around.  More recently, I've found it useful in mocking parts of the plug-in framework that are required by the code but not part of the test (such as the preferences' manager, for example).
<!--break-->
By way of example, here's set-up method from one of my plug-ins' test suite:
<blockcode lang=""csharp"">
public void SetUp()
{
    mocks = new Mockery();
    logger = mocks.NewMock<ILog>();
    pluginFramework = mocks.NewMock<IFramework>();

    prefsManager = new PreferencesManager();
    Expect.Exactly(4).On(logger).Method(""Info"");
    prefsManager.Initialise(logger, prefsFile, prefsFile);

    Expect.Once.On(logger).Method(""Info"");
    pluginBroker = new PluginBroker(logger, pluginFramework);
    Expect.Once.On(logger).Method(""Info"");
    Expect.Once.On(logger).Method(""InfoFormat"");
    Expect.Exactly(2).On(pluginFramework).GetProperty(""PreferencesManager"").Will(Return.Value(prefsManager));
    pluginBroker.Initialise(pluginDir);

    Expect.Once.On(logger).Method(""Info"");
    messageBroker = new MessageBroker();
    messageBroker.Initialise(logger);

    Expect.AtLeastOnce.On(pluginFramework).GetProperty(""MessageBroker"").Will(Return.Value(messageBroker));
    Expect.AtLeastOnce.On(pluginFramework).GetProperty(""PluginBroker"").Will(Return.Value(pluginBroker));
    Expect.AtLeastOnce.On(pluginFramework).GetProperty(""PreferencesManager"").Will(Return.Value(prefsManager));
}
</blockcode>

The NMock statements are those that start <code>Expect.Number_Of_Times.On(mock_object)</code>.  As you can see, the statements read very much like English, which I think is one of the strengths of the framework.  <strong>You can immediately tell what the expectation of this object is without having to think around the syntax of the framework</strong>.  Note too the heavy use of interfaces: NMock can only mock interfaces, which I think is another strength of the framework.  This focus on interfaces really drives to you programming via interfaces, which is a key part of designing by contract.

So what does this code do?  First of all, it creates a <code>Mockery</code> object, which is essentially a factory object for mock objects.  Next, a mock logger and mock plug-in framework object are created.  A real <code>PreferencesManager</code> is created, and we tell NMock that we expect exactly 4 calls to logger.Info in the <code>Initialise</code> method of <code>PreferencesManager</code>.  The same is done for a <code>PluginBroker</code> and its <code>Initialise</code> method.  Notice here that we're specifying too that the property <code>PluginBroker.PreferencesManager</code> will return a specific value, i.e., the <code>PreferencesManager</code> created earlier.  A <code>MessageBroker</code> is then created in the same way.  Finally, we state that we expect the properties <code>MessageBroker</code>, <code>PluginBroker</code> and <code>PreferencesManager</code> on <code>pluginFramework</code> will return the concrete objects already created.  

At the end of each test, which may contain further expectations of our mocked objects, we can run <code>Mockery.VerifyAllExpectationsHaveBeenMet()</code>, which does exactly what it says on tin (a victory for clear over concise method naming!).  Note, however, that this is an instance method and not a static one, so you'll need to call it on the <code>Mockery</code> object created at the beginning.  

NMock has a certain amount of flexibility in its interface.  For example, you can replace <code>Expect.x</code> with <code>Stub</code>, which is equivalent to <code>Expect.AtLeast(0)</code>.  There is also a raft of quantifiers for expectations in addition to <code>Once</code> and <code>AtLeast(x)</code>, such as <code>Between(x, y)</code> and <code>Never</code>.  

The downside of NMock is that your expectations end up littered with strings referencing parts of the API.  If a mocked part of the API changes, you have to update these so-called ""magic"" strings with the appropriate new values; it also enforces a certain amount of trial and error when initially testing your expectations.  A framework like <a href=""http://www.ayende.com/projects/rhino-mocks.aspx"">Rhino Mocks</a> will use the compiler to tell you when you've messed up by allowing you to reference the mocked properties on the mock object directly, e.g.:
<blockcode lang=""csharp"">
IFramework pluginFramework; // Create stub framework object via Rhino Mocks
IPreferencesManager prefsMan; // Create and initialise preferences' manager as above
pluginFramework.PreferencesManager = prefsMan;
</blockcode>

This is obviously more concise than the NMock equivalent, listed in the last line of the first code block.

Mocking frameworks are a very useful addition to the unit testing toolkit that allow you to abstract away some of the complexities of setting up unit tests.  They can also be used to test that an interface correctly obeys its contract.  Testing the contract should not, however, be confused with exercising the actual code in the implementation.  If you haven't already investigated mocks, why not look at incorporating one such framework into your next project's unit tests?"
Abstract Factories,NULL,"I've come rather late to the <a href=""http://en.wikipedia.org/wiki/Design_Patterns"" title=""Wikipedia entry on Design Patterns"">Design Patterns</a> ball; unfortunately it wasn't something that got taught at Warwick (although that may have changed with their new CS course), and so it wasn't until my final year that I even <em>heard</em> of these patterns.  

I am slightly embarrassed to admit this, because they're a pretty fundamental part of programming (or, at least, programming in a <abbr title=""Don't Repeat Yourself"">DRY</abbr> manner) and the CS education I received from Warwick was in most other respects first class; it's just that they didn't really cover much in terms of ""real world"" programming techniques.  The emphasis was (rightly, in my opinion) always on teaching the fundamentals and the underlying theories, rather than teaching us how to code Java.  For example, we didn't get any suggestions on good UI design and implementation, but we did get a course on Human-Computer Interaction which was much more wide-ranging and interesting (gotta love Psychology :-).  

Design Patterns provide ready-made solutions to a number of standard programming problems, and in this first post in a <a href=""http://www.alastairsmith.me.uk/category/coding/design-patterns"" title=""Design Patterns' series"">series of as-yet-unknown length</a> I will investigate the Abstract Factory pattern.
<!--break-->
I'm posting about the <a href=""http://en.wikipedia.org/wiki/Abstract_Factory"" title=""Wikipedia article on the Abstract Factory pattern"">Abstract Factory pattern</a> as I have recently found a need to make use of it!  I realised that the plug-in architecture in my <a href=""http://trac.alastairsmith.me.uk/FinanceProg"" title=""FinanceProg"">personal finance program</a> was creating a new instance of a plug-in each time a data object was created.  So to create a transaction, for example, a new instance of the Transaction plug-in was created, complete with all the metadata for the plug-in, such as its author and version, etc.  This seemed sub-optimal, and was causing problems with new Transactions not being initialised before use, so I decided to split the data objects from their plug-ins.  Suddenly I needed a way to be able to ask a plug-in for a new instance of a data object.  ""Great,"" I thought to myself, ""this sounds like a job for the Factory pattern"".  After a bit of digging around Wikipedia's articles on creational patterns, I came across the Abstract Factory pattern, and found that this did exactly what I wanted: it creates new objects of the same general type (e.g., ""an account""), but with different specifics (such as ""a credit card account"" or ""a current account"").  

The pattern goes a little something like this:  

<blockquote>A factory makes products.  My design for a factory specifies that it makes buttons.  They might be square buttons, round buttons, red buttons or blue buttons, but precisely which type doesn't matter, as they're all buttons.  It's only once the factory has been built to the specification that it starts making buttons of a particular type, and we can make a particular type of button by finding the appropriate factory and asking it for a button.  Later on, I find I want to be able to make green diamond-shaped buttons too, so I build a new factory to the same blueprint but whose final product is a green diamond-shaped button.  </blockquote>

[img_assist|nid=39|title=|desc=|link=node|align=center|width=400|height=256|title=The Abstract Factory Pattern in UML form]

Note from the description and the diagram that <strong>there are two distinct elements to the pattern: the factory and the product</strong>.  With the Abstract Factory pattern, the detail of both elements has to be abstracted; it doesn't make sense otherwise, and you end up with a regular Factory pattern that's behaving a little strangely.

So we end up with two interfaces: <code>IFactory</code> (<code>GUIFactory</code> in the diagram) and <code>IProduct</code> (<code>Button</code>).  <code>IFactory</code> defines the factory method (<code>createButton</code>) and <code>IProduct</code> the product's characteristics (e.g. <code>paint()</code>).  Each of these are implemented by concrete factories and products respectively, and in the manner that is required for the concrete factories (e.g. to make red round buttons).  The <a href=""http://en.wikipedia.org/wiki/Abstract_Factory#Java"">Java sample on the Wikipedia page</a> indicates that the abstract factory should define a single static method (<code>getFactory()</code>) to determine the correct factory to use; for example, the decision could be made based upon a configuration file value, or the platform upon which the program is running.  It is this method that distinguishes this pattern as something unique, and not just an implementation of the Factory pattern via the use of interfaces.  Depending on your situation, you could provide an override to <code>getFactory()</code> that specifies a preference for the object type returned.  However, this is mostly in violation of the abstraction inherent in the pattern, and may even be in violation of the pattern itself too.  

Constructors on the concrete products should be specified as <code lang=""java"">protected</code> in Java, or <code lang=""csharp"">internal</code> in C#, to prevent direct instantiation. <em>At least one constructor must be defined in order to hide the default constructor</em>, which is <code lang=""csharp"">public</code> and parameter-less in both Java and C#.  

By far, the greatest advantage of this pattern is that the client does not need to know the detail of the concrete implementation &mdash; an IProduct is an IProduct is an IProduct is an IProduct, to <a href=""http://en.wikipedia.org/wiki/A_rose_is_a_rose_is_a_rose_is_a_rose"" title=""A Rose is a rose is a rose is a rose"">paraphrase</a> &mdash; and so the concrete implementations can be passed around via references to their parent interfaces.  This maintains the abstraction and (partially) upholds the principles of design by contract.  These are all Good Things&trade;.

One thing that I'm not certain of in this and the Factory pattern is how a created object should be initialised.  Factory methods seem to be parameter-less by design, which means that the products then have to have their attributes specified later.  This seems a bit like paying for and receiving your new car and <strong>then having to paint it, fit the engine and upholster the interior</strong>.  Equally, however, I don't want to receive the same bog-standard Car as everyone else; I want a 2.0-litre diesel engine, I want leather upholstery, and I want it in <span style=""color:#004225; font-weight:bold"">British Racing Green</span>.  This would normally be done by passing parameters to the constructor, but this obviously isn't possible with a Factory pattern as the product's constructors aren't available for use; you <em>have to go through the factory</em>.  

The Abstract Factory pattern is a powerful method for creating objects in an abstract environment, such as a plug-in environment.  It also has other uses in, for example, a database abstraction layer and other similar situations where an abstraction for another part of the system should be provided in place of a concrete implementation."
Burton/Schumacher vs Nolan Round 1,NULL,"<em>Batman Forever</em> (1995) screened on Channel 4 last night.  Having not seen it in many years and having <a href=""http://www.alastairsmith.me.uk/films/2008/07/31/dark-knight.html"" title=""The Dark Knight review""><em>The Dark Knight</em></a> still relatively fresh in my mind, it was very interesting watching it again and comparing the tone of the two movie series.
<!--break-->
Tim Burton passed on the directorial reins to Joel Schumacher for <em>Batman Forever</em> (and the relatively awful 1997 film <em>Batman and Robin</em>), and instead took the producer's role for <em>Batman Forever</em> only.  Maybe unsurprisingly then, the film still has Burton's style stamped all over it, with a subtle gothic fairy-tale undercurrent, and maybe it's telling that Burton didn't stick around for <em>Batman and Robin</em>.  

What is also interesting is the flimsiness of some of the characters.  Two-Face (Tommy Lee Jones), for example, was but a cardboard cut-out of the character, and psychologist-and-love-interest Chase Meridian (Nicole Kidman) and Robin (Chris O'Donnell) were also relatively crudely drawn.  Maybe it's because there were so many new characters in this film that it wasn't feasible to flesh them all out properly; the genesis of Two-Face was skipped through very quickly via a handy news report Bruce Wayne happened to be watching, for example.  Comparing Aaron Eckhart's Two-Face with Tommy Lee Jones' particularly highlights the more mature tone of Christopher Nolan's <em>Batman</em> series: Two-Face was much more cold and cruel in <em>The Dark Knight</em> than Tommy Lee Jones' who was simply ""psychotic villain #2"".  

The Riddler (Jim Carrey), however, was a much more interesting character, and much more well-defined; this was mainly due to this being ""The Riddler's film"" (Two-Face evidently isn't an interesting enough character to have his own film in the same way: in all of the films in which he's appeared, he's been paired up with another rogue, usually The Joker).  I felt Carrey was perfectly suited to the role, drawing on his performances in <em>The Mask</em> and, to some small extent, <em>Ace Ventura</em> to provide an interestingly obsessional and deranged character.  The film traces Edward Nygma's path from his first introduction to, and rejection by, Bruce Wayne after being hired by Wayne Enterprises, through his development as a villain, right through to the tense-but-not-edge-of-your-seat conclusion.  

<em>Batman Forever</em> is highly stylised in a way that <em>Batman Begins</em> and <em>The Dark Knight</em> are not.  Whereas the latter two place Batman into a regular action-style movie, <em>Batman Forever</em> is a comic book film.  This is not necessarily a bad thing: there are many more and varied colours in <em>Batman Forever</em> than <em>The Dark Knight</em>, for example, so it is visually quite lively.  The style is very much that of comic books: baddies are bad, and borderline (or actually) insane; the hero is macho and gets the girl (and is most definitely a <em>hero</em> and not a <em>vigilante</em>); the girl is beautiful and submissive and breathy-voiced.  In many ways, though, this works fantastically well; I had forgotten how iconic the scene with Batman and Chase Meridian on the rooftop by the Bat Signal really is, for example.  However, it is clearly aimed at a younger audience and perhaps because of this it often runs very close to becoming a parody of itself; the music is over-egged, with cheesy sound effects emphasising the on-screen action to a point that is almost slapstick.  I believe it is only because of context of the rest of the film that it only just stays on the right-side of the line.   

It is also very much a product of its time.  The opening titles and some of the CGI really sticks out, such as in the opening titles where the rendering faltered for whatever reason, leaving them not completely smooth<sup><a href=""#footnote1"" id=""footnote1-ref"">1</a></sup>.  Additionally, some of the sets <em>are clearly sets</em>, such as the Bat Cave and the opening scenes with the bank vault.  I also love the way the film cuts from a just-escaped moment of peril to the characters dusting themselves down (check out the end of the climactic sequence where Batman has to save Robin and Chase at the same time).  

In summary, then, whilst <em>Batman Forever</em> is not a <em>bad</em> film, the script lets it down; were I reviewing the film, it would probably get a 3.5 or 4 out of 5.  It's got great, well-suited actors for the top roles, good action sequences, and some good style, but the writing is just <em>painful</em> in places.  I am in the process of acquiring the first two films in this series, Tim Burton's <em>Batman</em> (1989) and <em>Batman Returns</em> (1992), so that I can better compare this first series with Christopher Nolan's Batman for the 21<sup>st</sup> century.  I'm expecting them to be better and darker than the neon and somewhat immature <em>Batman Forever</em>.
<hr />
<ol>
<li id=""footnote1"">Having said that, CGI blending still affects films today, such as some scenes in <em>The Lord of the Rings: The Fellowship of the Ring</em> and <em>Hellboy 2: The Golden Army</em>.  For an example of good CGI blending, you need look no further than <em>Iron Man</em> which has some of the best CGI I've ever almost-not-seen.  <a href=""#footnote1-ref"">Return to post</a>.</li>
</ol>"
More on Message Passing!,NULL,"I recently posted a question to <a href=""http://www.stackoverflow.com/"" title=""Stack Overflow"">Stack Overflow</a>.  This is a great developer community site run by <a href=""http://www.codinghorror.com"" title=""Coding Horror"">Jeff Atwood</a> and <a href=""http://www.joelonsoftware.com"" title=""Joel on Software"">Joel Spolsky</a> that's currently in private <a href=""http://beta.stackoverflow.com"" title=""Stackoverflow Beta"">beta</a>.  It's a good mixture of discussion forum, <a href=""http://www.digg.com"">Digg</a>-style ratings' site, and wiki that promises to become a truly invaluable resource once development is complete; it's well on its way already!  The idea is that ordinary developers post questions and the community responds, with both questions and answers being voted upon by the community.  

I've included my question below to gain the issue a bit of extra exposure.  If you have a stackoverflow.com account, please visit <a href=""http://beta.stackoverflow.com/questions/50822/message-passing-in-a-plug-in-framework-problem"" title=""Message Passing in a Plug-In Framework Problem"">my question</a> and post a response there, or vote on the question and any existing answers.  Alternatively, please leave a comment on this post with your thoughts.
<!--break-->
<hr />First off, there's a bit of background to this issue available on my blog:
<ul>
<li>http://www.alastairsmith.me.uk/coding/2008/06/25/message-passing-a-plug-framework.html</li>
<li>http://www.alastairsmith.me.uk/coding/2008/07/31/message-passing-2.html</li>
</ul>

I'm aware that the descriptions in those posts aren't hugely clear, so I'll try to summarise what I'm attempting as best I can here.  The application is a personal finance program.  Further background on the framework itself is available at the end of this post.

There are a number of different types of plug-in that the framework can handle (e.g., accounts, export, reporting, etc.).  However, I'm focussing on one particular class of plug-in, so-called data plug-ins, as it is this class that is causing me problems.  I have one class of data plug-in for accounts, one for transactions, etc.

I'm midway through a vast re-factoring that has left me with the following architecture for data plug-ins:
<ul>
<li>The data plug-in object (implementing intialisation, installation and plug-in metadata) [implements <code>IDataPlugin<FactoryType></code>]  </li>
<li>The data object (such as an account) [implements, e.g., <code>IAccount</code>]  </li>
<li>A factory to create instances of the data object [implements, e.g., <code>IAccountFactory</code>]</li>
</ul>
Previously the data object and the plug-in object were combined into one, but this meant that a new transaction plug-in had to be instantiated for each transaction recorded in the account which caused a number of problems.  Unfortunately, that re-factoring has broken my message passing.  The data object implements <code>INotifyPropertyChanged</code>, and so I've hit a new problem, and one that I'm not sure how to work around: the plug-in object is registering events with the message broker, but it's the data objects that actually fires the events.  This means that <strong>the subscribing plug-in currently has to subscribe to each created account, transaction, etc.!</strong>  This is clearly not scalable.  

As far as I can tell at the moment I have two possible solutions:
<ol>
<li>Make the data plug-in object a go-between for the data-objects and message broker, possibly batching change notifications.  I don't like this because it adds another layer of complexity to the messaging system that I feel I should be able to do without.   </li>
<li>Junk the current event-based implementation and use something else that's more easily manageable (in-memory WCF?!)</li>
</ol>
So I guess I'm really asking: 
<ol>
<li>how would you solve this problem?</li>
<li>what potential solutions do you think I've overlooked?</li>
<li>is my approach even vaguely on-track/sensible?! :-)</li>
</ol>
Many thanks in advance for the time and effort you put into your responses.  As you will be able to tell from the dates of the blog posts, some variant of this problem has been taxing me for quite a long time now!  As such, any and all responses will be greatly appreciated.  

The background to the framework itself is as follows:  
<blockquote>My plug-in framework consists of three main components: a plug-in broker, a preferences' manager and a message broker.  The plug-in broker does the bread-and-butter plug-in stuff: discovering and creating plug-ins.  The preferences' manager manages user preferences for the framework and individual plug-ins, such as which plug-ins are enabled, where data should be saved, etc.  Communication is via publish/subscribe, with the message broker sitting in the middle, gathering all published message types and managing subscriptions.  The publish/subscribe is currently implemented via the .NET <code>INotifyPropertyChanged</code> interface, which provides one event called <code>PropertyChanged</code>; the message broker builds a list of all plug-ins implementing <code>INotifyPropertyChanged</code> and subscribes other plug-ins this event.  The purpose of the message passing is to allow the account and transaction plug-ins to notify the storage plug-ins that data has changed so that it may be saved.</blockquote>"
Stack Overflow,NULL,"<a href=""http://www.stackoverflow.com/"" title=""Stack Overflow"">Stack Overflow</a>, as I mentioned in my <a href=""http://www.alastairsmith.me.uk/coding/2008/09/08/more-message-passing.html"" title=""More on Message Passing!"">last post</a>, is a new community site developed by <a href=""http://www.joelonsoftware.com/"" title=""Joel on Software"">Joel Spolsky</a> and <a href=""http://www.codinghorror.com/"" title=""Coding Horror"">Jeff Atwood</a> aimed at enabling developers to to help each other.  Much like asking a question of a trusted friend or colleague (but with 5,379 to call on), there are no holds barred on the questions that you can ask.  Even questions that you think might be ""dumb"" questions are tackled.  For example, <a href=""http://beta.stackoverflow.com/questions/55375/add-values-to-enum"" title=""Add values to enum"">my curiosity was piqued by enums and whether values could be added to them dynamically</a>.  I already knew to a certainty of roughly 98% what the answer would be, but I asked it anyway.  As it turns out, two people thought it a ""good"" or ""useful"" question, too, and it picked up nine answers.  My community reputation is now pushing close to the magic 100 mark, when I can start down-voting questions and answers.
<!--break-->
Stack Overflow has been in beta for approximately three or four months, and <a href=""http://www.joelonsoftware.com/items/2008/09/11.html"" title=""Stack Overflow Podcast #21"">goes live on Monday</a>.  In a wicked cool crossover of <a href=""http://blog.stackoverflow.com/2008/09/help-us-beta-test/"" title=""Help us Beta test"">a wiki, a blog, a forum and a Digg/Reddit-type community site</a>, developers post questions on programming, and they get answered.  The good questions are applauded.  The bad ones are shuffled under the carpet.  And your reputation goes up and down according to the quality of your questions and answers (as decided by the readers).

In addition to the core value proposition of the site, it has a number of nifty web-2.0 features.  I particularly like the ""similar questions"" suggestions that appear when you're writing a question to try to cut down on duplication, and the live preview of your text to make sure you've got the (slightly clunky, I find) Markdown syntax correct.  Others are more ""expected"" in this day and age, like a comment or vote being registered via AJAX and thus not requiring a page refresh.  

Jeff and Joel have plenty of followers not immersed in the .NET framework, and there are already plenty of questions up on C++, Ruby, PHP, Java, and others.  There are also questions on topics related to programming, such as version control and continuous integration.  It's already become a hugely useful resource for me, and I hope it will continue to be so once the doors are opened next week.  Go check it out!  Details on how to access the beta are available from http://blog.stackoverflow.com/2008/09/help-us-beta-test/."
Test-Driven Development,NULL,"I've recently started reading Kent Beck's excellent book <a href=""http://www.amazon.co.uk/Test-Driven-Development-Addison-Wesley-signature/dp/0321146530/ref=sr_1_1?ie=UTF8&s=books&qid=1221258043&sr=1-1"" title=""Test-Driven Development by Example on Amazon.co.uk""><i>Test-Driven Development by Example</i></a> at work as part of my self-development goals for this quarter.  Test-Driven Development (TDD) is something that I'd heard about, and knew the basic principle of (write the unit tests first and write the code to pass them), but had never tried or bothered to read up on.  There's an emphasis in our team at work on moving to a more agile way of working so that we can better respond to the different demands of the two products the Web Interface supports: Citrix XenApp and Citrix XenDesktop.  It's one of those changes that is more easily talked about than accomplished, and for a while at least it will be mostly talk.  However, TDD seems to fit in quite nicely as a halfway house between our current development practices and a fully agile environment: we (obviously) currently write unit tests, although our test coverage is possibly lower than we'd like; TDD isn't explicitly agile, being an implementation method or coding technique rather than a project methodology.  In short, it would be a small step to move from where we are now to now + TDD, and then (hopefully) an easier step from now + TDD to ""agile"".  

But enough about Citrix and back to the book and TDD.  The book is written in four sections: an example application developed using TDD; an introduction to the <a href=""http://en.wikipedia.org/wiki/XUnit"" title=""Wikipedia article on the xUnit frameworks"">xUnit frameworks</a> (specifically <a href=""http://www.junit.org/"" title=""JUnit homepage"">JUnit</a>); an introduction to <a href=""http://www.alastairsmith.me.uk/category/coding/design-patterns"" title=""My Design Patterns' series"">design patterns</a>; and an introduction to re-factoring.  The last two may initially seem incongruous, but re-factoring is a core part of the TDD mindset; design patterns provide ""template"" implementations for common problems which speed up your coding and are proven quantities (and thus are easier to test).  The example application (multi-currency money support) is actually a good example, being simple enough to understand in a single a concept, and yet complex enough to make an interesting exercise.    

Beck walks through the example in a series of 10 or so chapters, adding roughly one new test per chapter and implementing the code that will be exercised by the test.  Each chapter follows the TDD cycle of:
<ol>
<li>Add a test</li>
<li>Run all the tests to check the new one fails</li>
<li>Write some code to fix/pass the test</li>
<li>Run all the tests to make sure they succeed and nothing's been broken by your change</li>
<li>Re-factor to clean up, remove duplication, etc.</li>
</ol>

This is repeated until the component is finished, and is begun again each time new functionality is added to the component.  

One of the (many) great things about TDD is that because you write the test first, you automatically produce the simplest, most useful interface to your classes.  For example:
<blockcode lang=""java"">
public void testFivePlusFiveEqualsTen() {
    Dollar five = new Dollar(5);
    Dollar result = five.plus(new Dollar(5);
    assertEquals(new Dollar(10), result);
}
</blockcode>

That's right folks, TDD makes you write cleaner code!

The tests continue in a similar vein throughout this section, with the code being implemented in parallel.  With that in mind <strong>it really is best to implement the code samples as you read.</strong>  I didn't do this, and regretted it by the end of the section, as some of the tests and changes had got quite complex.  

Another good thing about TDD is that you instantaneously get near-100% code coverage.  As is customary to mention at this point, coverage is no indicator of product or code quality, but it is a useful statistic for identifying ""holes"" of code that isn't tested at all.  

The book is well-written: a good sense of humour pervades throughout the text, without getting in the way.  Unlike <i>Don't Make Me Think!</i>, it is not littered with footnotes.  

I knew before reading <i>Test-Driven Development by Example</i> that TDD was good, and I ever so vaguely knew why (it gets you writing unit tests!), but I hadn't grasped <em>just how good</em> it actually is.  I've quickly become a bit of a TDD evangelist, and I most certainly will be looking to make use of it in my next task at work (it's too late for my current task to get any real benefit from it).  It's also got me thinking about unit testing our JavaScript code (<a href=""http://www.jsunit.net/"" title=""JsUnit homepage"">JsUnit</a> looks like a good start), as we make heavy use of this and it currently has low unit test coverage.  

If you're not already using TDD, I strongly recommend looking into it for your own projects &mdash; be they private home-brew projects or tasks at work &mdash; and buy a copy of this book as an introduction.  "
Cruise Control.NET Gadget for Vista Sidebar,NULL,"I randomly came across this earlier this evening: <a href=""http://codeclimber.net.nz/archive/2007/07/15/CruiseControl.NET-Monitor-Vista-Gadget-version-0.9.5.aspx"">a Cruise Control.NET gadget for the Vista sidebar</a>!
<!--break-->
This is a pretty sweet gadget, which gives you a greater insight into the the status of your build than the CCTray app does.  The current release is at version 0.9.5, and the project seems to have gone quiet since that version was released over a year ago.  

I haven't tried it out as I'm not (currently) running Vista on my home PC, but I'm hoping to give it a go at work to keep an eye on <a href=""http://www.analysisuk.com/blog/"" title=""Stephen Harrison's blog"">Steve's</a> CC.NET server! ;-)  Speaking of which, I may look into setting up my own <a href=""http://ccnet.thoughtworks.com/"" title=""Cruise Control.NET Homepage"">CC.NET</a> server at work as a proof-of-concept replacement for our own CI server (which is sparsely-featured, and is custom-developed, I believe).  

Further updates soon!"
Chaotic Computing,NULL,"Quantum Computing (basing computer processing and storage on subatomic particles, rather than discrete voltages) has long been hailed the next big step in computer science.  Exploiting the ""weirdness"" of quantum mechanics offers huge steps forward in parallel processing and cryptography, but progress has been slow.  After all, it's not easy pinning down a single photon, electron, or other particle, and getting it to obey your every command.  To be honest, it's a bit like herding cats.

Enter Chaotic Computing.  As featured in the current issue of the <em>New Scientist</em> and available online <a href=""http://technology.newscientist.com/channel/tech/mg20026801.800-in-chaotic-computing-anarchy-rules-ok.html"" title=""In chaotic computing, anarchy rules OK"">here</a> (subscription required), this relatively new field promises similar progress to Quantum Computing with a fraction of the overhead; chaotic circuits can be easily built with today's technology.

[img_assist|nid=50|title=|desc=|link=node|align=center|width=450|height=300]
<!--break-->
Generally speaking, chaos in computing is not desirable.  We computer scientists like our creations to be deterministic, and have come to expect them to behave that way.  After all, the CPU at the heart of all computers is a deterministic beast: put in 2, another 2, and instruct it to add the two numbers together and <strong>you will always get 4</strong>.  There are, of course, occasions when determinism breaks down, or is simply impossible to achieve (such as in concurrent programming), but even this rates way above <em>chaos</em>.  Electrical Engineers and the like may learn how to create chaotic circuits, but this is more as a guard against their creation by teaching students the characteristics of such circuits so they know to avoid them.  

So it's all a bit of a shock to learn that chaos can be ""tamed"" to provide a useful function.  What is chaos, though?  Is it a purely anarchical system with no logic nor order?  Is it a completely random system?  In a word, no.  For this is the chaos underpinning the Butterfly Effect, the idea that a tiny seed occurrence can have enormous consequences in a system with a heightened sensitivity to that occurrence: classically, a butterfly flapping its wings in Brazil could set off a tornado in Texas.  (<strong>Not-very-interesting side note</strong>: there are literally hundreds of variations on this.  Check out some of them <a href=""http://clearnightsky.com/node/428"" title=""Butterfly Effects - Variations on a Meme"">here</a>.)  There is very little logic, and certainly no way of predicting the output of anything but the most simple chaotic systems given any input, but there are some underlying properties that define them:
<blockquote cite=""http://en.wikipedia.org/wiki/Chaos_Theory#Chaotic_dynamics"">
<ol>
<li>it must be sensitive to initial conditions</li>
<li>it must evolve over time so that any given region of the system will eventually overlap with any other region</li>
</ol>
</blockquote>
<cite>Paraphrased from <a href=""http://en.wikipedia.org/wiki/Chaos_Theory#Chaotic_dynamics"" title=""Chaos Theory"">Wikipedia's definition</a>.</cite>

There is a third property that I didn't fully understand (follow the link for more information), but I'm hoping it doesn't dent my understanding too much, and the two above points seem to encapsulate the idea of a chaotic system quite nicely.  For example, think about adding milk to a cup of tea or coffee: your initial condition is a cup of black tea, which is a stable state until the milk is added.  It's stable in that it's not going to spontaneously turn green, but due to the Brownian motion of the water molecules, it's sensitive to other fluids being added to it.  You add the milk, and it ""billows"" its way down and then back up the tea causing regions of the black tea to become white and overlap with other regions of the black tea causing them to be white...  And so on until the diffusion has completed and you have a cup of white tea.  Ever wondered why the phrase ""storm in a teacup"" works so well?  <strong>That's exactly how a planet's weather systems work</strong>.  

Anyway, enough of the theory.  How does this relate to computing?  The basic idea is that although chaotic circuits are apparently random, there is an underlying set of voltages that is cycling in a predictable manner.  Think about the piercing feedback whine you get at a rock gig; this is a chaotic system, and it's generally not a nice one (unless it's well-controlled and you're a Hendrix fan).  However, it may start as a buzz in the amplifier caused by a nearby electrical source, such as a guitar, which is running on a particular voltage; the amp is also running on a particular voltage.  Cycling between the two causes the feedback loop and the loud whine.  

These predictable voltages can be used as the basis of a logic gate, as is the case with current technology.  The difference, however, is in the use of control lines.  Feed a pair of voltages into a logic gate's inputs, and you will always get the result according to the gate's function (there's that determinism thing again).  However, <em>feeding the same inputs into a ""chaogate"" returns different results according to the value of the control lines</em>: you might get an AND, a NOR or a NOT, for example.  

<strong>That's the first sucker-punch for determinism</strong>.  Who needs separate logic gates for AND, OR, NOT, etc., when I can build one chaogate and change the output from the gate on the fly?  Build a CPU out of these things, and it could easily also function as a GPU, just by flicking a hardware or software switch.

The second comes as a result of the first: <strong>a self-repairing circuit</strong>.  When one gate fails (maybe the chaos circuit broke and threw itself into array) another can be re-tasked to do its job.  Maybe the first gate only stopped working for AND, but will continue working fine for NOT and OR; it still continue its job, but the AND function hasn't been lost from the system.  

More interesting still is the third outcome of this technique: <a href=""http://en.wikipedia.org/wiki/Content-addressable_memory"" title=""Wikipedia article on Content-Addressable Memory"">Content-Addressable Memory</a> (CAM).  Whilst normal memory works on the basis of addressed locations, CAM circuits allow you to locate an item of data in memory simply by knowing its value.  For example, if I load data into CAM, <em>I can find all instances of a datum by simply feeding it into the retrieval circuit</em>.  As you can probably imagine, this has huge ramifications for searching and indexing, so it's no surprise to learn that ""a commercial customer"" (anyone else thinking Google here?) has commissioned such a system for internal testing.  

The problem with traditional CAM is the extra circuitry required to implement it, and the associated costs in power and heat (as well cash money).  However, because chaogates have a large number of possible stable states, it is much easier to store large amounts of information.  For example, the absolute minimum number of bits required to represent every letter in the standard English alphabet is 5 (2<sup>5</sup> = 32, so even then there are a few spaces going).  Using chaogates, this would consume one Non-linear Bit (or ""nit""), with plenty left over.  One of the researchers has stored the entirety of Lincoln's Gettysburg Address (1452 characters, or 11616 bits/1.4KB) <em>in just 264 nits, 1/44th of the storage requirement</em>.  Part of this saving is achieved through the system's ability to use one nit to represent an entire word, and so low-level memory searches can be conducted on actual words, using a simple AND on the voltage representing that word.  

There are issues with the technology, however.  For example, chaotic circuits are vulnerable to interference by electronic noise (think back to the feedback at the rock concert), and so it may be that they require shielding.  The switching circuits to alter the chaogates' behaviour will be tricky and therefore expensive to develop, and it's apparently proving difficult to sell the concept to existing chip manufacturers.  

There's no doubt however, that this technology has the potential to provide some impressive improvements in searching, indexing, storage efficiency and processor sizes, not to mention the idea of self-repairing circuits.  It may also prove to be an enabling technology for the more exotic approaches like quantum computing.  I for one am looking forward to the first chaotic search engine, and the day that my computer has inside it a chaos chip.  You can do your own jokes around that sentence.  
<hr>
For more information, check out the <em>New Scientist</em> article linked above, or William Ditto's older article at http://www.fortunecity.com/emachines/e11/86/mastring.html (contains more technical detail)."
Trackback Spam,NULL,"As you may have noticed, I've not blogged in a while.  The good news is, I'm back! :-)

Unfortunately, over roughly the same period, I've not really been keeping on top of clearing out my trackback spam, to the point where I deleted in the region of 1000 spam trackbacks this evening.  Short of disabling the trackback function (which I don't really want to do), I'm after a bit of advice on combating trackback spam.  

Oddly, I don't have any problems with comment spam at all.  

Comments welcome, as always! :-)"
"Akismet Re-Enabled, Comments Now Misbehaving",NULL,"As I mentioned in <a href=""http://www.alastairsmith.me.uk/random-stuff/2008/11/03/trackback-spam.html"" title=""Trackback Spam"">a previous post</a>, I've been suffering quite badly from trackback spam.  I dread to think what it's done to my Google ranking (and related search terms), and what impact it's had on my bandwidth consumption.
<!--break-->
That aside, as <a href=""http://www.oneangrydwarf.co.uk/"">Tim</a> suggested, I have re-enabled <a href=""http://www.akismet.com/"">Akismet</a>.  We'll see whether this sorts out the problem, although I'm quite hopeful.  

I'm afraid this does have a knock-on effect for you, dear reader.  The <a href=""http://drupal.org/project/akismet"">Akismet module</a> for Drupal is still undergoing the upgrade to Drupal 6 and is still a bit buggy...  As such, when you submit a comment, you will end up with a blank screen; the module calls through to a function that was removed in Drupal 6.  <a href=""http://drupal.org/node/240894"">This has been logged</a> as a bug, and I've looked into it myself in some detail, but haven't worked out how to resolve it yet.  There's no patch available to resolve this issue yet, and from the <a href=""http://drupal.org/project/akismet"">module homepage</a>:
<blockquote cite=""http://drupal.org/project/akismet"">Sept 5, 2008 Update: this module is pretty unsupported at this point. </blockquote>

My apologies for the inconvenience.  If it starts to annoy you, please leave a comment saying so ;-)  More seriously, drop me an email using <a href=""http://www.alastairsmith.me.uk/contact"">the contact form</a> and I'll look for an alternative solution."
Calling PHP Developers,NULL,"My new favourite resource for development-related questions, <a href=""http://www.stackoverflow.com/"">StackOverflow.com</a>, already provides <a href=""http://stackoverflow.com/questions/tagged/php%20ide"">a number of answers to this</a>.  However, I'm interested to hear from you, my small community of readers, what IDE you use for PHP development, and why you like it.
<!--break-->
I'm looking for a good PHP IDE; I've tried Eclipse in the past, and whilst it goes like the clappers on my work machine, I've found it dog slow on my laptop.  I also remember finding the PHP plug-in not being as good as the Java Development Tools in terms of re-factoring support.  The syntax highlighting was top-notch, though :-p  

I use Eclipse and Visual Studio at work for Java/J# and C# development, and they are both excellent for their languages.  Visual Studio is a bit rubbish at Java/J#, but I've recently switched to Eclipse and I'm glad I did so.  It's the gold standard for IDEs when it's in C# mode, particularly once you've got <a href=""http://www.jetbrains.com/resharper/"">Resharper</a> or similar installed, which adds some of the good stuff from Eclipse like quick and customisable code templates, file markers, more re-factorings, etc.  

I'm looking for good Subversion support (but also the ability to move to Git/Mercurial relatively painlessly), syntax highlighting, and good re-factoring support.  I'm not too keen to pay for the IDE; I've used Zend in the past and didn't like it at all, and my PHP work these days is mostly tinkering so it's not really worth the investment.  

But most of all, I'd like to know what the professionals use in the field.  I'd like to know what PHP's gold standard IDE is."
"Newbie Errors, and Horses, not Zebras",NULL,"I've been doing a lot more JavaScript development at work over the last few months, working on a site that is AJAX'd up to the hilt.  Everything's done with fancy jQuery effects and asynchronous calls back to the web server.  The page in the browser literally never reloads.  

Something that caught me out today: there's no <code language=""java"">String.equals(String)</code> in JavaScript.  My first thought was ""what kind of half-baked language doesn't provide an equals method for strings?!"".  Then it occurred to me that select isn't broken (or ""Horses, not Zebras"").  Hell, I've been doing Java and C# for so long now, that I'd forgotten that most times you can just do <code language=""javascript"">""string"" == ""string""</code> and you don't <em>need</em> <code language=""java"">.equals()</code>.  

""Select isn't broken"" is Tip No. 26 from <a href=""http://www.pragprog.com/the-pragmatic-programmer"">The Pragmatic Programmer</a>, an excellent book for professional devs that is on quite a few ""must-read"" lists.  If you're a dev and you haven't read this book, do so; although some of it is starting to look dated, and a chunk of it is pretty *nix-oriented (but not specific to those platforms), it's one of the classic texts that helps you look at your job in a different way.  I found it quite an enlightening read, and <a href=""http://www.amazon.co.uk/Pragmatic-Programmer-Andrew-Hunt/dp/020161622X/"">it's relatively cheap on Amazon</a>.
<!--break-->
"
Interview Questions from Software Development Interviewees,NULL,"Having been with Citrix for over a year now, I've got a good hold on my job as a professional software developer, and an idea of what I'd be looking for if I were to start looking to move to another company.  As I've learnt more about the role (albeit specific to Citrix), I've occasionally found myself thinking, ""that would make a good interview question"".  Note that I'm talking here about questions <em>from</em> the interviewee rather than <em>for</em> them.  I'll go over some advice that I've found previously, and present some of my own questions based on my new-found experience.  And, of course, you're welcome to submit your own suggestions by commenting below.
<!--break-->
There's quite a bit of information on the web on questions for the interviewee, but not much on the sorts of questions in which I'm interested: the sorts of questions that tell me something I want to know but hasn't been covered already, either in the interview or in company material (hiring brochures, etc.).  Even the ever-faithful <a href=""http://www.stackoverflow.com/"">StackOverflow.com</a> can only provide <a href=""http://stackoverflow.com/questions/tagged/interview-questions"" title=""Questions tagged &quot;Interview Questions&quot;"">a list of questions posted by interviewers</a>.  

So, I'm starting a list here.  I'll divide it up into categories, and I'll indubitably re-visit it a few times in the future to add new ones.  

<h3>Office Environment</h3>
<dl>
  <dt>How noisy is the environment on a day-to-day basis?</dt>
  <dd>Like most developers, I can't work well in a noisy environment.  Individual offices would be fantastic, but it is sadly rather idealistic to expect this.  Cubicles are anti-social and still noisy.  Open-plan is a good compromise, but only if it's a quiet office.  </dd>
  <dt>Is the working environment I've seen today representative of the environment I would be working in?</dt>
  <dd>An extension of the above.  Some interviews include a tour of the office, although there is potential for them to show you only the nicer bits rather than where you'll actually be working.  If you're not sure, you should ask to see the environment you'll be working in; there's nothing worse than being shown something close to your ideal working environment at interview and then turning up on your first day to find you'll be working on the roof.  Unless <em>that's</em> your ideal working environment.</dd>
</dl>

<h3>People</h3>
<dl>
  <dt>Which person or company do you most admire in the software industry, and why?</dt>
  <dd>I came across this as a suggestion of what to ask an interviewee, but I think it can work both ways.  The key is obviously in the qualification (the ""and why?""), but an answer of Apple, for example, would indicate a focus on usability and maybe a predilection for shiny things with moderate substance.  An answer of Adobe might imply a respect for solid engineering and quality products.  If you're being interviewed by your potential manager, you could ask about inspirational leaders, or more simply what qualities they see as being good in a manager.</dd>
</dl>

<h3>Product and codebase</h3>
<dl>
  <dt>How do you fare against <a href=""http://www.joelonsoftware.com/articles/fog0000000043.html"">the Joel Test</a>?</dt>
  <dd>Obviously I wouldn't ask this question in that way, but I would put the Joel test to the interviewer and calculate their score.  <strong>This is a <em>really</em> important one to get right</strong>, although unlike Joel, I'd probably settle for a score of 10, maybe even 9.  My department at Citrix gets a 9 (with a couple of grey areas), and we seem to be doing ok.  I'm sure with a higher score we'd be doing better, though.  </dd>
  <dt>How long do your builds take to complete?</dt>
  <dd>A product with a slow build cannot be a truly agile product.  Slow builds have big issues with developer productivity, and discourage check-ins after a certain time each day, in case the check-in breaks the build.</dd>
</dl>

<h3>The company/organisation</h3>
<dl>
  <dt>How do you measure developer productivity and performance?</dt>
  <dd>I think the correct response here is ""with weekly status reports and quarterly reviews"" (for me, at least).  Although I sometimes find writing status reports tedious, they are a good way of tracking productivity (and a good way of keeping all but the most lazy or disillusioned developers on track).  Reviews should be at least biannual, and I personally prefer quarterly reviews, partly for the increased rate of feedback, and partly because it often makes sense to track performance against company reporting periods.</dd>
  <dt>How much independence do staff have?  What opportunities are there for side projects and self-development?</dt>
  <dd>A company that doesn't give its staff independence tends to stifle its employees' growth.  Google is famous for its <a href=""http://www.google.com/support/jobs/bin/static.py?page=about.html&about=eng"" title=""Google Jobs"">""twenty-percent time""</a>, in which employees can work on something they're passionate about.  Some of their next-gen products (such as Google Suggest, now integrated into Google Search) are a result of the ""twenty-percent time"".  Google is relatively unusual in allowing this, but a compromise situation is to allow employees time for self-development, which is crucial to both the company and the employee.</dd>
</dl>

So, that's my list (so far).  What would you ask?  What are you looking for from your next place of work?"
An Evaluation of Web Frameworks I: Introduction,"Following on from a recent comment discussion, I'm taking on a small project to evaluate a number of the major web development frameworks currently available.  I've settled on a very simple blogging system as my sample project, with a common MySQL backend.  In this first post of the series, I set out the terms of reference for the evaluation, including the (very basic) requirements for the sample application.
","
Following on from a recent comment discussion, I'm taking on a small project to evaluate a number of the major web development frameworks currently available.  I've settled on a very simple blogging system as my sample project, with a common MySQL backend.

<h3>Outline</h3>

I intend to evaluate one framework per language, as follows:
<ul>
  <li><strong>PHP:</strong> <a href=""http://www.cakephp.org/"">CakePHP</a></li>
  <li><strong>ASP.NET:</strong> <a href=""http://www.asp.net/mvc/"">ASP.NET MVC</a>.  <em>[Note: I may throw in some <a href=""http://msdn.microsoft.com/en-us/library/ms973868.aspx?ppud=4"">WebForms</a> and <a href=""http://quickstarts.asp.net/3-5-extensions/dyndata/ASPNETDynamicDataOverview.aspx"">Dynamic Data</a> too, depending on progress and whether it's suitable or not.  <a href=""http://www.hanselman.com/"">Scott Hanselman</a> did <a href=""http://www.hanselman.com/blog/PlugInHybridsASPNETWebFormsAndASPMVCAndASPNETDynamicDataSideBySide.aspx"">a good post that combined the three in a ""hybrid"" application</a>.]</em></li>
  <li><strong>Python:</strong> <a href=""http://www.djangoproject.com/"">Django</a></li>
</ul>

Depending on how I progress, I will also investigate the following secondary frameworks, in order of precedence:
<ol>
  <li><strong>PHP:</strong> <a href=""http://framework.zend.com/"">Zend Framework</a></li>
  <li><strong>Ruby:</strong> <a href=""http://www.rubyonrails.org/"">Ruby on Rails</a></li>
</ol>

<h3>Requirements</h3>

When I say the blog application will be simple, I <em>really</em> mean it.  Each implementation is a proof-of-concept experiment to attempt to get a handle on the frameworks, so it won't support formatting, picture uploading, or anything else even vaguely fancy.  It will support two different roles: reader (anonymous) and full admin (authenticated).  Posts will contain a title, body, publication date, and author's name/handle.  Comments will be anonymous with no verification, not least because there won't be any user management.  

<h3>Progress Reporting</h3>

I'll publish my evaluation on one or more <a href=""http://trac.edgewall.org/"">Trac</a> sites; my Trac index is available at <a href=""http://trac.alastairsmith.me.uk/"">.  I haven't decided whether or not to go to trouble of creating separate repositories and sites for each implementation yet.

Each framework will be the subject of one or more blog posts, describing current progress for that implementation, frustrations along the way, and neat features of the framework that have eased the implementation.  <strong>Update:</strong> You can find the whole series <a href=""http://www.alastairsmith.me.uk/category/coding/web-frameworks-evaluation"">here</a>, and there's a link to an RSS feed for the series on that page too.  

Please leave me a comment if you'd like to see something else feature in this experiment.  <strong>The only thing I won't add to is the list of frameworks</strong>; I think this is big enough already! :-)

I hope to get started this weekend.  Wish me luck!"
Escaping svn:externals,NULL,"I've started using <a href=""http://svnbook.red-bean.com/en/1.5/svn.advanced.externals.html"">svn:externals</a> definitions to manage my libraries and tools like NAnt and NUnit.  In the words of the <a href=""http://www.svnbook.com/"">SVN Book</a>:
<blockquote cite=""http://svnbook.red-bean.com/en/1.5/svn.advanced.externals.html"">The convenience of the svn:externals  property is that once it is set on a versioned directory, everyone who checks out a working copy with that directory also gets the benefit of the externals definition. In other words, once one person has made the effort to define the nested working copy structure, no one else has to bother—Subversion will, after checking out the original working copy, automatically also check out the external working copies.  

<cite>Ben Collins-Sussman, Brian W. Fitzpatrick, C. Michael Pilato (2002). <em>Version Control with Subversion</em>. Sebastopol, CA: O'Reilly Media. 155.</cite></blockquote>

This works well for me: my external dependencies are versioned by Subversion, and are automatically downloaded/updated with my working copy of the current project once the externals' definition has been created.  It's pretty painless.  I did hit one bump today, however, when setting up my repository for my web frameworks' evaluation, and here I'll detail how I got over it.
<!--break-->
Possibly foolishly, I had set up the ASP.NET MVC repository structure under a directory called ""ASP.NET MVC"".  I did my usual thing of creating my externals' definition in a file, ran <code language=""bash"">svn propset svn:externals ./ --file FrameworksEvaluation.externals</code>, and BAM!  It exploded on me:
<blockcode>
svn: Error parsing svn:externals property on '.': 'ASP.NET MVC/trunk/tools/nanth
ttp://svn.alastairsmith.me.uk/tools/nant'
</blockcode>

Bugger.  I tried a couple of things, namely escaping the '.' and the space with a backslash, and quoting the local relative path.  No luck: I just kept getting the same error.  After a bit of Google searching resulting in my landing <a href=""http://osdir.com/ml/version-control.subversion.tortoisesvn.devel/2005-08/msg00607.html"" title=""Re: can't set svn:externals property to reference a directory with a space: msg#00607 version-control.subversion.tortoisesvn.devel"">here</a>, I remembered something that the SVN book mentioned about externals' definitions: SVN doesn't use paths as such; instead it uses URLs.  <strong style=""text-decoration:line-through;"">URL-encoding the space as %20 fixed the solution.</strong>  It seems that <a href=""http://svn.haxx.se/dev/archive-2008-10/0213.shtml"">there's a fix for this ""bug"" in SVN 1.5</a>, so watch out if you're upgrading.

<h2>Update</h2>
Whilst it is true that URL-encoding spaces as %20 fixes the issue for the <code>propset</code>, when I ran <code>svn up</code> to download the external dependencies, I ended up with a new directory called ""ASP.NET%20MVC"".  Not what I wanted.  It appears this is <a href=""http://subversion.tigris.org/issues/show_bug.cgi?id=2461"" title=""Subversion Issue 2461:svn:externals doesn't support spaces in local path"">a known bug</a>, and further looks to be another instance of the bug I reported as having been fixed.  It looks as though I'm going to be upgrading to SVN 1.5 this weekend..."
An Evaluation of Web Frameworks II.i: Introduction to ASP.NET MVC,NULL,"ASP.NET MVC is still only a beta, although it has only recently gained this status from the previous ""Tech Preview"" releases.  The rumour is that it will RTM as part of .NET 4.0.

Despite the beta status, it has a pretty solid feel to it, and it's obviously quite stable: <a href=""http://www.stackoverflow.com"">Stack Overflow</a> was developed with this framework.  It integrates well with VS 2008 Pro SP 1, and promotes the use of <a href=""http://www.alastairsmith.me.uk/coding/2008/09/13/test-driven-development.html"">Test-Driven Development</a>, primarily through Microsoft's own Unit Testing framework (also known as MSTest).  However, it will support other frameworks such as my preferred <a href=""http://www.nunit.org/"">NUnit</a>.
<!--break-->
The first task I undertook was to get NUnit support for test projects integrated into VS 2008, mainly just to see how it was done.  A quick search on StackOverflow.com came up with <a href=""http://stackoverflow.com/questions/21137/aspnet-mvc-and-nunit"">this question</a> and after a bit of fiddling I'd managed to create myself the option of using NUnit over MSTest.  It was trickier than it probably should have been (including registry editing, yuck), but the blame, such as it is, should be laid at the door of NUnit as much as Microsoft.  I understand there are plans to include a VS 2008 template with NUnit 2.5, but I couldn't find anything along these lines in the most recent alpha release.  

With that completed, I began work on the project itself.  With the ASP.NET MVC framework installed, it presents a new option in the Visual Studio ""New Project"" dialog box:

[img_assist|nid=60|title=Figure 1|desc=Creating a new web project from Visual Studio|link=none|align=center|width=450|height=306]

[img_assist|nid=61|title=Figure 2|desc=Components of an ASP.NET MVC Solution|link=none|align=right|width=277|height=576]  Once the project is created (complete with Unit Test project), you end up with a Visual Studio solution with a number of folders and other items ready for use (see figure 2).  Let's go through what we've got there.

Firstly, I'll pick out the folders ""Models"", ""Views"" and ""Controllers"".  This being an <a href=""http://www.codinghorror.com/blog/archives/001112.html"">MVC</a> application, the division makes sense, and actually forces you think in those terms.  Under ""Models"" go your classes representing the data objects; you'll see I've got classes to represent each of posts, users and comments.  ""Views"" stores the HTML, in this case as ASPX pages representing templates for the various models.  The ""Controllers"" are a little trickier, but ultimately all they do is apply the right view to the right model.  In ""Controllers"" is where the clever stuff happens in an MVC web site, the butler to the Models and their clothes, the Views.  

It's also interesting to note that the default ASP.NET MVC project contains all the necessary workings for not just a homepage, but account management functionality (change password, register, and login) too.  In fact, you can hit F5 as soon as you've created the project and get a working website up in a browser, complete with (admittedly basic) CSS styling.  You do need to create an SQL Server database to get the account management functionality working off the bat.  

There are two ""secondary"" folders: ""Content"" and ""Scripts"".  These are fairly self-explanatory: ""Content"" holds things like images, CSS files, etc., whilst ""Scripts"" holds your AJAX and other client-side script files.  It's sad to note that the Beta release of ASP.NET MVC does not include the <a href=""http://weblogs.asp.net/scottgu/archive/2008/11/21/jquery-intellisense-in-vs-2008.aspx"">jQuery Intellisense</a> file by default, but maybe the <a href=""http://weblogs.asp.net/scottgu/archive/2008/02/08/vs-2008-web-development-hot-fix-roll-up-available.aspx"">hotfix</a> will resolve that.

What I like about this explicit MVC approach is that it seems to do a good job of separating concerns as far as is possible.  MVC is an important pattern for the web, given the nature of the layers of presentation and business logic that these sorts of applications involve.  It's got some traction at work, although we haven't adopted it as fully as we'd like.  It's very easy in web applications (more so than in desktop applications, it seems) to get the business logic horribly tangled up with the presentation which leads to unmaintainable code, so anything that improves on this situation can only be good.  

The Views (the .aspx files) can be mostly HTML with only the odd bit of C# (or VB if you prefer) to squirt in the necessary bit of data.  I don't know if ASP.NET MVC supports this sort of thing natively, but certainly with the integration with ASP Dynamic Data, you shouldn't even need to do loops to do things like displaying tables of information; instead, you just insert one or two lines of code and ASP will do the rest for you.  Complete with full paging support if required.  There's a bit of (seemingly fairly standard) ASPX at the top of the Views' files to refer to a master template/page, and ensure the 

The Models are just Plain Ol' Objects, business objects in their most real sense.  My Post object, for example, contains properties that map directly to the fields in the database, constructors, and methods for manipulating their state.  They don't need to concern themselves with how they save themselves to the database, or how to present themselves to the world at large.  They just <em>exist</em>.  

The Controllers, as I mentioned before, tie the Models and Views together, and manage external resources, like the database connection.  As such, they are the most complex bit of the application, but because they are separate from the Models and Views, they are much less complex than they might be.  Take, for example, the code to display the ""Create Post"" form:
<blockcode language=""csharp"">
public ActionResult CreatePost()
{
    return View(""CreatePost"");
}
</blockcode>

That's the framework covered... The next post will be on the actual implementation."
iPods,NULL,"I'm looking for a new iPod.  My current 5G 60GB iPod Video has served me well, but it's getting a bit beat up, and is missing some of the features of the newer models that I consider ""killer"" features.  Features like search.  So I'm in the market for a new one.  Note that I'm specifically looking at iPods; I'm not interested in other makes, as I have iPod-compatible accessories that I don't want to have to replace, and all my music is nicely categorised in iTunes, with most ""recent"" rips (i.e. a year or so old) completed in AAC.  I'm not prepared to change media player or faff about with my library to support a new MP3 player, so <strong>an iPod it is</strong>.
<!--break-->
However, I'm a bit stuck as to which model to get.  They all have pros and cons, and none of them seem to fit my needs as exactly as I'd like.  

<h2>iPod Classic</h2>
The iPod Classic is a good model and represents unrivalled value for money, and comes with an unmatched storage capacity of 120GB.  It's an update to my existing model, and so seems to be the most logical model to opt for.  I will be able to carry around my entire 35.5GB of music (and counting) and still have more than 80GB to spare for videos, recordings, and god knows what else.  

The storage capacity represents a bit of an issue for me, though; it's simply <em>far too big</em>.  I can't justify buying a model that size: the vast majority of it (and therefore the money paid for it) will go to waste, unused.  Even my current 60GB 5G iPod Video suffers 1/3 wastage.  The older iPod classic 80GB was much more in line with my needs, and slightly cheaper to boot, but you can't get it for love nor money now.  

<h2>iPod Touch</h2>
This is a lovely bit of kit, and multi-functional too, but really quite pricey.  I'd have to opt for the 32GB model (and still not be able to hold everything), and thus shell out a cool £250-300.  That's nearly double the cost of the iPod Classic.  And if I can't fit all my music on it, why not save a packet and go for...

<h2>iPod Nano</h2>
The new Nanos are very slick indeed.  They're incredibly lightweight in comparison with my 5G iPod, and have an impressive amount of storage for their size.  They also now support widescreen video!  Is there anything Apple can't do?  Oh yeah, hold any more than half of my music on one of these things: 16GB is the limit here.  And they cost nearly the same as the iPod Classic.  

The problem with not being able to cram on all my music is that I'd have to manage the synchronising of the device manually, and when you've got 6412 tracks that gets to be a bit of a headache (doesn't it?).  Maybe I'm just lazy :-)  Either way, it's not something I really want to be doing, so maybe I should just bite the bullet and buy 80GB of space I'm not going to use."
Downtime,NULL,"I've suffered a bit of downtime of late &mdash; about the last month, actually &mdash; which is why my blog has been unavailable and my web frameworks' evaluation has crawled to a halt.  Apologies to anyone that might have been following that series.  

The good news is, however, that I'm back for 2009 thanks to a particularly felicitous finding on my laptop that resolved the underlying problem.  I'm hoping to move at least my blog (if not some of my other bits and pieces) to a more reliable hosting package sometime this year.  363 days and counting to accomplish this!"
An Evaluation of Web Frameworks II.ii: Status Report,NULL,"It's been a little while since I made any progress on this little project of mine, due to the <a href=""http://www.alastairsmith.me.uk/random-stuff/2009/01/03/downtime.html"" title=""Downtime"">downtime</a> I suffered throughout December.  I couldn't get access to my virtual machine server, so I wasn't able to restart the VM hosting my blog and my Subversion repositories; everything went down together.  

As such, it's probably time for an update on where I've got to so far.
<!--break-->
I spent a fair bit of time getting the repository and the project set up correctly, including much shifting around of stuff, playing around with <a href=""http://www.alastairsmith.me.uk/coding/2008/11/22/escaping-svnexternals.html"">svn:externals</a>, setting up <a href=""http://ccnet.thoughtworks.com/"" title=""CruiseControl.NET Homepage"">CruiseControl.NET</a> for automated builds and testing, pulling out some of the default guff from the ASP.NET MVC project (and, of course, working out what was needed and what wasn't), etc., etc., etc., ad nauseam.  Over and above all that, however, I have spent some time defining my models.  I have classes representing Posts, Comments and Users, and I've made a start on my Controllers too.  The controllers house most of the logic, and so are the most complicated part to develop.  

I have already discovered, for example, through the few automated tests I already have, that I need to factor out my database logic into supporting code.  The tests for saving a post run fine on my development machine, but fail on my build server.  A bit of <a href=""http://www.alastairsmith.me.uk/coding/2008/09/13/test-driven-development.html"" title=""Test-Driven Development"">TDD</a>, partnered nicely with <a href=""http://www.alastairsmith.me.uk/coding/2008/08/24/nmock-framework.html"" title=""NMock Framework"">NMock</a> should (hopefully!) give me a decent data access layer that I can later mock again using NMock in my unit tests for the Controllers.  I also need to get myself more into the TDD mindset of writing the tests first and then writing the code to fix the tests.  

Discipline is everything when practising a new methodology.  Hopefully I'll have another post for this series in a few days or so.  We'll see!"
Casting types in .NET,NULL,"In Java, there's only one way to explicitly cast a variable from one type to another, and that's using the bracket syntax.  In .NET (well, C# anyway), there are a couple: the Java-esque bracket syntax and the <code language=""csharp"">as</code> keyword.  Whilst I have used both frequently in the past, a situation occurred a couple of days ago where I realised I needed to clarify what <code language=""csharp"">as</code> did under the hood.
<!--break-->
I was working through the Java examples of <a href=""http://www.alastairsmith.me.uk/coding/2008/09/13/test-driven-development.html"" title=""Test-Driven Development""><em>Test-Driven Development by Example</em></a> in C# using the MSTest framework included with Visual Studio 2008 when I came across some funny polymorphism and casting that seemed valid in the example but wouldn't compile in C#.  It later transpired that I'd missed a step in the super-class, but as I tried the two different methods of casting, I realised they were different and I wasn't sure how.  

In C#, the explicit cast bracket syntax is identical to that in Java:
<div style=""float: left; width: 50%;"">
<strong>Java</strong>
<blockcode language=""java"">
char x = 'x';
int y = (int) x; 
</blockcode>
</div>
<div>
<strong>C#</strong>
<blockcode language=""csharp"">
char x = 'x';
int y = (int) x;
</blockcode>
</div>

Under the hood, this also works the same way as it does in Java: it attempts to cast the one type to the other, and throws an exception if the types are not compatible.  

Both languages also support implicit casting, which allows you to do things like:
<blockcode language=""csharp"">
int x = 10;
long y = x;
</blockcode>

C# also provides an <code language=""csharp"">as</code> keyword for reference types; it won't work on value types like int, double, etc.  This is a second form of explicit casting, and it is used as follows:
<blockcode language=""csharp"">
Superclass superclass = new Superclass();
Subclass obj = superclass as Subclass;
</blockcode>

This is a nifty trick, providing a form of checked casting.  What happens when you use the <code language=""csharp"">as</code> keyword is that the type is cast as before, but if the cast to <code language=""csharp"">Subclass</code> fails, <code language=""csharp"">obj</code> is set to <code language=""csharp"">null</code>.  Under the hood, this is what happens:
<blockcode language=""csharp"">
expression is type ? (type)expression : (type)null
</blockcode>
(where <code language=""csharp"">is</code> is C#'s equivalent of Java's <code language=""java"">instanceof</code>).  Note that in this example <code language=""csharp"">expression</code> is only evaluated one for the entire statement.

This is rather basic C# stuff, but sometimes you get a situation where it helps to remind yourself what's going on behind the high-level constructs."
Web Frameworks Evaluation II.iii: Mocking Database Connections in Unit Tests,NULL,"One of the reasons progress on my web frameworks' evaluation has been so slow of late is that my last check-in broke my <abbr title=""CruiseControl.NET"">CCNET</abbr> automated build, and, as the good programmer I am, I felt that this needed fixing before I made any more progress.  

I'd written some unit tests for the <code language=""csharp"">PostController</code> (good) that were accessing the database directly (bad), and for whatever reason the build machine couldn't successfully connect to the database.  A classic case of <a href=""http://www.codinghorror.com/blog/archives/000818.html"" title=""The &quot;Works on my Machine&quot; Certification Program"">""works on my machine!""</a>, and, as is usually the case with such things, a good example of poor design.
<!--break-->
First off, my tests were reliant on an external resource to run.  This is not a good thing, as it increases the scope for things to go wrong (as I found out to my cost), and in such a way as to reduce the amount of control the developer has over his tests.  Doubly bad.  

From there, it also pointed to poor design.  The <code language=""csharp"">PostController</code> knew about the database, intimately.  There were methods liberally sprinkled with SQL; the PostController maintained a connection to the database, including a hard-coded connection string.  In other words, it was all horribly, horribly wrong.  

I quickly came to the conclusion that I needed a Data Abstraction Layer (DAL) that would allow me to mock the database in my unit tests.  My first port of call was to investigate Object-Relational Mapping (ORM) solutions.  As usual, my trusty <a href=""http://www.stackoverflow.com/"" title=""StackOverflow"">Stack Overflow</a> came to the rescue, with <a href=""http://stackoverflow.com/questions/tagged/orm"" title=""Questions tagged &quot;ORM&quot; on Stack Overflow"">a whole host of questions on ORM</a>; <a href=""http://stackoverflow.com/questions/66156/whose-data-access-layer-do-you-use-for-net"">""Whose Data Access Layer do you use for .NET""</a> and <a href=""http://stackoverflow.com/questions/146087/best-performing-orm-for-net"">Best Performing ORM for .NET</a> proved particularly instructive.  Frameworks like <a href=""http://www.nhibernate.org/"" title=""NHibernate homepage"">NHibernate</a>, <a href=""http://subsonicproject.com/"" title=""SubSonic homepage"">SubSonic</a>, and <a href=""http://www.castleproject.org/activerecord/index.html"" title=""Castle ActiveRecord homepage"">Castle ActiveRecord</a> all seemed very good, and particularly feature-rich, but they all seemed far too heavy-weight for my slim and floaty-light blog application.  They were all filed under ""E"" for ""experience"" and ""F"" for ""future"".  

One response to a Stack Overflow question suggested looking into the <a href=""http://martinfowler.com/eaaCatalog/repository.html"" title=""Repository Pattern from Patterns of Enterprise Architecture"">Repository Pattern</a>, as described by Martin Fowler in his book <em>Patterns of Enterprise Architecture</em>.  Like all good design patterns, the Repository Pattern describes a simple solution to a common problem.  A repository is provided for each data type stored (e.g., posts, comments, users, etc.), and an interface is defined for each repository providing basic <abbr title=""Create, Retrieve, Update, Delete"">CRUD</abbr> support.  This interface is then implemented for each concrete implementation required.  The original pattern seems to provide in-memory and persisted implementations, whereas I only really needed the persisted implementation.  

So, I defined my posts' Repository:
<blockcode language=""csharp"">
    public interface IPostRepository
    {
        Post GetPostById(int id);
        IList<Post> GetAllPosts();
        bool Save(Post post);
        bool Delete(Post post);
    }
</blockcode>

and implemented this interface on a <code language=""csharp"">MySqlPostRepository</code>:
<blockcode language=""csharp"">
    public class MySqlPostRepository : IPostRepository
    {
        private MySqlConnection db;

        public MySqlPostRepository() {
            db = new MySqlConnection(GetConnectionString());
            db.Open();
        }

        public Post GetPostById(int id)
        {
            var sql = ""SELECT * FROM Posts WHERE PostId = ?PostId"";

            if (id == Post.BLANK_POST.Id)
            {
                sql = @""SELECT * FROM Posts WHERE PostId = 
                        SELECT MAX(PostId) FROM Posts"";
            }

            using (var cmd = new MySqlCommand(sql, db))
            {
                var postIdParam = new MySqlParameter();
                postIdParam.ParameterName = ""?PostId"";
                postIdParam.Value = id;
                cmd.Parameters.Add(postIdParam);

                using (var results = cmd.ExecuteReader() as MySqlDataReader)
                {
                    var post = new Post();

                    if (results.Read())
                    {
                        post.Id = id;

                        post.Title = (string)results[""PostTitle""];
                        post.Summary = (string)results[""PostSummary""];
                        post.Body = (string)results[""PostBody""];

                        post.Author = (int)results[""PostAuthor""];
                        post.DateStamp = (DateTime)results[""PostDateTime""];

                        return post;
                    }
                }
            }

            return new Post();
        }

        ...
    }
</blockcode>

You can see <a href=""http://trac.alastairsmith.me.uk/FrameworksEvaluation/browser/ASP.NET_MVC/trunk/src/BlogApplication/Repository/PostRepository.cs?rev=14"" title=""PostRepository.cs at revision 14"">the full source code for <code language=""csharp"">MySqlPostRepository</code></a> in my <a href=""http://trac.edgewall.org/"" title=""Trac homepage"">Trac</a> <a href=""http://trac.alastairsmith.me.uk/FrameworksEvaluation"" title=""Trac environment for the Frameworks' Evaluation project"">environment</a> for the project.

This re-implementation led to a much nicer set of unit tests for the <code language=""csharp"">PostController</code>:
<blockcode language=""csharp"">
    [TestFixture]
    public class PostControllerTests
    {
        Mockery mockery;
        IPostRepository postRepos;

        private const string REPOS_GET_POST_BY_ID = ""GetPostById"";
        private const string REPOS_SAVE = ""Save"";

        [SetUp]
        public void SetUp()
        {
            mockery = new Mockery();
            postRepos = mockery.NewMock<IPostRepository>();
            
            Post newestPost = new Post();
            newestPost.Id = 10;
            newestPost.Author = 1;
            newestPost.Title = ""Test Post"";
            newestPost.Summary = ""This is a test post."";
            newestPost.Body = ""This is my latest post.  It is a test."";
            newestPost.DateStamp = new DateTime(2009, 1, 24, 17, 2, 4);

            Post post2 = new Post();
            post2.Id = 2;
            post2.Author = 1;
            post2.Title = ""Post 2"";
            post2.Summary = ""This is another test post."";
            post2.Body = ""This is my second post.  It is a test post.  It has an ID number of 2."";
            post2.DateStamp = new DateTime(2008, 08, 3, 18, 54, 42);
            
            Stub.On(postRepos).Method(REPOS_GET_POST_BY_ID).With(-1).Will(Return.Value(newestPost));
            Stub.On(postRepos).Method(REPOS_GET_POST_BY_ID).With(2).Will(Return.Value(post2));

            Stub.On(postRepos).Method(REPOS_SAVE);
        }

        ...

        /// <summary>
        /// More thorough test, that tests the default Post object (postId = -1)
        /// as a sanity check, and then reads a post from the database.
        /// </summary>
        [Test]
        public void TestReadViewData()
        {
            var controller = new PostController(postRepos);

            // Mock Post
            var result = controller.Read(-1) as ViewResult;
            var post = (Post)result.ViewData.Model;
            Assert.AreEqual(""Test Post"", post.Title);

            // Stored in the DB
            result = controller.Read(2) as ViewResult;
            post = (Post)result.ViewData.Model;

            Assert.AreEqual(2, post.Id);
            Assert.AreEqual(""Post 2"", post.Title);
        }

        ...

    }
</blockcode>

Sadly, my <code language=""csharp"">Save()</code> test is still being re-written as some of the test cases I had are no longer valid with the new architecture.  However, the <code language=""csharp"">TestReadViewData()</code> test provides a fairly instructive example.  

Before each test is run, the <code language=""csharp"">SetUp()</code> method is called, which creates two <code language=""csharp"">Post</code> objects, <code language=""csharp"">newestPost</code> and <code language=""csharp"">post2</code>.  These are ""mock"" objects for the most recently submitted post and the second post submitted respectively.  I use the term ""mock"" loosely - they're not mock objects in the usual sense of the word (as we'll see shortly), but they are fake posts that we expect to see returned from the mocked <code language=""csharp"">IPostRepository</code>.  The <code language=""csharp"">SetUp()</code> method then creates three stubs on <code language=""csharp"">IPostRepository</code>, two for <code language=""csharp"">GetPostById()</code> and one for <code language=""csharp"">Save()</code>.  The stubs expect <code language=""csharp"">GetPostById()</code> to be called with the argument -1 (a brand new post that hasn't been saved to the database yet, in which case it should return the most recent post, defined as the post with the highest ID), and with the argument 2 (which should return the specific post with ID = 2).  The stub on <code language=""csharp"">Save()</code> doesn't do anything at the moment, as per my previous note about the <code language=""csharp"">Save()</code> method.  As you can see from the example above, the NMock framework produces a very nice syntax for defining stubs and expectations, which I go into more detail about in <a href=""http://www.alastairsmith.me.uk/coding/2008/08/24/nmock-framework.html"" title=""NMock Framework"">my previous post on NMock</a>.

Note also that there is an element of <a href=""http://en.wikipedia.org/wiki/Dependency_injection"" title=""Dependency Injection on Wikipedia"">dependency injection</a> here, specifically constructor injection: the <code language=""csharp"">PostController</code> now needs to be passed an <code language=""csharp"">IPostRepository</code> at construction.  Dependency Injection is a form of inversion of control, and decouples <code language=""csharp"">PostController</code> from the <code language=""csharp"">IPostRepository</code> implementation, all Good Things&trade;.  This allows me to fully test the <code language=""csharp"">PostController</code>'s interactions with the <code language=""csharp"">IPostRepository</code> without requiring a database to be present.  What it can't do, unfortunately, is allow me to test the <code language=""csharp"">MySqlPostRepository</code> without a database.  That might need something more inventive, or just not be part of the <abbr title=""CruiseControl.NET"">CCNET</abbr> build.

My next step is to re-factor the <code language=""csharp"">IPostRepository</code> interface into a generic <code language=""csharp"">IRepository<T></code> interface so that I don't need to define a new repository interface for each data type.  Luckily, the interface is already generic enough (aside from method names) to allow me to accomplish this quite easily."
"Interview Questions for Employers, Part 2",NULL,"A little while back, <a href=""http://www.alastairsmith.me.uk/random-stuff/2008/11/21/interview-questions-software-development-interviewees.html"" title=""Interview Questions from Software Development Interviewees"">I posted up some interview questions <em>for</em> employers</a>.  You know, to ask at that awkward moment when the interview is basically over and the interviewer asks you if you have any questions for them.  I didn't receive a great deal of feedback to the post at the time, but my friend <a href=""http://www.analysisuk.com"">Steve</a> recently came across another blog post that provided a much more comprehensive list.

[img_assist|nid=70|title=|desc=|link=node|align=center|width=400|height=267]
<!--break-->
<a href=""http://www.basilv.com/psd/"" title=""Basil Vandegrind: Professional Software Development"">Basil Vandegrind recently posted a list of <a href=""http://www.basilv.com/psd/blog/2009/100-interview-questions-to-ask-employers"">100 Interview Questions to Ask Employers</a>.  100 seemed a bit over the top to me at first glance, until I read the intro to his post:

<blockquote>
<strong>Warning: Do not simply ask the interviewers these 100 questions in the first interview!</strong> Your first priority in an interview process is to convince the interviewers to hire you by selling yourself. The important but secondary priority is to determine that the company is a good fit. It does not matter how good a fit the company is if they will not hire you &hellip; I would not recommend asking all of these questions directly of the interviewers, especially those from the human resources department, as they likely do not know what the specific working conditions are like and may sugar-coat the facts. I recommend instead talking directly to a few regular employees, preferably in a neutral setting outside of the office.
</blockquote>

So, in short, don't wander in with your four sides of A4 and reel them off one at a time! :-)  In fact, you will be able to find some of your answers in other places: the majority of the ""Organisation"" questions Basil poses can be answered by reading the company's annual report, for example.  <strong>It's always a good idea to read the annual report before going for an interview</strong> (it's an invaluable source of all sorts of information), and, if you're not sure about financial measures, get someone who is to help you out.  The last thing you want to do is get a dream job at a fantastic company that is going to go out of business in 6 months' time.  Publicly-trading companies (<abbr title=""Public Limited Company"">PLC</abbr> in the UK, <abbr title=""Incorporated"">Inc.</abbr> in the US) are required by law to make their financial statements public; in this day and age, they will more than likely be posted up on the company's website under ""Investor Relations"" or ""Company Information"" or similar.  If you do struggle to get hold of a copy of the annual report (for example, if the company is small and privately held), ask at your first interview for a copy; it shows interest in the company beyond the narrow focus of your own role, and will likely impress the employer.  Even with the annual report at your fingertips, you may want some clarification of some points.  <strong>Be sure that you target these at the right person</strong>; the technical lead interviewing you is not going to be interested (or, more likely, even <em>able</em>) in answering these sorts of questions.  

Many of the remainder of the questions fit better into the ""talk to employees at the pub"" category, and the 101st bonus question Basil lists will get you a long way towards a situation where you can ask these questions.  Interestingly, Basil refers his readers to <a href=""http://www.joelonsoftware.com/articles/fog0000000043.html"">The Joel Test</a> I mentioned in my previous blog post on this subject, and also <a href=""http://www.codinghorror.com/"" title=""Coding Horror"">Jeff Atwood's</a> excellent <a href=""http://www.codinghorror.com/blog/archives/000666.html"">Programmers' Bill of Rights</a>.  Both are well worth a read.  However, I'd like to use the remainder of the post to pick out a few of Basil's suggestions that I think are particularly important, and why.
<ol>
  <li value=""9""><strong>What is your strategy for empowering employees?</strong>  Employees that feel under-valued and under-developed are unproductive and unhappy.  Empowerment is about giving employees the means (knowledge, political power, whatever) to do their job, and do it to the best of their ability.  As an interviewee, you need to know that your employer will invest in your training, and will not micro-manage you or ignore you.  </li>
  <li value=""16""><strong>How do you minimise interruptions for developers?</strong>  Developers, like many other professionals, ""get into the zone"".  This is when they are most productive and produce their best work.  It takes time and effort to get to that mental state, but once obtained it is easily broken.  Telephone conversations, <a href=""http://www.analysisuk.com/blog/archives/50-Open-Plan-Offices-What-a-HUGE-Mistake..html"" title=""Open Plan Offices &ndash; What a HUGE Mistake"">noisy open plan offices</a>, and many other factors will interrupt a train of thought or break a developer's concentration, dropping him out of the zone, or preventing him ever reaching it.  There's even <a href=""http://www.news.com.au/business/story/0,27753,24906913-5017672,00.html"" title=""Open-plan offices are making workers sick, say Australian scientists"">some evidence to suggest that open-plan offices are bad for your health</a>.  Sadly, though, open-plan offices are the norm these days, so you will be <em>incredibly</em> lucky to find a company offering private offices, but beware companies that put sales guys in the same room as developers.  </li>
  <li value=""33""><strong>What opportunities will there be to work with new, interesting technologies?</strong>  A developer's greatest asset is his knowledge; it's the main reason he is hired.  Learning new technologies drives innovation within the company, and keeps developers' skills fresh and up-to-date.  Companies should be careful not to lock developers into their existing role, however, as they will then be harder-pressed to adapt to changes that might occur at a later date.  Role-specific knowledge is great whilst within that particular role, but pretty useless outside of it.  <a href=""http://www.lhotka.net/weblog/DSLsNdashFunCoolButMaybeABadIdea.aspx"" title=""DSLs &ndash; fun, cool, but maybe a bad idea?"">Rocky Lhotka's article on Domain-Specific Languages</a> makes a pretty compelling argument against <abbr title=""Domain-Specific Languages"">DSLs</abbr> on account of the lock-in effect they can have on the company, the product and the developers themselves, but the ideas apply to more general situations too.  </li>
  <li value=""39""><strong>Are high quality chairs provided?</strong>  Comfortable <em>and fully-adjustable</em> chairs are an absolute must, as I've found out the hard (and painful!) way.  High-quality chairs do not come in cheaply (check out <a href=""http://www.google.co.uk/products?q=herman+miller+mirra"">the range of prices charged for the Herman Miller ""Mirra"" model</a> that I have at work), but they are worth the investment.  High-quality chairs last much longer than entry-level models and keep you more comfortable throughout the day.  You will regret spending 30 hours a week in an uncomfortable chair, and it will have knock-on effects for your health.  From the company's perspective, an uncomfortable developer is an unproductive developer, and they are more likely to take time off because of back/neck problems, suffer from RSIs, etc., whilst using a cheap chair.  </li>
  <li value=""45""><strong>Will I have the freedom to install the tools I want on my workstation?</strong>  Again, this ties into developer productivity.  Every developer has their own favourite Notepad replacement, their own preferred IDE (or Emacs), and they will be sufficiently practised with these tools to have achieved an excellent level of productivity.  If you lock down your developers' workstations, or require them to use specific tools, then you are taking away some of their freedom.  You are giving a left-handed person a right-handed pair of scissors; a carpenter a spanner.  Sure, they can still do the job, but there are better options available.  Money should be spent on buying the best tools available, too; the best tools will pay for themselves many times over in increased productivity.  </li>
  <li value=""47""><strong>Do projects have realistic schedules, resources, and scope that are actively managed and adjusted?</strong>  No one likes a project that over-runs, suffers from requirements' creep, or is under-staffed.  You want to work for an organisation where projects are efficiently managed.  Take time to learn a little about project management methodologies so you can effectively evaluate the interviewing company.  </li>
  <li value=""54""><strong>What development methodologies do you use? Describe how they are put into practice.</strong>  If you prefer the up-front style of the waterfall model, you might struggle with an Agile environment, and vice versa.  Feel free to get into specifics, such as <a href=""http://www.alastairsmith.me.uk/coding/2008/09/13/test-driven-development.html"" title=""Test-Driven Development""><abbr title=""Test-Driven Development"">TDD</abbr></a> or <abbr title=""Domain-Driven Design"">DDD</abbr>, if there are areas of particular interest.  </li>
  <li value=""75""><strong>How much paid training do you provide to each employee per year?</strong>  This relates to the question of empowerment earlier in the list.  If a company does not provide much training for their developers, the skill sets of their employees will become out-dated and the company will find it harder to adapt to new trends and technologies.  Equally, developers with out-dated skills will find it harder to get a new job when leaving, and so they might choose to move on sooner.  A good company will invest in their developers' skills to retain the best minds and safe-guard the company's own future.  </li>
  <li value=""99""><strong>What open source software do you support? What form does this support take?</strong>  This one interested me, as it's not one I've ever thought to ask before.  Probably more companies than you might think contribute to open source software in some form; even Citrix does so now, following our acquisition of XenSource.  This shows good community awareness, and, to an extent, good business smarts too, as most software thrives around a community these days.  The company doesn't necessarily have to contribute code to an open-source project; it could be that a company has a particularly high presence in the project's community because they use it a great deal themselves.  For example, a company using <a href=""http://ant.apache.org/"" title=""Apache Ant homepage"">Ant</a> a great deal may give something back to the Ant project by maintaining a good presence on the support lists, or donate a modest annual sum.  </li>
  <li value=""100""><strong>Who are you competing with locally for recruiting software developers? Who are you losing developers to?</strong>  This is important, as it provides a basis for comparing the role for which you're interviewing, and might also give you ideas of where else to apply for a job!  </li>
</ol>"
SOLID Principles of OOD,NULL,"Sadly for all you <a href=""http://www.macmillandictionary.com/new-words/050425-Whovian.htm"" title=""Definition of &quot;Whovian&quot;"">Whovians</a> out there, I don't mean <em>those</em> Ood.  Instead, I want to talk about the SOLID Principles of Object-Oriented Design, as proposed by ""Uncle"" Bob Martin back in the mid-1990s.  The SOLID Principles gained a level of notoriety in the .NET community a couple of months ago, after <a href=""http://www.codinghorror.com/"">Jeff Atwood</a> and <a href=""http://www.joelonsoftware.com/"">Joel Spolsky</a> made some, er, <em>unguarded</em> comments about them in <a href=""http://itc.conversationsnetwork.org/audio/download/ITC.SO-Episode38-2009.01.20.mp3"">one of their StackOverflow podcasts</a>.  This post is long overdue, so I'm glad to finally get it out there! :-)

[img_assist|nid=71|title=|desc=|link=node|align=center|width=400|height=233]
<!--break-->
The SOLID Principles of OOD are a bunch of guidelines for developing object-oriented systems.  They can be interpreted rigidly, and indeed religiously, or they can be applied in a more relaxed fashion.  The key, as with any of these principles and practices we learn, is to stick to them in general, and know them well enough to be able to correctly judge when to bend them.  You can find the full definitions of the SOLID Principles in Uncle Bob's <a href=""http://butunclebob.com/ArticleS.UncleBob.PrinciplesOfOod"" title=""Uncle Bob's Principles of OOD"">original blog post</a>; alternatively, you can hear it straight from the horse's mouth in <a href=""http://www.hanselminutes.com/default.aspx?showID=163"" title=""SOLID Principles with Uncle Bob &mdash; Robert C. Martin"">episode #145</a> of <a href=""http://www.hanselminutes.com/"" title=""Hanselminutes"">Hanselminutes</a>.  I'm just going to summarise the five main principles of class design; you may be surprised to see that they're actually pretty simple, and that you're probably already following a number of them unwittingly.  Take a look:

<dl>
  <dt><strong><a href=""#SRP"">Single Responsibility Principle</a></strong></dt><dd>A class (or method) should only have one reason to change.</dd>
  <dt><strong><a href=""#OCP"">Open Closed Principle</a></strong></dt><dd>Extending a class shouldn't require modification of that class.</dd>
  <dt><strong><a href=""#LSP"">Liskov Substitution Principle</a></strong></dt><dd>Derived classes must be substitutable for their base classes.</dd>
  <dt><strong><a href=""#ISP"">Interface Segregation Principle</a></strong></dt><dd>Make fine grained interfaces that are client specific.</dd>
  <dt><strong><a href=""#DIP"">Dependency Inversion Principle</a></strong></dt><dd>Program to the interface, not the implementation.</dd>
</dl>

<h4 id=""SRP"">The Single Responsibility Principle</h4>
The crux of this principle is that not only should a class encapsulate an idea, such as a blog post, but that encapsulation should be quite restricted to ""real"" properties and behaviours of that idea.  This means that all ancillary operations related to the idea should be split out into separate classes.  Using the blog post example, your blog post class might have a <code>SaveToDatabase</code> method and a <code>RenderHtml</code> method.  A change in the database schema or display layout would result in a change in the class.  This is in violation of the Single Responsibility Principle, because <strong>these two methods change for two different reasons</strong>.  <code>SaveToDatabase</code> more properly belongs in a <a href=""http://www.alastairsmith.me.uk/coding/2009/01/24/mocking-databases.html"" title=""Mocking Databases"">Repository class</a>, and <code>RenderHtml</code> in the View part of a Model-View-Controller pattern.  

<h4 id=""OCP"">The Open Closed Principle</h4>
Software modules should be open for extension and closed for modification.  Stop and think about that for a moment.  <strong>Open for extension and closed for modification.</strong>  What does that even mean?!

Well, to my mind, it essentially means that you should treat your modules (classes, methods, assemblies, whatever) as black boxes <em>unless you are working directly on that module</em>.  If you have a <code>Foo</code>, and you want to extend it to make a <code>FooBar</code>, you shouldn't have to do anything to <code>Foo</code> to get your <code>FooBar</code>.  This comes down to writing good, clean interfaces to your modules, and utilising polymorphism to its fullest extent.  For example, you might have an <code>Account</code> base class; you could then use polymorphism to define, manipulate, and otherwise work with various accounts such as <code>CurrentAccount</code> (with an overdraft facility), <code>CreditCardAccount</code> (with a credit limit), etc.  <code>Account</code> is marked as abstract, and defines the core functionality of a financial account: recording transactions.  The subclasses modify the abstract base class' behaviour by introducing a kind of limit on the balance: the overdraft or credit limit.  <strong>You add new features by adding new code, not by changing existing, tested code.</strong>  How's that for robust?

<h4 id=""LSP"">The Liskov Substitution Principle</h4>
This follows on nicely from the Open-Closed Principle.  Given an abstract class or interface, anything that refers to it should be able to handle one of its derived classes.  It's sort of another way of saying that ""a rose is a rose is a rose is a rose"".  That's all a bit of a mouthful, so let's try an example.  

If you drive a Ford, you should also be able to drive a Toyota.  Or a Hyundai.  Or a Volkswagen, Mercedes or Ferrari.  A car is a car is a car is a car: <strong>given the abstract class of ""car"", you should be able to drive that car, no matter what make and model it is.</strong>  

A good metaphor for the Liskov Substitution Principle not being followed is Microsoft Office.  For many years, they got by with the menu-based system.  With the release of Office 2007 they introduced a new Ribbon interface.  Suddenly people didn't know how to use Office 2007.  A similar thing happens with new release of Windows: try finding anything in the Vista Control Panel if you're used to XP (the same applies to the XP Control Panel if you're used to Windows 98, 2000, etc.).  In order for these new interfaces to be adopted, a form of the Liskov Substitution Principle has to be applied; hence Microsoft's efforts to get third-party developers using the Ribbon and the similarity between Vista's Control Panel and Windows 7's.  

How does this apply to your code?  Well, is a square a rectangle?  Your instinct might be to say ""yes, it's a special case of the rectangle""<sup>*</sup>.  That's not strictly true, though: what does it mean to talk of a square's height and width as opposed to a rectangle's height and width?  When implementing squares and rectangles in code, the rectangle should rather be a generalisation of the square than the square a specialisation of the rectangle, otherwise your subclasses do not behave like their superclass.  

<h4 id=""ISP"">The Interface Segregation Principle</h4>
This principle applies to very fat classes, i.e., with many tens or hundreds of methods, and draws on the theme of <a href=""#SRP"" title=""The Single Responsibility Priniciple"">the Single Responsibility Priniciple</a>.  Very often, only subsets of the members of classes like these are used at any one time, but because they're all defined in the same class, a client of that class may need to change because part of the fat class has changed (often even if it's an unrelated part of the fat class).  The idea here, then, is to <strong>define those subsets of methods as interfaces, and have the fat class implement each interface</strong> (or, ideally, split the subsets into smaller classes implementing those interfaces).  You can then pass around references to these interfaces (see <a href=""#DIP"" title=""The Dependency Inversion Principle"">the Dependency Inversion Principle</a>, below) rather than the full fat class, meaning that you only have available the methods that are relevant to that particular usage of the class.  It's a bit like partitioning the class according to function, with the option of easier re-factoring in the future.  

<h4 id=""DIP"">The Dependency Inversion Principle</h4>
In Uncle Bob's own words, this is like a restatement of the Open-Closed Principle with a 90-degree rotation.  Put simply, this says ""depend only on things which are abstract"", what I have called in the past, ""interface programming"" or ""programming to the interface"".  In essence, you should not rely on any concrete implementations of any classes, be they your own or framework objects.  For example, in the .NET framework, there are a number of List classes: <code>List</code>, <code>LinkedList</code>, etc.  If you declare a class member of type <code>List</code>, and then realise that a <code>LinkedList</code> would be better (performance reasons, whatever), you have to go through and change all <code>List</code> declarations that may handle your list.  If instead you had referred to it as an <code>IList</code> throughout, all you'd need to do is change the line(s) that construct the list object.  If you realise that a <code>List</code> is <em>far</em> too specific, an <code>IEnumerable</code> or even <code>ICollection</code> would be a better choice.  

This is one of the best lessons I learnt during my internship at Citrix back in 2006, and it's stayed with me ever since.  <strong>Using interfaces greatly simplifies your code, because you end up dealing with a number of Foo-shaped boxes into which you can slot any form of Foo you like.</strong>  If I wanted a Collection-shaped box, I could use any form of Collection I liked, be it List or not.  

<h4>Conclusion</h4>
The sum of these principles is a set of guidelines that will help you write good, clean, testable code.  Give them a whirl in your next coding task, and see what you make of them.  Then leave me a comment!

<hr />

<sup>*</sup> I made this mistake in an interview once.  Oops."
Search in Windows Vista,NULL,"I know I'm way behind ""the curve"" blogging about a new feature in Windows Vista, and that it's seriously old news these days, but I wanted to do a quick post on the search functionality that was introduced with Vista.  As such, this isn't going to be a post trying to sell the feature to you (as it might be if the feature were still new), but more an indication of how I use it on a day-to-day basis, and why.  This post will be less technical than some of my usual posts to make it accessible to a wider audience.
<!--break-->
First off, ever since I was at college back in the very early noughties, using Windows 98 and 2000, I've been a big fan of the ""Run"" dialog.  In later versions of Windows, you can press [Flag]+R to bring this up; alternatively, it's right there in the Start Menu.  For years, I've been hitting [Flag]+R and typing ""winword"" to fire up Word rather than trying to find it in the Start menu.  This is part of my developer mindset: at my computer, my keyboard is my home, and I touch the mouse as little as possible.  <a href=""http://www.asktog.com/TOI/toi06KeyboardVMouse1.html"" title=""AskTog: Keyboard vs. The Mouse, pt 1"">The keyboard is consistently faster</a>, and <strong>I would urge you to <a href=""http://www.codinghorror.com/blog/archives/000825.html"" title=""Coding Horror: Going Commando - Put Down The Mouse"">put down your mouse</a></strong>, particularly if you're a laptop user.  The one exception is web-browsing; I'm still fairly mouse-focussed in this activity, particularly in my use of the context menu ""Back"" function instead of the ""Back"" button, but not to the extent of many users.  

What does this have to do with search?  Whilst Mac OS X has had desktop search (named ""Spotlight"") since Tiger shipped in 2005, Windows took a little bit of time to catch up as Vista became delayed.  Google Desktop Search filled the gap in the market somewhat on Windows XP, but the integration has never been as good as it was in OS X.  The Vista search box in the Start Menu (accessibly by pressing [Flag] only) resolves that issue, <em>and doubles as the Run dialog</em>.  Run is still available separately, however; I assume Microsoft identified a separate set of use cases for this feature.  

All these search systems are dependent on an indexer running in the background, which scans selected files and directories and stores a trimmed-down version of their content and metadata (data describing the files and directories, such as name, size, dates, etc.) for fast access.  The indexer is similar to the spidering process that Web search engines like Google use to discover web pages, images, etc., but it is more constrained than this as it's only seeking information in the directories specified and not actively seeking new stuff wherever it might be.  Vista also indexes your Start menu.  

The upshot of the indexer is that <strong>searching your hard disk is near-instantaneous.</strong>  Typing a search query into my search box in Vista returns results straight away, and it's a matter of just a few seconds before the list of results is complete.  Emails, documents, pictures, music, videos are all indexed and may appear in results.  If I have a photo of <a href=""http://en.wikipedia.org/wiki/Johann_Sebastian_Bach"" title=""JS Bach on Wikipedia"">JS Bach</a>, MP3s of Bach Keyboard Partitas, and a paper written on Bach, and an email conversation regarding the proof-reading of that paper, each of those items will be returned in the list of search results for ""Bach"".  

Sadly, at work, I can't get the full benefit of the search feature because of our corporate systems, but I've really started getting used to it at home.  I no longer know where to find anything in the list of programs in my Start menu any more, as I just search for it.  I don't have to click Start -> All Programs -> iTunes -> iTunes to fire up iTunes; I just hit [Flag] and start typing ""iT&hellip;"".  By the time I've typed those two letters (and I'm a fast typer :-) iTunes has appeared as the top hit.  

Search is built right into the Vista operating system, so if you're a Vista user, you've probably used it in one form or another.  Next time you're rooting around for a file, try using the Start menu search function, or hit Ctrl+E in an Explorer window and see what it turns up."
Windows 7,NULL,"I recently upgraded my main PC at home to the public beta (build 7000) of Windows 7.  Whilst it's no longer available from Microsoft, I'd downloaded this during the public availability phase with the intention of setting it up in a Virtual Machine.  The public beta and later builds are available if you know where to find them, and build 7000 is still available to MSDN subscribers.  This post covers my experiences with Windows 7 so far.
<!--break-->
I started off by attempting an upgrade of my existing Windows Vista installation.  This appeared to go ok after requiring a restart to release file locks on system files, but later failed.  Sadly, I didn't manage to work out why this was, so I'm not able to offer any solutions to other people encountering this, nor submit the appropriate feedback to MS.  I then went ahead with a clean installation (obviously, after backing up), but didn't format the disk first.  As such, I have my old Vista installation still present but unusable in a folder called Windows.old.  

The <span style=""text-decoration: line-through"">first</span> second thing you notice is how much like Vista Windows 7 is.  The actual first thing you notice is <a href=""http://www.youtube.com/watch?v=2m1OeK7ZJIs"" title=""YouTube - Extracted Windows 7 Beta Boot Animation From Build 7000"">the jaw-droppingly beautiful boot animation</a>.  This is one of those little changes that really puts the gloss on the finished product, and given that the Vista boot animation was still pretty poor for a modern OS (640x480x16pp!!), it's good to see this finally brought into the 21<sup>st</sup> century.  The Microsofties have managed to get <a href=""http://blogs.msdn.com/e7/archive/2009/02/18/engineering-the-windows-7-boot-animation.aspx"" title=""Engineering Windows 7 : Engineering the Windows 7 Boot Animation"">an entire blog post</a> out of the new animation (with an appropriately mixed bag of comments!).  <strong>Health warning: it contains words like ""bioluminescence"" and ""organic""</strong>.  I'm not going to do the same, you'll be pleased to hear!

<object width=""400"" height=""324"" style=""text-align: center""><param name=""movie"" value=""http://www.youtube-nocookie.com/v/2m1OeK7ZJIs&hl=en&fs=1&rel=0""></param><param name=""allowFullScreen"" value=""true""></param><param name=""allowscriptaccess"" value=""always""></param><embed src=""http://www.youtube-nocookie.com/v/2m1OeK7ZJIs&hl=en&fs=1&rel=0"" type=""application/x-shockwave-flash"" allowscriptaccess=""always"" allowfullscreen=""true"" width=""425"" height=""344"" style=""text-align: center""></embed></object>

So back to similarities with Vista.  Vista was sold by Microsoft as a ""platform release"", that is to say, it's a new Windows platform, unlike anything gone before it, and it would form the basis for future versions of Windows.  That is evident with Windows 7, as the installation experience is identical to Vista's, allowing for the necessary string changes.  Once Windows 7 is installed, you're greeted with the Vista configuration, but with a new colour scheme; whilst Vista was greeny-blue (dare I say turquoise?), Windows 7 is most definitely a deeper blue.  It also seems visually sharper, if that's possible (I'm not using an HD monitor or connection).  Fonts, window layouts, etc., are as they were in Vista.  
There are other visual tweaks, too: whilst maximised windows in Vista turned the title bar from a glassy transparency to an opaque black, in Windows 7 they remain transparent.  The same modification applies to the task bar.  

And there's the task bar itself.  This is perhaps the biggest, most obvious new feature of Windows 7, and how Microsoft haven't landed themselves a patent-infringement lawsuit with Apple, I don't know.  The new task bar is very similar to OS X's dock.  It's not an exact copy, but the style is the same.  As in previous versions of Windows (XP upwards), windows are grouped by application to reduce the amount of space occupied in the task bar.  However, now the task bar only displays a large icon, not a trimmed window title too.  A subtle (but not <em>too</em> subtle) ""stacking"" effect is used to indicate that there are multiple windows open in that application.  

[img_assist|nid=76|title=The Windows 7 taskbar|desc=Not unlike the Apple Dock...|link=none|align=center|width=400|height=32]

[img_assist|nid=77|title=|desc=|link=none|align=left|width=53|height=72]  The Windows Pearl (or Orb, as it was called in Vista) acts as before, providing access to the Start Menu.  Unlike in Vista, it fits within the task bar rather than protruding from the top (an effect I quite liked).  Given this, it's now a little hard to identify amongst the application icons - it's only about two pixels larger in each axis than the green <a href=""http://www.spotify.com"" title=""Spotify Music Streaming Service"">Spotify</a> icon in the screenshot above.  A nice new effect, however, is the way the Pearl ""crackles"" visually when it's clicked (see left).  

Frequently-used applications can be ""pinned"" to the task bar, providing ""quick launch"" functionality that has been in Windows for a little while now, albeit sometimes disabled by default.  Right-clicking the icon can provides a number of other options, such as pinning recent or specified items of content to the application and giving basic control of the application (e.g., next, play, etc.).  

Window management has been tweaked a little.  For example, you can now opt to maximise a window in the vertical axis only by dragging it to one side of the screen or the other; when the mouse encounters the screen edge, the cursor ""pulses"" and a preview is shown via Aero Peek.  

Aero Peek is another of the new features, and comes in two halves.  First, hovering the mouse cursor over an application's icon on the task bar provides a good-sized preview of all the windows opened with that application.  This preview is actually sizeable enough to be able to identify separate instances of an application by their content, and so is actually useful.  

The second half of Aero Peek works as follows.  Hovering over a window preview will display the window's location on the screen by fading out all the other windows to an outline; each window retains a glassy sheen to them, which is more noticeable for background maximised windows.  These effects are played out via a short and slick animation (seemingly a fade-in/fade-out transition mostly).  This is quite useful for locating windows, but I've found it actually quite hard to get used to.  This may be because I'm always swapping between Vista (at work) and Windows 7 (at home), but my instinct is to go for the revealed window with the mouse rather than simply clicking as I should.  The problem is that by the time the mouse cursor has got to the revealed window, it has disappeared behind the other windows again!

[img_assist|nid=78|title=Aero Peek|desc=Three windows are open in total, with the Firefox window being the subject of the ""peek"".  The other two are Vuze and the Outlook reminders window.|link=none|align=center|width=400|height=250]

Paint and Calculator have been given an overhaul to bring their UI into the 21<sup>st</sup> century: Paint now sports the Ribbon interface that made its debut in Office 2007, and uses PNG as the default file type (replacing JPEG in Vista and bitmap in all previous versions).  

Performance- and stability-wise, I've found Windows 7 to be incredibly stable.  I think I have had just one crash so far (caused by installing a virtual DVD drive), and the only on-going problems I've encountered are a couple of niggling driver issues with my graphics card and sound card.  It also appears to be much more performant than Vista; for example, my current memory usage (whilst running iTunes, Outlook, Firefox and Vuze) is a smidgeon over 1GB, 50% of my total RAM and about 66% of what Vista used.  This bodes well for the netbook support Microsoft are offering with Windows 7 (primarily to get everyone off XP).

There are so many other new things to cover that I simply can't fit them all into this blog post without making it any more epic than it already is.  There is, for example, <a href=""http://www.google.com/search?q=windows+7+multi-touch"" title=""Google search results for &quot;Windows 7 Multi-Touch&quot;"">multi-touch and gesture support</a>, <a href=""http://www.google.com/search?q=windows+7+jump+lists"" title=""Google search results for &quot;Windows 7 Jump Lists"">jump lists</a>, the inclusion of PowerShell 2.0 (it's even enabled by default!) and <a href=""http://www.google.com/search?q=windows+7+homegroup"" title=""Google search results for &quot;Windows 7 HomeGroup&quot;"">HomeGroups</a>.  I've aimed to give an overview of the most interesting (to me!) new stuff that hopefully will inspire you to try it out for yourself."
PowerShell -whatif,NULL,"PowerShell is Microsoft's new command line shell for Windows.  Version 1 was released in 2006, and version 2 has been in <abbr title=""Community Technology Preview"">CTP</abbr> for about a year now; it will finally RTM with Windows 7.  It's a truly fantastic shell, and if you haven't looked into it already, I would urge you to do so.  There's a wealth of information out there, not least on <a href=""http://blogs.msdn.com/powershell/"">the PowerShell blog</a>, that will get you started very quickly.  

I'm such a fan of PowerShell that I'm quite surprised I haven't posted about it here in any detail before.  One of the key advantages is that it is a first-class .NET language, giving you access to the entire .NET framework from the command-line; this arises from the basic data item being an object (rather than plain text as it is in UNIX shells).  I hope to post an introduction/tutorial to PowerShell sometime in the near future.  

The subject of this post, however, is a specific feature of PowerShell, the <code>-whatif</code> switch.  I assume familiarity with basic PowerShell features, and the PowerShell syntax.
<!--break-->
Recently at work, I was writing some PowerShell scripts to automate some XML manipulation.  Unfortunately, I got the script a bit wrong, and so the XML manipulation didn't work as expected.  This could have been avoided had I not forgotten about <code>-whatif</code>.  Additionally, I was running the script in a sandbox location, which meant duplication of files and directory structure; this too could have been avoided if by using the -whatif switch.  

<a href=""http://computerperformance.co.uk/ezine/ezine157.htm"" title=""Ezine 157 - PowerShell's WhatIf"">More of a magic shield than a magic bullet</a>, <strong><code>-whatif</code> does a dry run of your script</strong>.  Supplying <code>-whatif</code> to your command line runs the script <em>exactly as it would be run</em>, but without making any actual modifications.  As such, it is only valid on cmdlets that make modifications; supplying it to a get-* cmdlet will throw an error.  

What follows is a slightly contrived example, but bear with me :-)  Say my log directory (e.g., <code>C:\logs</code>) was getting a bit full and hard to manage, I might want to delete all old log files, where ""old"" means more than one week old.  The following code looks good, but there's a not-so-subtle error in it.  

<blockcode>
Get-ChildItem -Filter *.log C:\logs |? { 
    $_.LastWriteTime -gt (Get-Date).AddDays(-7) 
} | Remove-Item
</blockcode>

Running this one-liner (formatted to reduce scrolling) as-is will cause a huge screw-up: I've got the logic wrong for the date comparison, and suddenly I've deleted all my most recent log files.  

<blockcode>
Get-ChildItem -Filter *.log C:\logs |? { 
    $_.LastWriteTime -gt (Get-Date).AddDays(-7) 
} | Remove-Item -whatif
</blockcode>

Adding that one flag does a dry-run, and gives me the following verbose output:

<blockcode>
What if: Performing operation ""Remove File"" on Target ""C:\logs\270209.log"".
</blockcode>

""Wait a minute"", I think, ""that file is dated with today's date.  I must have got the logic wrong!""  And so my ass and my log files are saved.  Flicking the comparison from a <code>-gt</code> to a <code>-lt</code> and leaving the <code>-whatif</code> switch enabled confirms my suspicions, and I now have the confidence to let PowerShell remove those old log files for me.  

This is a really important and useful switch, and if you write PowerShell artefacts (scripts, cmdlets, etc.) that you share with others, <strong>you really should support this switch</strong>.  <a href=""http://blogs.msdn.com/powershell/archive/2007/02/25/supporting-whatif-confirm-verbose-in-scripts.aspx"" title=""Windows PowerShell Blog : Supporting -Whatif, -Confirm, -Verbose &mdash; In SCRIPTS!"">And don't forget to implement the other standard switches</a> while you're at it.  

Now all I need to do is train myself to test out my PowerShell artefacts with this switch before I run them for real."
Windows 7 Aero Snap,NULL,"In my recent <a href=""http://www.alastairsmith.me.uk/science-and-technology/2009/02/24/windows-7.html"" title=""Windows 7"">blog post on the Windows 7 beta</a>, I alluded to the new window management features in Windows 7, called <a href=""http://blogs.msdn.com/e7/archive/2009/03/17/designing-aero-snap.aspx"" title=""Designing Aero Snap"">Aero Snap</a>.

That second link gives you most of the low-down on what Aero Snap is and what it does, and even how it was designed if that interests you, but I wanted to call your attention to the two main features of Aero Snap, that I think threaten to make single-monitor working <em>almost</em> as productive as multi-monitor working.
<!--break-->
I say almost only because multi-monitor set-ups provide vastly more screen area to work with; I've got into using Aero Snap at home in my single-monitor 20.1"" widescreen set-up, and it works well.  On a single 24"" or 30"" screen, I reckon Aero Snap would work just as well as my two 19"" monitors at work.  Sadly, perhaps, the economics of providing a single large screen over two smaller screens still don't add up.

The first feature of Aero Snap that I want to call out is side-by-side windows (<a href=""http://goview.com/?id=890f4875-483a-48c9-a45e-4459b6310eea"" title=""Aero Snap Side-By-Side Windows demonstration video"">watch a short demonstration</a>*).  At work, I use my two monitors to view multiple things simultaneously: a bug report and the associated class or method, for example.  This makes me more productive because I don't have to constantly flip between the two windows.  Sometimes, I have found myself wanting a third screen.  Using Aero Snap's side-by-side windows, I can display two items simultaneously on a single screen.  Simply restore the window so that it is not maximised, and drag it to the left or right edge of the screen.  Via Aero Peek, you will get an outline preview of the window's new size and location, snapped to that edge of the screen.  Release the mouse button and the window will adopt its new position and shape.  

The second feature is vertically-maximised windows (<a href=""http://goview.com/?id=4471a67c-a44d-49e2-a71f-23d0d5e1b38a"" title=""Aero Snap Vertically-Maximised Windows demonstration video"">watch a short demonstration</a>*).  With widescreen windows becoming more common, it no longer makes complete sense for a window to always and only maximise in both dimensions.  For example, when reading a large document or email, your eyes can struggle to follow a line across the entire width of a widescreen window (I speak from experience; I've been using my 20.1"" widescreen monitor at 1680x1050 for nearly three years now).  This is less of an issue with websites, as the visual design of websites tends to mitigate the issue with fixed-width columns, etc.  My own blog, for example, limits the width of the content section to 700 pixels on all devices; on my monitor, I have a large pale grey/blue gutter of 3-400 pixels on either side of the main body of the site.  

Vertically-maximised windows allow you to get the full benefit of the vertical resolution of your monitor, whilst only consuming a fraction of the horizontal resolution.  This saves you from having to scan very long lines of text.  To give you an idea of just how long, my monitor is nearly 45cm wide, over twice the width of two sheets of A4 paper.  You can vertically-maximise a window by double-clicking the top edge of the title bar (when the cursor changes to a vertical-resize icon), or by vertically-resizing the window to the edge of the screen in either direction.  

Aero Snap makes window management much easier in Window 7, and I think it promises to be a widely-used feature.  It's also one of those features that's such a good idea, it makes you wonder why it hasn't been implemented in an earlier version of Windows.  I also can't wait to see the RC of Windows 7 (due soon?) and try out <a href=""http://blogs.msdn.com/e7/search.aspx?q=rc&p=1"" title=""Posts on Windows 7 RC at the Engineering Windows 7 blog"">the many improvements over the beta</a>.  

<hr />

* Recorded using <a href=""http://goview.com/"" title=""Citrix GoView Beta"">Citrix GoView Beta</a>.  My apologies for the background crackling noises on the audio; this is a product of my soundcard getting old and dusty, and not GoView."
The Terminal,"[4/5] An engrossing, charming, and ultimately very human comedy about a man stranded in an airport terminal.
","
[4/5]

<em>The Terminal</em> is an interesting movie.  This is mainly because it's a comedy, not a genre that director Steven Spielberg is known for contributing to in any quantity.  There's also not that much that's obviously funny about a man trapped in an airport terminal indefinitely, particularly when his country is wracked by a military coup and a rebellion against it.  The plot steals a couple of elements from the rom-com genre, although this is no chick flick, and the sun doesn't set on the happy couple.

Tom Hanks puts in a stirling performance as Viktor Navorski, displaying a knack for comic timing and delivery that hadn't really been on show since his earlier works such as <em>Big</em>.  His character's Eastern European accent and poor grasp of English are portrayed convincingly, and Hanks renders the character with the quiet grit and determination of a man on a mission.  Catherine Zeta-Jones is an air stewardess with man troubles, and that's all I can really say about her.  Her performance was ""good enough"" and certainly average against Hanks', and by the end, her part is largely superfluous anyway, seeming to serve only to draw the story out to full feature length.  Having said that, there are some lovely scenes between her and Hanks, such as their dinner date, and there is obvious chemistry between the actors throughout.  I do think, however, that the only reason she received top billing for this film was because of her status in the Hollywood A-List; the part certainly doesn't demand it.  

[img_assist|nid=81|title=Catherine Zeta-Jones, Kumar Pallana and Tom Hanks in ""The Terminal""|desc=Copyright &copy; 2004 Dreamworks LLC|link=node|align=center|width=484|height=322]

Chi McBride, Kumar Pallana, Diego Luna and Stanley Tucci all put in good supporting performances.  Tucci is the cold, officious, bureaucratic immigration officer Frank Dixon whose attempts to cover his ass ultimately expose the character's greatest flaws.   McBride, Pallana and Luna play ""The Three Amigos"" who bond with Navorski, and there's a small cameo from <em>24</em>'s Mike Novick, Jude Ciccolella.  

The film opens with Navorski arriving at JFK International Airport, New York.  Navorski is fervently clutching a can of dried peanuts at passport control when his passport is rejected.  After being passed up the hierarchy to Director Frank Dixon, we learn that Navorski's native Krakhozia has fallen to a military coup.  During the transition, all Krakhozian passports were revoked by the government, and the US authorities annulled all entry visas from Krakhozia.  There are no return flights to Krakhozia.  This sequence of events has left Navorski without country, and so is deemed an ""unacceptable"", required to remain within the terminal until such time as he is able to legally enter the United States or return home.  

All this passes Navorski by as his English is poor to say the least, until he returns to the terminal and sees scenes of the Krakhozian war on CNN.  Now understanding the nature of his plight, he finds himself an unused airport gate to sleep at, and starts a daily routine.  Collecting unused luggage trolleys, he returns them to the depots to be rewarded with a quarter per trolley, which he uses to buy food.  Buying an English version of his New York travel guide, he improves his English.  He looks for a job; eventually he's taken on as a construction contractor, earning a good wage.  Within time, he has befriended most of the customer-facing staff at the various airport outlets, and in the security team too.  

Viktor has a couple of chance encounters with Amelia Warren (Zeta-Jones), an air hostess who is having an affair with a married man in New York.  All is not well in her relationship, and she asks Viktor out to dinner in New York a couple of times, which, of course, he must refuse.  The language barrier provides some fuel for the inevitable misunderstanding between these characters.  At the same time, he is helping Enrique Cruz (Luna), a food truck driver, with some girl problems of his own; he has his eye on the beautiful immigration officer Delores Torres (Zoë Saldana, soon to be playing Uhura in the new <em>Star Trek</em> feature), but has no way to woo her.  Navorski's daily trips to Torres to request entry into the US provide the perfect entry point for Cruz, so he offers to feed Navorski in exchange for carrying information between himself and Torres.  

There are a couple of features of the plot that spoiled the story for me a little.  For example, there is no concept of the amount of time passing in the film.  This device generally works quite well, as airports are rather timeless places, where hours can pass in seconds and minutes can pass in days.  However, it also makes character development, such as Viktor's self-education in English seem too fast to be realistic.  There's also the non-sequitur involved in Viktor being taken on as a building constructor: he's refused work at all the outlets in the terminal, and on his way home he comes across a half-finished wall.  Over night he completes the work with a beautiful flair and when the foreman (Ciccolella) returns with his crew the next day, he's offered a job based on the quality of his work.  

Zeta-Jones' character, as I've mentioned already, appears to be mostly superfluous, providing only a way for Viktor to tell his full story and reasons for travelling to New York.  By the end of the film, she has found him a way out of the airport and into the US, but by the time this happens, the war in Krakhozia is over and Navorski is a free man again.  As much as the other flaws, the Warren character silently screams ""I'm a plot device"" over and over.  

Having said all that, the film is a truly charming and enjoyable comedy, and tells a very human story.  It's also the first film I've watched at home for a while that's truly engrossed me.  It is easy to sympathise with Navorski's situation, as most people have been stuck in airport hell at one time or another and can extrapolate that experience to something more permanent.  The script is frequently witty, and there's a liberal sprinkling of visual gags to keep the slapstick fans entertained (there's a running visual joke with a wet floor).  It has something for everyone, and as such would probably make a good date/quiet-night-in-with-your-partner movie."
Now available at codebork.com and SEO,NULL,"I thought it was high time that my blog, being called CodeBork an' all, be located at a codebork domain, so I just bought codebork.com and codebork.co.uk.  codebork.co.uk has been configured as a ServerAlias for codebork.com.  The site as a whole will continue to be accessible indefinitely at the current domain, alastairsmith.me.uk, but requests to this domain will be redirected (via a 301 Moved Permanently response) to codebork.com.  Hopefully this will go some way to avoid annoying Google; after all, I now <a href=""http://sethgodin.typepad.com/seths_blog/2009/04/how-to-make-money-with-seo.html"" title=""How to make money with SEO"">""own""</a> the CodeBork search term on Google, and it would be dumb to upset that.
<!--break-->
In fact, it was that link (from <a href=""http://sethgodin.typepad.com/"" title=""Seth's Blog"">Seth Godin's rather excellent blog</a>) that put me on to the idea.  I'd mulled with buying these domains for a while, but hadn't thought it really necessary.  Given that my blog has been going under the name ""CodeBork"" for nearly a year now, and two of my blog posts currently occupy the top spots on <a href=""http://www.google.co.uk/search?q=codebork"" title=""Search Google for &quot;CodeBork&quot;"">the appropriate Google search</a>, I thought I'd try a spot of <abbr title=""Search Engine Optimisation"">SEO</abbr> and see if I can't get more than the just the first two results.

So there we have it: the next step in ""building the CodeBork brand"".  Or possibly the ""brand of Alastair"", depending on how egocentric I'm feeling. :-)"
Serverfault.com Private Beta,NULL,"You may remember <a href=""http://www.codebork.com/coding/2008/09/11/stack-overflow.html"" title=""Stack Overflow on Codebork"">a post</a> I wrote a while back advertising the <a href=""http://stackoverflow.com/"" title=""Stack Overflow"">Stack Overflow</a> site created by <a href=""http://www.joelonsoftware.com/"" title=""Joel on Software Blog"">Joel Spolsky</a> and <a href=""http://www.codinghorror.com"" title=""Coding Horror Blog"">Jeff Atwood</a>.  It's been live a bit over six months, and it's grown enormously in features and user base since I wrote that post.  

<a href=""http://blog.stackoverflow.com/2009/04/server-fault-private-beta-begins/"" title=""Server Fault Private Beta Begins"">Jeff and Joel have now put the software behind Stack Overflow to another use</a>, powering a System Administration community site entitled <a href=""http://serverfault.com"" title=""Server Fault"">Server Fault</a>, currently in private beta.
<!--break-->
This is a somewhat less private beta than the Stack Overflow beta.  If you have greater than 100 reputation on Stack Overflow, you can simply provide the password ""alt.sysadmin.recovery"" (as published on the Stack Overflow blog) and log in with your Open ID; you'll be rewarded with 100 reputation points on Server Fault for your trouble.  If you don't currently use Stack Overflow, you can sign in with an Open ID account and send an email to team@serverfault.com to be granted access.  

This promises to be another great resource.  As with the Stack Overflow site, off-topic questions will generally be closed fairly quickly (and moderators are already in place), but I doubt many people will see that as a bad thing; it's certainly not hurt Stack Overflow's adoption.  

Go check it out, post a question or two, feed back some answers, and play around.  Given that Jeff has <a href=""http://blog.stackoverflow.com/2009/03/it-stack-overflow-update-naming-is-hard/"" title=""IT Stack Overflow Update: Naming is Hard"">repeatedly</a> <a href=""http://blog.stackoverflow.com/2009/04/server-fault-private-beta-begins/"" title=""Server Fault Private Beta Begins"">described</a> the Stack Overflow code base as ""mature"", I've a feeling it will probably go live fairly rapidly."
Windows 7 Release Candidate Available,NULL,"In case you'd somehow missed <a href=""http://www.theregister.co.uk/2009/04/26/windows_7_release_candidate/"" title=""Microsoft names Windows 7 RC1 dates"">the news</a> and <a href=""http://www.theregister.co.uk/2009/04/20/windows_7_release_candidate_5_may/"" title=""Windows 7 Release Candiate coming 5 May"">the leaks</a>, the RC of Windows 7 is publicly available as of today.  It's been available to MSDN subscribers (and, by extension, the wider Internet) since last Thursday, but Microsoft are officially opening the doors to the unwashed hordes tomorrow.
<!--break-->
I'll post a review of the RC once I've got it installed.  Based on my <a href=""http://www.codebork.com/science-and-technology/2009/02/24/windows-7.html"" title=""Windows 7"">previous</a> <a href=""http://www.codebork.com/science-and-technology/2009/03/21/windows-7-aero-snap.html"" title=""Windows 7 Aero Snap"">experiences</a> with Windows 7, I'm excited to try the RC, and the famed <a href=""http://community.winsupersite.com/blogs/paul/archive/2009/04/24/secret-no-more-revealing-virtual-windows-xp-for-windows-7.aspx"">""XP Mode""</a>, which should hopefully solve my one backwards compatibility issue: my <a href=""http://www.netgear.com/Products/Entertainment/DigitalMediaPlayers/EVA8000.aspx"" title=""Netgear EVA8000"">Netgear EVA8000</a>'s desktop software component doesn't work with the ""new"" Windows 7 firewall.  I'm also hoping I'll be able to get my iPod Nano syncing with iTunes again (ideally outside of XP mode).<sup>*</sup>

In the meantime, go give the RC a spin.  Use a <abbr title=""Virtual Machine"">VM</abbr> if you don't have a machine to spare; the experience will still be more than favourable.  

<hr>
<sup>*</sup>For the record, I don't believe this to be an iTunes/Windows 7 bug; rather, it's a problem with my Windows 7 beta installation.  Sync initially was working correctly on Windows 7, and I think something happened with a dodgy eject sequence that screwed up iTunes' pairing with the Nano."
Star Trek (2009),"An enjoyable, rollicking space adventure in the best tradition.
","
[4/5]
Before I get onto the film itself, it's worth setting up some context for this review.  I'm no Trekkie; in fact, you could barely even call a fan of <em>Star Trek</em>.  That's not to say I haven't enjoyed watching it (I particularly liked The Next Generation and Voyager), but it's not something I ever went out of my way to watch.  In the great unspoken rivalry between <em>Star Trek</em> and <em>Star Wars</em>, I always sided with <em>Star Wars</em>.  On the other hand, I'm a big fan of science fiction as a genre, and it's been a little while since I've seen a film in truly rooted in that vein.  

With that out of the way, onto the film.  You can't have failed to notice that the team behind <em>Star Trek</em> is the same team that brought you <em>Lost</em>.  J. J. Abrams (also responsible for the hits <em>Alias</em> and <em>Fringe</em>) has since turned his hand to movies, writing and directing <em>Mission: Impossible III</em> and producing <em>Cloverfield</em>; Michael Giacchino (composer) has previously scored <em>Cloverfield</em> and <em>Speed Racer</em>, as well as numerous TV and video game projects; this is Damon Lindelof's first foray into movies, however.  With that kind of pedigree (particularly given the popularity of <em>Lost</em>), many kinds of expectations are set high.  Expectations of the plot, expectations of the special effects, expectations of the characters, expectations of the actors.  

Mostly, it meets those expectations; it never <em>exceeds</em> those expectations at all, but the bar is set incredibly high.  The acting is as excellent as could be hoped for.  Visually, the film is exciting and compelling.  The plot, whilst not flawless, is well-devised, and is supported by a witty and entertaining script.  

The film is exceptionally well cast, which might be considered surprising given how unknown the majority of the cast were.  Abrams has a good track record working with relative unknowns, however: Jennifer Garner made her name on <em>Alias</em>, and the cast of <em>Lost</em> were mostly small TV actors before the island gig came along.  

Chris Pine fills William Shatner's shoes well.  His swaggering arrogance is believable, and his character provides a firm base for Shatner's ""future"" Kirk.  Zachary Quinto is truly excellent as Spock, although in many respects he's treading familiar ground.  Both his regular character in <em>Heroes</em>, Sylar, and Spock have a cold, calculating air to them. Spock exhibits it through the Vulcan side of his personality: emotionless and logical to the last; Sylar through his choice and treatment of his victims.  Leonard Nimoy effortlessly reprises his role as Spock (credited as ""Spock Prime""), and is understandably more warm with the younger crew than Quinto's Spock.  Karl Urban, best known for his role as Eomer in <em>Lord of the Rings</em>, is an inspired choice for Dr Leonard ""Bones"" McCoy: he looks and sounds a <em>lot</em> like DeForest Kelley.  There's sadly not much more to say about Bones; he tempers some of Kirk's wildness, and there's an ingenious scene where he smuggles Kirk aboard the <em>Enterprise</em>, but he's mostly in the background here.  Zoë Saldana is delectable as Uhura, although she is under-utilised in the final third of the film.  Simon Pegg makes a late entrance, complete with dodgy Scottish accent and comic timing; he, along with John Cho (Sulu) and Anton Yelchin (Chekov) are also mostly relegated to the background.  

To an extent, it's difficult to take Pegg seriously.  He's an excellent comic actor, but I'm in two minds about his role as Scotty.  On the one hand, he plays the role well; on the other, it seems as though the ""re-envisioning"" of Scotty has tailored the role to Pegg.  His arrival on the <em>USS Enterprise</em> is comical, bordering on the farcical, and many of his lines play him as the amusing sideshow, the light entertainment, the comic relief.  I found this didn't work so well, because there was plenty of humour (and a better class of it) in the film already.  

The writers were obviously aware of the loyalty of the fan base.  Conventions from the original series have been observed (observe carefully the team sent to destroy the drilling platform at Vulcan), and I believe there's much to like here for existing fans.  There's much to tempt new fans, as well, as this is Star Trek with a 21<sup>st</sup> century make-over, and I mean that in a good way: it has been updated and re-styled to meet the expectations of today's audience.  The wobbly sets are out, and I am thankful for that.  

What is perhaps most interesting about this film, however, is the number of parallels between <em>Star Trek</em> and <em>Star Wars: Episode IV - A New Hope</em>.  Both feature a planet-destroying weapon, both feature a character on the wrong side of the weapon, both feature a cocky, young upstart hero, both share the same closing scene.  Both, too, are good ""beginning"" films, standing alone well, but with a sense that there is more to come in later instalments.  I find these similarities significant for a couple of reasons: first, it is a tacit acknowledgement of the true significance of <em>Star Wars</em> in movie history; second, I believe it makes a strong case for the fusion of the ""space opera"" and more serious science fiction in the best traditions of <em>The Next Generation</em> and <em>Deep Space Nine</em>.  It doesn't have to be either-or, it can be both.  

The ending may leave you cold; a couple of people described it as a ""generic space adventure film ending"".  I didn't find it as jarring as that, but by no means was it anything innovative.  It sets up the reinvigorated franchise without mandating further films, and makes for a rollicking adventure that will thoroughly divert for its two-hour running length."
Code Complete Introduction,NULL,"Next week I'm starting an adventure.  I mean that in the truest sense of the phrase: I feel exhilaration and trepidation in equal measure.  The path is littered with potholes and other small dangers.  But at the end of it, I will emerge a better developer.  Next week, I am starting to read <a href=""http://www.amazon.co.uk/Code-Complete-Practical-Handbook-Construction/dp/0735619670/"" title=""Code Complete: A Practical Handbook of Software Construction""><em>Code Complete</em> (2<sup>nd</sup> ed.) by Steve McConnell</a>.
<!--break-->
This is a seriously important book in Software Engineering literature.  Jeff Atwood, a .NET developer and prolific blogger that I respect, classes this as <a href=""http://www.codinghorror.com/blog/archives/001264.html"" title=""Pseudocode or Code?"">""the single most recommended programming book""</a> and even went as far as to name his blog, <a href=""http://www.codinghorror.com/"" title=""Coding Horror"">Coding Horror</a>, after one of the key concepts in the tome.  My friend <a href=""http://www.analysisuk.com/blog/"" title=""Steve's Personal Blog"">Steve</a> maintains that reading this book was his transition from amateur to professional developer.  Many software houses mandate that their employees have read the book before writing any code.  

In short, it's a book I should have read over a year ago when I started out in my chosen profession, and if you haven't read it already, so should you.  

In an effort to aid the small community I have built up around my blog, to ensure my understanding of the material, and make me a better technical writer as well as developer, I will be writing a series of posts on this book, summarising the key ideas from each chapter.  Stay tuned to the <a href=""http://www.codebork.com/category/coding/code-complete"" title=""Code Complete posts on CodeBork"">Code Complete</a> category, or subscribe to the <a href=""http://www.codebork.com/taxonomy/term/122/0/feed"" title=""RSS feed for the Code Complete series on CodeBork"">topic feed</a>.  I look forward to having you along for the ride!"
Code Complete: Preface and Chapter 1,NULL,"Today was Day 1 of the Rest of my Life: I read the preface and first chapter of <em>Code Complete</em>, and here's what it had to say.
<!--break-->
The book is aimed at four groups of people: experienced programmers, technical leads, self-taught programmers, and students.  In short, this is a book for everyone.  The justifications for each are as follows.  

For experienced programmers, the book serves as a reference for, and reminder of, best practices in software construction (more on that term later).  It reminds you that <strong>[Software Development] != [Programming]; software development is a bigger process than writing code.</strong>  

For technical leads, <strong><em>Code Complete</em> has real use as a mentoring aid</strong> for the education of junior developers in your team, and can help you fill your own knowledge gaps in the software construction process.  

An interesting statistic quoted in the preface claims that <strong>~50,000 new developers enter the profession each year, and only ~35,000 software-related degrees are awarded each year</strong> (I believe these figures refer to the US).  Thus, there is a sizeable proportion of the total development community that don't have software-related degrees: bankers, administrators and accountants all might have development-type aspects to their jobs.  Scientists write code to model data and perform their experiments.  None of these people have had formal education in how to program computers, but <em>Code Complete</em> can fill the gap.  

The final group the book is aimed at is students.  The single reason McConnell gives for this is that <strong>fresh graduates are often rich in theoretical knowledge, but poor in practical know-how</strong> (hi there!).  This gap is now being addressed by courses and modules covering software engineering practices, but the reality is that this too will be theoretical knowledge with little or no hands-on experience of the concepts.  

The overall aim of the book, therefore, is to be ""A Practical Handbook of Software Construction"".  As such, it can be tackled cover-to-cover or dipped into for a refresher on particular topics.  

What is this ""Software Construction"" thing, anyway?  McConnell likens the process of building software to the process of constructing a building of some kind.  You take the plans and you design and build your individual components and assemble them into a whole.  I'm not completely sure I agree with this metaphor &mdash; I've always felt that it was more akin to the art of writing, what with its plans and re-drafts and what-have-you &mdash; but I'm happy to run with it for now.  Maybe that's what appealed to me about the profession in the first place: software development can be viewed as a craft, like carpentry for example, and thus has elements of an artform.  Well-written, elegant code can be as breath-taking and beautiful as Rodin's <em>The Kiss</em>, in its own way.

[img_assist|nid=89|title=|desc=|link=node|align=center|width=400|height=564]

Anyway, I digress.  McConnell includes the following processes, amongst others, in the concept of software construction:
  <ul>
    <li>Design</li>
    <li>Coding</li>
    <li>Debugging</li>
    <li>Developer Testing (i.e., unit tests and integration test)</li>
    <li>Reviews</li>
    <li>Integration</li>
  </ul>

In short, software construction is the end-to-end process of developing a software product.  It is not, however, the full lifecycle of the product, nor even the project; note that requirements' gathering, architecture design and maintenance/support are all out of scope.  

Why is this process so important?  <em>Is</em> it that important?  Well, in a word, <strong>yes</strong>.  There are many reasons for this, but fundamentally:
<blockquote>Construction is the central activity in software development, and the only activity that's guaranteed to be done</blockquote>

Let's stop and think about this for a second.  Requirements gathering, architecture design, and even testing can all be &mdash; and <em>have</em> all been &mdash; skipped over to get a product released to market on time.  <strong>The construction, the actual building of the product, is the only activity that will be completed.</strong>

The key implication of this, and the motivation for the entire book, is that <strong>improving the process of software construction is guaranteed to improve the product quality</strong>.  Even if other steps in the process are skipped out altogether, the quality of the construction determines the quality of the product.  This concept applies elsewhere, as well.  Look at the iPod, for example.  It is market leader not because of its features or sound quality (both of which have been criticised in the past) but because it is so well constructed.  The design and build quality of any of the models in the iPod range is second to none in the portable media player market.  

The corollary of this idea is that your understanding of how to do construction determines how good a programmer you are.  If you have a good understanding of software construction, you're likely a very capable and professional developer; if you don't, you're very likely at the other end of the scale.  That's where I am, and I intend to do something about it.  <strong>Will you?</strong>"
Code Complete: Software Development Metaphors,NULL,"Chapter 2 of <em>Code Complete</em> covers software development metaphors, critically evaluating a number of metaphors that have been proposed over the last thirty or so years, including code as writing, growing a system, system accretion, and software construction.
<!--break-->
Before we get on to the metaphors themselves, a concept of this chapter is the idea of ""The Intellectual Toolbox"".  This is also covered by <a href=""http://www.amazon.co.uk/Pragmatic-Programmer-Andrew-Hunt/dp/020161622X/ref=sr_1_1?ie=UTF8&s=books&qid=1242693332&sr=8-1"" title=""Buy The Pragmatic Programmer from Amazon (UK)""><em>The Pragmatic Programmer</em></a>, and you will often hear programmers using tool-related metaphors, such as ""when you have a hammer, everything looks like a nail"".  The idea here is to build up your toolbox as you progress, just as any other craftsman would, but in place of hammers, chisels, routers and lathes, we have debugging tools, coding tools and testing tools: stack traces, protocol traces, design patterns, development methodologies, IDEs and productivity add-ins like ReSharper or CodeRush Refactor!, Mocking and Unit Testing frameworks.  Note that this represents a combination of intellectual (e.g., development methodologies) as well as ""physical"" tools (e.g., an IDE).  

So why do we, and why should we, use metaphors for understanding software development?  Well, metaphors are hugely important for furthering the understanding of complex ideas.  For example, the <a href=""http://en.wikipedia.org/wiki/Dynamical_billiards"" title=""Dynamical billiards (Wikipedia)"">billiard-ball model of gasses</a> and the <a href=""http://en.wikipedia.org/wiki/Light#Wave_theory"" title=""Wave theory of light (Wikipedia)"">wave theory of light</a> both propelled forward research, knowledge and understanding of the two concepts (dynamical systems and light).  In McConnell's words,

<blockquote>
The power of models is that they're vivid and can be grasped as conceptual wholes.
</blockquote>

They provide a heuristic abstraction to a complex problem: they're not precise models, but rather a bit of a guess.  Because of their heuristic nature, metaphors can lead you down false paths.  Perhaps the best example of this is again the wave theory of light.  Extending this metaphor to its logical conclusion led scientists to hunt for a medium that propagated light in the same way that air propagates sound, which they termed the ""ether"".  No one ever found the ether, precisely because it doesn't exist: the wave model of light was only half of the wave-particle duality that we now understand to be correct.  

<h3>Software Penmanship: Writing Code</h3>
This metaphor holds that software development is like writing a casual letter: you might make a number of drafts, screwing up and throwing away the unwanted ones.  This doesn't hold up for more than the smallest of projects, though.  Writing is usually a one-person activity, for example, whilst software development is frequently team-based.  Similarly, you can't change a letter once it's been posted; as we all know, software is only very rarely regarded as ""complete"".  Additionally, writing focusses on originality, whereas software development focusses on reusing ideas, code and test cases.  McConnell therefore dismisses the ""writing"" metaphor as too simple and rigid to be effective.  

<h3>Software Farming: Growing a System</h3>
The farming metaphor is similarly dismissed.  This metaphor holds that development should be like planting seeds and growing crops: you design, code and test a piece, and add to the system a little at a time.  This is, however, good technique described by a bad metaphor, and McConnell's advice is to retain the incremental approach and discard the ""terrible"" metaphor.  

The real weakness of this metaphor lies in the suggestion that you, the developer, have no control over how the software is developed.  Farming involves maintaining the ground in which the crops are grown, which may start out unsuitable for the crops you want to grow.  You're a slave to the weather: a particularly wet summer might destroy your entire crop of strawberries, or a dry summer kill off your wheat.  The best you can do is manage the factors affecting your crop, not control them.  This is not the same in software: you have direct control over the ground your software is built on, how it is built and, to an extent at least, what is built.  

<h3>Software Oyster Farming: System Accretion</h3>
McConnell believes that Oyster Farming is what people really mean when they mention the farming metaphor.  Oyster Farming, or software accretion, is the idea that software grows like a pearl inside a clam: it's added to layer by layer until the pearl is ripe for harvesting.  

The word ""accretion"" refers to growth, or some other increase in size, by a gradual external addition or inclusion.  This can be easily applied to software development: start with the simplest possible version that will run.  This forms the skeleton of the system, and slowly but surely the muscle and skin are added to the skeleton until the feature is completed.

As you might have already guessed, a lot of the Agile practices that have been developed in the last ten or fifteen years have their roots in the System Accretion metaphor.  And this is a good metaphor, too: it doesn't over-promise (like the software penmanship metaphor), nor is it easy to extend inappropriately (like the software farming metaphor).  

McConnell believes we can go one better, however.  

<h3>Software Construction: Building Software</h3>
The software construction metaphor has by far the most time and space devoted to it, perhaps unsurprisingly as it is McConnell's favoured metaphor.  He believes this to be a more useful metaphor than the ""growing"" or ""writing"" metaphors, as it is not only compatible with the idea of accretion, but also provides more detailed guidance.  Mistakes on small construction projects are easily rectified by tweaks or even starting again; however, mistakes on larger projects are harder to fix this way.

As with construction, labour is the single biggest cost to developing software.  In my case, I have a workstation-class PC, an aging desktop PC, two 19"" monitors, a height-adjustable desk, a <a href=""http://www.hermanmiller.com/mirra/"" title=""Herman Miller Mirra Chair homepage"">Herman Miller Mirra chair</a>, an MSDN subscription, and a ReSharper licence.  This most likely totals something in the region of £5000, so the single biggest cost to developing software is my time and the time of my colleagues.  This is one reason the ReSharper licence is so important &mdash; it saves me a bunch of time, making me more productive.

I digress somewhat.  The point of highlighting the cost of labour is that, in construction, moving a wall six inches to the left is expensive only because of the time it takes, not because of the materials used.  This is magnified if the wall happens to be load-bearing.  The same applies to software: changing a report in an application is fairly trivial, but is time-consuming, particularly if the query (or other backing routine) needs changing.  Add to that the time required to test the change, and the simple job has cost you hundreds, if not thousands, of pounds.  

McConnell wraps up by returning to the concept of software metaphors.  He reminds us that metaphors are heuristic, and so are not mutually exclusive.  There can be some value in combining them if the circumstances require and allow for it.  Additionally, precisely because they are heuristics, they represent something ""more like a searchlight than a road map"".  He therefore recommends that they be used to give you insight into your problems and processes, and help you think about your programming activities and imagine better ways of doing things.

<hr />
<h3>Key Points</h3>
Quick-reference ""Key Points"" directly quoted from <em>Code Complete</em>:
<ul>
  <li>Metaphors are heuristics, not algorithms, and so can be a bit sloppy</li>
  <li>Metaphors help you understand the software-development process by relating it to other activities you already know about</li>
  <li>Some metaphors are better than others</li>
  <li>Treating software construction as similar to building construction suggest that careful preparation is needed and illuminates the difference between large and small projects</li>
  <li>Thinking of development practices as tools in a toolbox suggests that every programmer has several tools and no single tool is right for every job.  Choose the right tool for each problem.  </li>
  <li>Metaphors are not mutually exclusive.  Use the combination of metaphors that works best for you.</li>
</ul>"
reCAPTCHA re-enabled,NULL,Apologies to anyone who tried to comment on one of my previous entries and found they couldn't.  The issue with reCAPTCHA that was preventing this has now been resolved and you should be able to post comments again without any problems.  
"Code Complete: Measure Twice, Cut Once (Part 1 - The Importance of Prerequisites)",NULL,"Any good carpenter, joiner, or other worker of materials will tell you to ""measure twice, cut once"".  This is a good philosophy to apply to life and your craft as a software engineer.  It implies attention to detail, efficiency and proper preparation; it results in ""right first time"" components and products, quality and reduced waste.  

So, getting your pre-requisites right is important.  There are different opportunities to emphasis quality: at the beginning of the project (planning and design), during the construction of the product and at the end of the project (testing).  During the construction phase, your only option is to build the product solidly, with quality materials and tools.  At the end of the project, when your only option is testing, you can't detect that your product is the wrong solution for the problem, or that it is the right product built in the wrong way.  Testing is only one part of quality assurance, and only ensure that the thing is fit for purpose.  

Therefore, the planning and design stages are your one opportunity to ""get it right"", and the cheapest opportunity to resolve any issues.  You can, and <em>should</em>, make sure you have the right project and the right plans, and ensure the design is fit for the product.  It's a risk reduction process.  As my Dad used to tell me, and as his boss (fittingly, in the construction industry) used to say, remember ""The Seven Ps"": <strong>Proper Preparation and Planning Prevents Piss-Poor Performance</strong>.
<!--break-->
Conversely, many programmers (myself included), rarely stick around to prepare ahead, and just dive right in.  Why is this?  Why don't programmers prepare ahead?  McConnell proposes a number of reasons:
<ol>
<li>They don't know how</li>
<li>They can't wait to start programming</li>
<li>Pressure from above</li>
</ol>

There are, of course, possible solutions to these, ranging from education to self-discpline to resistance (overt or covert).  The key thing, though, is to learn from your rushed jump-in-the-deep-end attempts.  Are the problems you experienced similar to other problems?  Could they have been foreseen?

McConnell also proposes his four-point ""Utterly Compelling and Foolproof Argument for Doing Prerequisites Before Construction"" (UCFADPBC, for those who prefer snappy initialisms).
<ol>
  <li><em>Appeal to logic</em>&nbsp;&nbsp;&nbsp;It makes sense to plan first, and the amount of planning that is sensible is directly proportional to the size of the project.  It also makes sense to design first; going down blind alleys wastes time, and time is money.</li>
  <li><em>Appeal to analogy</em>&nbsp;&nbsp;&nbsp;Building a house will require architectural drawings, blueprints, etc., before you can lay foundations.  You don't start decorating the Christmas tree before it's in the stand.  There are more :-)</li>
  <li><em>Appeal to data</em>&nbsp;&nbsp;&nbsp;McConnell references studies conducted by industry giants such as Hewlett-Packard, IBM, and others, that prove that it pays to do things right the first time, and that unnecessary changes are expensive. The table below illustrates the relative costs of fixing defects based on when they're introduced into the product, and when they're detected.  </li>
  <li><em>Boss-readiness Test</em>&nbsp;&nbsp;&nbsp;It's difficult to summarise this.  Essentially, ""make sure your boss understands the importance of pre-requisites"".  </li>
</ol>

<table summary=""Average Cost of Fixing Defects Based on When They're Introduced and Detected"" style=""width: 100%"">
  <caption style=""font-weight: bold; font-style: italic;"">Average Cost of Fixing Defects Based on When They're Introduced and Detected</caption>
  <thead style=""background-color: #ccc;"">
    <tr>
      <th></th>
      <th colspan=""5"">Time Detected</th>
    </tr>
  </thead>
  <tbody style=""text-align: center;"">
    <tr>
      <td><strong>Time Introduced</strong></td>
      <td>Requirements</td>
      <td>Architecture</td>
      <td>Construction</td>
      <td>System Test</td>
      <td>Post-Release</td>
    </tr>
    <tr>
      <td>Requirements</td>
      <td>1</td>
      <td>3</td>
      <td>5-10</td>
      <td>10</td>
      <td>10-100</td>
    </tr>
    <tr>
      <td>Architecture</td>
      <td>&mdash;</td>
      <td>1</td>
      <td>10</td>
      <td>15</td>
      <td>25-100</td>
    </tr>
    <tr>
      <td>Construction</td>
      <td>&mdash;</td>
      <td>&mdash;</td>
      <td>1</td>
      <td>10</td>
      <td>10-25</td>
    </tr>
  </tbody>
</table>

Different projects need different approaches; for example, <a href=""http://www.fastcompany.com/magazine/06/writestuff.html?page=0,0"" title=""They Write the Right Stuff | Fast Company"">the space shuttle software development process</a> is really very different from your favourite Agile methodology: the former is very sequential and very bureaucratic by necessity; the latter is fast, light-weight and very iterative.  McConnell mentions that projects generally aren't exclusively sequential or iterative, but are a mixture of the two.  For example, for a systematic change, you might do 80% up-front work; for an incremental change, it might only be 20% up-front work.  The choice of approach depends on the type of the project, the formality of the project, the technical environment, staff capabilities and the project's business goals.  McConnell provides the following comparison of when sequential and iterative methodologies might be appropriate:


<table style=""width: 100%"">
<thead style=""background-color: #ccc"">
  <tr>
    <th>Sequential</th>
    <th>Iterative</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>Stable requirements</td>
    <td>Requirements are not well-understood</td>
  </tr>
  <tr>
    <td>Straightforward and well-understood design</td>
    <td>Design is complex, challenging, or both</td>
  </tr>
  <tr>
    <td>Dev team is familiar with the application's area</td>
    <td>Development team is unfamiliar with the application's area</td>
  </tr>
  <tr>
    <td>Project contains little risk</td>
    <td>Project has a lot of risk</td>
  </tr>
  <tr>
    <td>Long-term predictability is important</td>
    <td>Long-term predictability is not important</td>
  </tr>
  <tr>
    <td>Cost of changing requirements, design and code downstream is likely to be high</td>
    <td>Cost of changing requirements, design and code downstream is likely to be low</td>
  </tr>
</tbody>
</table>

The next post in this series will cover specific pre-requisite activities, such as requirements and architecture."
"Code Complete: Measure Twice, Cut Once (Part 2 - Essential Prerequisites)","<a href=""http://www.codebork.com/coding/2009/05/21/code-complete-measure-once-cut-twice-part-1-importance-prerequisites.html"" title=""Code Complete: Measure Twice Cut Once (Part 1)"">Part 1</a> of this post covered the importance of pre-requisites: why it is worth doing them, and doing them well; why it is a bad idea to jump straight into coding; and how to ensure that they are completed at your organisation (if they aren't automatically already).  

This second post covers the three main pre-requisites, namely Problem Definition, Requirements, and Architecture.  

<strong>Note: this is a long post!</strong>
","
<a href=""http://www.codebork.com/coding/2009/05/21/code-complete-measure-once-cut-twice-part-1-importance-prerequisites.html"" title=""Code Complete: Measure Twice Cut Once (Part 1)"">Part 1</a> of this post covered the importance of pre-requisites: why it is worth doing them, and doing them well; why it is a bad idea to jump straight into coding; and how to ensure that they are completed at your organisation (if they aren't automatically already).  

This second post covers the three main pre-requisites, namely Problem Definition, Requirements, and Architecture.  

<h3>Problem Definition</h3>
The very first thing needed is a clear statement of the problem the system is supposed to solve.  As such, it should sound like a problem, such as ""We can't keep up with orders for the Gigatron"".  Sometimes problem definitions are worded more like solutions (e.g., ""We need to optimise our automated data-entry system to keep up with orders for the Gigatron""); this is bad because it obscures the real issue and can quietly close off new alleys of thought that might lead to a better solution.  Problem definitions should be worded in user language, and written from the user's point of view.  Failing to define the problem well can waste a lot of time solving the wrong problem.  

<h3>Requirements</h3>
Why should we bother with the requirements pre-requisite?  From many years of software engineering projects (and many, many failed ones along the way), it has been found that good products are user-driven.  Requirements help ensure that the user, rather than the programmer, drives system functionality.  This is a big win for customer satisfaction: they get to specify what they want from the system and the completed system is acceptable to them because it meets their requirements.  It also helps avoid arguments within the development team over functionality; they provide a baseline to work to, and prevents programmers adding that neat (but ultimately useless) bit of functionality as they go.  Finally, the requirements help minimize code changes to a system after development begins.  As we saw in the last post, errors in requirements are expensive to fix after the requirements phase is complete; if done correctly, the requirements pre-requisite is a firm hand on the customer's prerogative to change the requirements for the project.  

Change in requirements can be very hard to accommodate, but unless you are very lucky, <em>requirements will change</em>.  McConnell provides a shortlist of items to check to help you deal with changing requirements.  The first of these is to use the provided requirements' check-list to assess the quality of your requirements.  You'll have to read the book to get this (and the other check-lists) I'm afraid!  

The second suggestion is to ensure that everyone knows the cost of requirements changes.  This is an education task as much as anything, and remember that we saw in the previous post that changes in requirements can cost anything up to 100x the ""base"" cost if made after this pre-requisite is completed.  This will help prevent the customer, project manager, bosses' bosses' bosses, etc., from taking the process of changing the requirements too lightly, and will ensure that it only happens for mission-critical items.  If you're at the code-face, be sure to keep your eye on the business case for the project and the feature in spite of the changing requirements, as this will help you tolerate the change.  You should actively try to incorporate the change into your perception of the business case for the project; discussing it with your team and the project manager (and other stakeholders) will help achieve this.  

Setting up a change control provider (such as Subversion or Git) will allow you to recover that dropped feature if necessary, and will also allow you to branch your code base for each feature, release, etc.  You can also adopt development approaches that accommodate changes (such as Agile methodologies).  In the recent <a href=""http://www.noop.nl/2009/05/the-big-agile-practices-survey-report-part-1.html"" title=""The Big Agile Practices Survey Report (Part 1)"">Big Agile Practices Survey</a> conducted by <a href=""http://www.noop.nl/"" title=""NOOP.NL"">Jurgen Appelo</a>, source control was voted the most important Agile practice by 100% of the sample (and, thankfully 100% of the sample responded that they implemented it, too).  It is one of those software development practices that should just be common sense in this day and age; not using source control is akin to jumping from a plane without a parachute.  

If all else fails, you can simply dump the project, although this is quite obviously an option of last resort.  If your organisation drops the project, it can be damaging to their reputation; equally, moving yourself on can be difficult within an organisation if you're leaving mid-project, and finding a position with a new organisation can take time.  As someone at work recently mentioned, 

<blockquote>""I've always found job hunting to be a full-time job.""</blockquote>

<h3>Architecture</h3>
The quality of the architecture will determine the conceptual integrity of the system.  With the wrong architecture, you will be tackling the right problem but in the wrong way.  This can make construction trickier and more time-consuming.  

At all levels, and in all parts of the architecture, there should be evidence that alternatives were considered, and the decisions justified.  This is partly to head-off arguments further down the line when problems are run into (these kind of discussions might start off, ""Why was the architecture designed this way?  It doesn't make sense in the context of this problem, it's making this harder.""  Or it might be stronger language than that.)  These discussions aren't constructive when the team has committed itself to an architecture, so having justifications already prepared and readily available is a good thing.  

McConnell identifies a number of typical components of an architecture design.  It may be that some of the sections do not apply to your current system (for example, you may not have any focus on performance, or may not need to localise your application).  I'll tackle each component here, one at a time.

<h4>Program Organisation</h4>
This section defines the main modules of the system.  Each feature should be covered by at least one module, and each modules' reponsibilities should be well defined and loosely coupled.  Communication rules should be well defined.

<h4>Major Classes</h4>
The purpose of this section is to identify the responsibilities and interactions of the major classes in the system.  This will include some or all of class hierarchies, state transitions and object persistence.  

<h4>Data Design</h4>
All major files and table designs to be used should be documented in this section.  It should also document which module or class will be providing access to the data (and it should only be one, except for access classes providing persistence abstraction).

<h4>Business Rules</h4>
Identify your business rules here and describe their impact on the system's design.  For example, you may have a business rule that defines a ""preferred customer"" as one who has bought from you more than 10 times, or whose orders total more than £1,000.  Will this information be represented in the raw data, or calculated from the stored data?  If it is to be calculated, will it be done in the database (in a stored procedure) or in your application?

<h4>User Interface Design</h4>
This should be defined in the architecture only if it is not specified in the requirements.  It should describe how the system is to be modularised to allow major changes of UI.  This can be the part of the system that sees the most change, and can also be one of the hardest bits to change if not implemented with change in mind.  

<h4>Resource Management</h4>
Here you should describe your plan for managing external resources like database connections, threads and handles. It should also include memory management techniques, if appropriate to your development environment.

<h4>Security</h4>
It is arguable that all applications should define a threat model in this era of fast, permanent Internet connections.  Threat modelling is a whole book in itself, so I won't go into it in much detail here.  If you're after more information on application security from a developer's perspective, however, a good pointer is <a href=""http://www.amazon.co.uk/Writing-Secure-Second-Michael-LeBlanc/dp/0735617228/"" title=""Writing Secure Code on Amazon.co.uk"">Writing Secure Code</a> by Michael Howard and Steve Lipner, which could be considered the <em>Code Complete</em> of secure programming.  

This section should cover (amongst other things) buffers, approaches to handling untrusted data, encryption, the amount of detail exposed in error messages, and how in-memory secret data is handled.  

<h4>Performance</h4>
If performance is a concern for your application, then performance goals should have been specified in the requirements.  The section should include performance estimates, as well as identifying which areas are at risk of not meeting those estimates.  If certain areas require specific algorithms or data types, these should be documented here, along with the justifications for requiring them.  

<h4>Scalability</h4>
If scalability is not being designed for, this should be made explicit.  In all other cases, however, you need tackle how the system will address growth.  

<h4>Interoperability</h4>
Will your application share data or resources with other software?  If so, what data/resources, and which applications?  How is this to be achieved?

<h4>Internationalisation/Localisation</h4>
If you're working on an application for a small business, a school, or other similar organisation, internationalisation and localisation may not be concerns for you.  It is likely that public-facing applications for government, or applications written by large software companies, will be subject to internationalisation and localisation.  You need to identify whether these topics will impact your system or not, as they need to be baked into the architecture; it is very difficult indeed to bolt internationalisation and localisation support on afterwards.  

If this is a concern for your application, you will need to consider character sets, resource consumption, and how you will maintain and translate strings without touching code.  It will also impact your UI design.  

<h4>Input/Output</h4>
What are the reading schemes that your application will use?  Where are I/O errors detected: the field, record, stream, or file; or somewhere else entirely?

<h4>Error Processing</h4>
This section should define your techniques for handling exceptions and errors.  Most notably, will your error processing be corrective or detective, anticipative or reactive?  How will your system handle error propagation?  What conventions will it put in place for handling errors?  Will errors be handled at the point of detection, in an error-handling class, or passed up the call chain?  What responsibilities do classes have for their own data validation?  

McConnell also poses the question of the choice of exception handling mechanisms, i.e. will you use the built-in one, or your own?  I believe this is a sign of the book's age: the first edition was published in 1993, when there were few languages around with exception handling frameworks.  Even in 2004, when the second edition was published, languages such as C, without native exception handling support, were (and still are!) prevalent.  However, Java and latterly C# have come to dominate business applications, both sporting excellent exception handling support as part of the language/framework, so in my opinion there is no excuse for using your own exception handling mechanism in any modern language.  

<h4>Fault Tolerance</h4>
What techniques will your application employ to increase its fault-tolerance?  Is it even necessary?  Possibilities include back-off, auxiliary code and voting.

<h4>Architectural Feasibility</h4>
The architecture should show that the project is feasible.  This section should list the architect's concerns, and these should be addressed with investigations, such as prototypes and research.

<h4>Over-engineering</h4>
This section depends on the system, but effectively boils down to the question, ""Should developers over-engineer or do the simplest thing that works?""

<h4>""Buy vs. Build"" Decisions and Reuse Decisions</h4>
These sections are rather obvious: can you buy or download existing software to accomplish some of the tasks (e.g., logging, GUI controls, etc.)?  Can you reuse components from another application?  

Again, in my opinion, if there's a well-trusted third-party library that will achieve the required task, you should use this rather than rolling your own.  The one exception may be <a href=""http://www.joelonsoftware.com/articles/fog0000000007.html"" title=""In Defense of Not-Invented-Here Syndrome"">if the task is a core business function</a>.  

<h4>Change Strategy</h4>
This section should cover your techniques for anticipating and designing for changes.  By this I mean future developments of the system, not necessarily changes in requirements, etc., during the life of the existing project.  Techniques include version numbers, reserving fields for future use, etc.

<h4>General Architectural Quality</h4>
This section should describe a polished conceptual whole with few ad hoc additions.  The objectives of the system should be clearly stated, and the motivations for all major decisions should be described.  The architecture should be largely machine- and language-independent, and should be neither under-specified nor over-specified (remember the concept of gilding the lily).  

Risky areas should be identified, and the architecture should contain multiple views.  These views should be from the perspectives of different concepts in the system (e.g., data flow vs. user work flow), and possibly also for different consumers of the document.  For example, in my team at work, our Functional Specifications are written for ourselves, Test, Security, Globalisation and Technical Publications.  We provide appendices that group specific changes together to make it easier for these different departments to sift through.  

Finally, the architectural description should make sense to you, otherwise how can you implement it?

<h3>Duration and Effort</h3>
The amount of time you should spend on the upstream pre-requisites depends entirely on the needs of your project.  Generally speaking, you should expend 10-20% of the effort and 20-30% of the schedule on pre-requisites.  Remember, too, that detailed design is part of construction, and not part of the planning phase.  

Unstable requirements will take extra time to formulate and finalise.  For example, you might be working with a requirements analyst (on large, formal projects), or having to expend more of your own time ensuring requirements are well-defined (on smaller, informal projects).  However, on any-sized project, <strong>treat requirements work as its own project</strong>, and use a similar approach to requirements development as for architecture development.

<hr />
Here are the Key Points from the end of the chapter; as such they cover the <a href=""http://www.codebork.com/coding/2009/05/21/code-complete-measure-once-cut-twice-part-1-importance-prerequisites.html"" title=""Code Complete: Measure Twice Cut Once (Part 1)"">previous post</a> as well as this one.  They are paraphrased from McConnell's original wording as before.

<ul>
  <li>The overarching goal of preparing for construction is risk reduction.</li>
  <li>Attention to quality must be part of the process from beginning to end (unless you're not fussed about quality)</li>
  <li>Part of a programmer's job is to educate non-techies about the software development process.  This includes the importance of adequate preparation.</li>
  <li>The kind of project you're working on significantly affects the prerequisites.  Iterative vs. Sequential.</li>
  <li>Without a good problem definition, you might end up solving the wrong problem.</li>
  <li>Without good requirements work, you might have missed important details.  Requirements changes are 1-2 orders of magnitude more expensive during/after construction as before.</li>
  <li>Without a good architectural design, you might be solving the right problem the wrong way.  Architectural changes are an order of magnitude more expensive during/after construction as before.</li>
  <li>Understand what approach has been taken to the prerequisites, and choose your construction approach accordingly.</li>
</ul>"
Key Construction Decisions,NULL,"This is a relatively short post, covering the key construction decisions Steve McConnell highlights in his book <em>Code Complete</em>.
<!--break-->
The choice of programming language is an important one to get right and may be difficult to decide.  It's worth keeping in mind the team of programmers you have at your disposable, as programmers are more productive using a language familiar to them.  Equally, you should aim for a higher-level language if you can, as programmers working with lower-level languages like assembler or C tend to be less productive than those working in high-level languages.  Additionally, you will often find that the languages you have learned stay with you.  This has the unfortunate side-effect of manifesting itself in ugly ways, such as stretching C++ to be Fortran, or C# to be Java (something I've seen at work), so keep an eye out for this in your own code.  <a href=""http://www.hanselman.com/"" title=""Scott Hanselman's Computer Zen"">Scott Hanselman</a> has a great post on this exact topic, where <a href=""http://www.hanselman.com/blog/BackToBasicsVarDim.aspx"" title=""Back to Basics: var != Dim"">he tries to bend C# 3.0's <code language=""csharp"">var</code> keyword to be VB's <code language=""vb"">Dim</code> keyword</a>.

McConnell provides the following example ordering (high-level vs. low-level) of various programming languages, relative to C.  The list is by no means complete (note the under-representation of Functional languages, for example), but, generally speaking, if your language isn't on there, one very similar to it is (e.g., PHP => Perl, C# => Java):

<table style=""width:100%;text-align: center"">
  <caption style=""caption-side:bottom;font-weight:bold;"">Source: Adapted from Estimating Software Costs (Jones&nbsp;1998), Software Cost Estimation with Cocomo II (Boehm&nbsp;2000), and ""An Empirical Comparison of Seven Programming Languages"" (Prechelt&nbsp;2000)</caption>
  <thead style=""font-weight: bold"">
    <tr>
      <td>Language</td><td>Level Relative to C</td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>C</td><td>1</td>
    </tr>
    <tr>
      <td>C++</td><td>2.5</td>
    </tr>
    <tr>
      <td>Fortran 95</td><td>2</td>
    </tr>
    <tr>
      <td>Java</td><td>2.5</td>
    </tr>
    <tr>
      <td>Perl</td><td>6</td>
    </tr>
    <tr>
      <td>Python</td><td>6</td>
    </tr>
    <tr>
      <td>Smalltalk</td><td>6</td>
    </tr>
    <tr>
      <td>Visual Basic</td><td>4.5</td>
    </tr> 
  </tbody>
</table>

McConnell then goes on to summarise the history of a number of languages, which I will not present here.  It's worth noting however, that McConnell's pr&eacute;cis is objective, and doesn't try to ""push"" or ""sell"" one language over another.  It's an interesting read.

Programming conventions are next to fall under McConnell's critical gaze, and he makes a good point that these conventions are important as a reflection of the system's conceptual integrity; they also help maintain it.  As such, the programming conventions to be used should be spelled out before work begins, as the details are too specific to be retrofitted.  

Last of all, McConnell points to something he calls ""your location on the technology wave"", because your position determines the richness of the available tools.  If, for example, you compare web development today, with all its frameworks (ASP.NET MVC, Rails, Django, CakePHP, to name just a few) and languages and developer tools, with web development in the early 2000s &mdash; let alone the 1990s! &mdash; you will see that in some ways it is less desirable to be a the front of the technology wave.  

""Late-wave"" environments provide all manner of benefits, such as a number of programming language choices; comprehensive error checking; powerful debugging tools; automatic, reliable performance optimisation; (nearly) bug-free compilers; good documentation; integrated tools; and training.  In contrast, ""early-wave"" environments provide the exact opposite.  Programmers must invest time trying to figure out how the language works; working around bugs in the language; coping with primitive tools; fixing code broken by new compiler/framework releases; and juggling multiple tools.

That's not to say you should avoid early-wave environments, however, as working in this space will often place you ahead of the competition.  For example, Lotus 123, MS Word, and Amazon were all built in this space.  And of course, the advice dispensed by <em>Code Complete</em> will help even more in primitive environments!  Most of the important principles in <em>Code Complete</em> depend not on specific languages, but on the way you use them.  

So, in closing, the Key Points to take away from this post are:
<ul> 
  <li>Every programming Language has its strengths and weaknesses.  Be aware of those of your chosen language.</li>
  <li>Establish conventions before you begin programming, as it's virtually impossible to make code match them later.</li>
  <li>More construction practices exist than can be used on your project.  Consciously choose the practices best-suited to your project.</li>
  <li>Are the programming practices you're using a response to the language, or controlled by it?</li>
  <li>Your position on the technology wave determines what approaches will be effective, or even possible.  Identify your current position and alter your plans and expectations accordingly.</li>
</ul>

<hr />

<h3>References</h3>
Boehm, Barry, et al. (2000) <em>Software Cost Estimation with Cocomo II.</em> Boston, MA: Addison-Wesley
Jones, Capers. (1998) <em>Estimating Software Costs.</em> New York, NY: McGraw-Hill
Prechelt, Lutz. (2000) ""An Empirical Comparison of Seven Programming Languages"", <em>IEEE Computer</em>, <strong>October 2000</strong> 23-29."
Code Complete: Design in Construction (Part 1 - Design Challenges),"Chapter 5 of <em>Code Complete</em> covers the concept of Design in software construction.  This is a pretty weighty chapter, so I'm going to tackle it in a mini-series of posts.  Here I cover McConnell's description of the design challenges, including why it's so hard to get right.  

This post is a something of a bite-size chunk, and hopefully it'll provide a measure of breathing space between two large posts.

[img_assist|nid=98|title=Design can be a wicked problem|desc=|link=none|align=center|width=400|height=292]
","
McConnell states that design links the requirements phase to coding and debugging: it is an intermediary representation of real-world issues and concepts.  As such, good design is always useful, but often it is indispensible; this is particularly true with the ever-increasing level of complexity in computer software.  

McConnell believes that design is a good example of a ""wicked"" problem, which Rittel and Webber (1973) describe thus:
<blockquote>
""One that could be clearly defined only by solving it, or by solving part of it""
</blockquote>
The upshot of this is that you have to ""solve"" the problem once in order to clearly define it.  You then go ahead and solve it again &mdash; and you do so better this time, given your better understanding of the problem &mdash; which results in a better problem definition.  This implies that design is an heuristic process, and possibly inherently iterative also.  

McConnell identifies the <a href=""http://en.wikipedia.org/wiki/Tacoma_Narrows_Bridge_(1940)"" title=""Wikipedia article on the Tacoma Narrows Bridge collapse in 1940"">Tacoma Narrows Bridge collapse in 1940</a> as a real-world example of a wicked problem.  A more up-to-date example that is arguably more relevant to my readership is that of <a href=""http://en.wikipedia.org/wiki/London_Millennium_Bridge"" title=""Wikipedia article on the London Millennium Bridge"">London's Millennium Bridge</a>: just three days after it was opened, the Millennium Bridge had to be closed because it wobbled.  The wobble arose from ""unexpected lateral vibration"" created by the pedestrians using the bridge; whilst bridges are designed to account for resonance caused by vertical loads, it was not known until the structure was completed that the problem of lateral vibration would need to be taken into consideration too.  It was particularly problematic at opening, because of the popularity of the structure: it was carrying up 2,000 people at any one moment, vastly increasing the amplitude of the lateral vibrations.  

[img_assist|nid=98|title=London's Millennium Bridge|desc=View from the Tate Modern gallery|link=none|align=center|width=600|height=438]

This is, by and large, something from which University courses will shelter you: if the requirements for a piece of coursework were to change constantly, the lecturer wouldn't be able to effectively assess your solutions.  The ""wickedness"" of the design problem is therefore vastly reduced for course assignments.  

McConnell also explains that even though design produces a tidy result, it is implicitly a sloppy process.  As you work towards a design, you will inevitably make a lot of mistakes and be led up a number of blind alleys.  This is, however, the whole point of the design process, and it is much safer to make those mistakes here than when you have already got code seared into the architecture.  Corollary: it is much harder to know when your design is ""good enough"", and it can sometimes be difficult to stop tinkering.  The Agile principle of YAGNI (You Ain't Gonna Need It) provides a useful counterbalance to this instinct.  

Design is all about trade-offs and priorities.  You will have to reach a compromise over competing design characteristics.  We recently had this scenario at work: there were two different approaches to solving one particular design problem, one that focussed on performance and one that might damage performance to an unknown degree but resulted in cleaner and more maintainable code.  The argument against the former was that it was premature optimisation; the argument for it was that performance is a key quality metric of our component and so we should not introduce anything that would damage that.  

Design invariably involves restrictions, and sometimes deliberate constraints are required.  One of the clearest signals that your design is not suitably constrained is the notion of ""feature creep"", where new requirements and features are added to the design on an on-going basis.  This invariably endangers projects, and can sometimes kill them altogether.  

Design is a non-deterministic process, in that there is no single right answer: there is always more than one way to skin a cat.  It is also an emergent process, in that it doesn't spring, fully-formed, from someone's brain.  The best way to achieve a good design is to do so iteratively, as if you were drafting a story, a blog post, or some other bit of written communication: produce multiple drafts.  Turn the wickedness of the problem to your advantage: solve it once, take what you learned from the process and solution and then solve it again, better.  Lather; rinse; repeat as needed.  

<hr />
<h3>References</h3>
Rittel, H. and Webber, M. (1973) ""Dilemmas in a General Theory of Planning."" <i>Policy Sciences</i> <b>4</b> 155-69"
Code Complete: Design in Construction (Part 2 - Key Design Concepts),"Design is essentially an exercise in managing complexity, and it is incredibly important to manage correctly.  Dijkstra (1989) stated that a single person working on a software development project needs to grapple with anything from one bit to a few hundred megabytes: this is 9 orders of magnitude.  Given that software is always increasing in complexity, McConnell posits that this figure could be as much as 15 orders of magnitude or more today.  

This post covers in some depth the issues around managing complexity, ways to attach it, and the importance of doing so.  It will also cover desirable characteristics of design, and the different levels of design.  

<strong>Note: this is a long post!</strong>
","
Design is essentially an exercise in managing complexity, and it is incredibly important to manage correctly.  Dijkstra (1989) stated that a single person working on a software development project needs to grapple with anything from one bit to a few hundred megabytes: this is 9 orders of magnitude.  Given that software is always increasing in complexity, McConnell posits that this figure could be as much as 15 orders of magnitude or more today.  

Brooks (1987) describes complexity in terms of Aristotle's essential and accidental properties.  An essential property is something that must be there in order for the item to be classed as such; for example, a car must have an engine, wheels, and a cabin.  An accidental property, meanwhile, is something the item just happens to have; re-using the car example, it might have a V8 engine, 5 doors, and/or a convertible roof.  The accidental <em>difficulties</em> of programming were mostly solved some time ago, such as by moving from assembler to High-Level Languages such as Java and C#, or moving from batch operating systems to time-sharing operating systems.  There are many other examples.  The progress on the <em>essential</em> difficulties of programming, however, has been slower as might be expected: software development is a process of determining the details of a ""highly-intricate, interlocking set of components"", which arise from interfacing with the real world.  As such, the details can require exact correctness, etc.  Even with Domain-Specific Languages, programming is still hard.  As such, the root of all the essential difficulties in programming is complexity, both accidental and essential.

McConnell lists managing complexity as the single most important technical topic in software development, and goes on to refer to this task as software's Primary Technical Imperative.  Dijkstra (1989) stated that a single mind needs to grapple with anything from one bit to a few hundred megabytes: 9 orders of magnitude.  With software complexity closely following an exponential path, McConnell posits that this figure could realistically be 15 orders of magnitude or more today.  

The consequence of this statistic is that no one brain can store all the details of a modern computer program (Dijkstra, 1972).  Programs should therefore be organised in a way to allow us to safely focus on one bit at a time.  There are a number of techniques for achieving this, including:
<ul>
  <li>Using subsystems, loose coupling, modularisation, orthogonality, well-designed objects and packages...</li>
  <li>Keeping routines short</li>
  <li>Writing programs in terms of the problem domain (rather than the low-level implementation details)</li>
  <li>Working at the highest level of abstraction</li>
</ul>

Programmers who compensate for inherent human limitations write code that's easier for themselves and others to understand, and that has fewer errors.

Overly costly, and therefore ineffective, designs arise in three ways: either you have created a complex solution to simple problem; or you have coded a simple but incorrect solution to complex problem; or your solution is both inappropriate and complex for a non-trivial problem.  I reckon the last of these must be pretty terminal.  Re-visiting the Aristotelian definitions of complexity, you can see managing complexity as a two-stage process: minimizing the amount of essential complexity that has to be addressed at any one time, and preventing accidental complexity from needlessly proliferating.  These two activities will help you avoid the three classes of ineffective design.  

<a href=""http://en.wikipedia.org/wiki/R._Buckminster_Fuller"" title=""Wikipedia article on Buckminster Fuller"">R. Buckminster Fuller</a> famously said that,

<blockquote>""When I am working on a problem I never think about beauty.  I think only how to solve the problem.  But when I have finished, if the solution is not beautiful, I know it is wrong.""</blockquote>

It's very easy to get hung up on trying to create an elegant or a beautiful design.  This is a little bit like trying to delicately paint the ears and stalks of wheat into a field before you've sketched out where the field will actually sit on the canvas.  That is to say, the beauty of the painting has more to do with the proportions of the different elements (golden ratios, etc.) than it does the individual elements.  If you sketch the painting well, it will be beautiful.  

McConnell lists the following ""internal"" characteristics of design; these he describes as inherent only to design, not other software-quality attributes.  
<ul>
  <li>Minimal complexity: no ""clever"" designs, just ""simple"" and ""easy-to-understand"".</li>  
  <li>Ease of maintenance: imagine the questions a maintenance programmer would ask about your code</li>
  <li>Loose coupling</li>
  <li>Extensibility</li>
  <li>Reusability</li>
  <li>High fan-in: have a high number of classes that use a given class. Implies it was designed to make good use of utility classes at lower levels.</li>
  <li>Low-to-medium fan-out: a given class uses a low-to-medium number of other classes.  High fan-out (>~7) indicates high complexity.</li>
  <li>Portability</li>
  <li>Leanness: has no extra parts. (Elegance?) A book is not finished when nothing more can be added, but when nothing more can be taken away (Voltaire).</li>
  <li>Stratification: Keep levels of decomposition stratified - view system at any level and get a consistent view (i.e., without dipping into other levels).  Open-Closed Principle?</li>
  <li>Standard techniques: e.g., patterns.</li>
</ul>

McConnell describes a number of levels of software design, ranging from an entire system right down to an individual routine.  The main problem encountered with the design of an entire system is that developers sometimes jump straight from here to class design.  This is obviously unwise, and can lead to some major issues down the line which might require heavy refactoring or a large design overhaul.  

The second level of design is that of subsystems, or packages; this can be expected to last a few weeks per subsystem.  After all major subsystems have been identified, the usage and communication scenarios between subsystems must be defined.  It is important to follow a couple of key heuristics here to achieve an elegant and loosely-coupled design.  For example, it is easier to be more restrictive at design-time and relax those restrictions later on that it is to try the other way around.  Erring on the side of simpler inter-system relations will help reduce coupling; calling a routine should be chosen over using a class, and the last resort should be to inherit a class.  Drawing a diagram of your communications flows will help you identify circular dependencies; the diagram should be acyclic to achieve maximum flexibility and efficiency.  Examples of common subsystems include business rules (laws, regulations, policies and procedures, etc.), the user interface, data access and system dependencies. Using the <abbr title=""Model-View-Controller"">MVC</abbr>, <abbr title=""Model-View-Presenter"">MVP</abbr> or <abbr title=""Model-View-ViewModel"">MVVM</abbr> can help achieve loose coupling between the UI and other subsystems, whilst an <abbr title=""Object-Relational Mapper"">ORM</abbr> or similar library helps to hide the implementation details of the database.  System dependencies should be isolated to maximise portability to other platforms. 

Class design is the third level, and can take anything up to a few days per class to complete.  All classes in the subsystem need to be identified and enumerated.  A database subsystem, for example, might be partitioned into a data access class, a persistence framework (which might have a number of classes), and metadata.  Class interactions must also be defined, and it's worth applying the <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html#ISP"">Interface Segregation</a> and <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html#DIP"">Dependency Inversion</a> principles here.  

Routine design is a natural successor to class design, as the interfaces defined in level 3 will declare some routines.  This fourth level defines the <em>private</em> routines on each class.  Often the process of completing this level of design results in a better understanding of the interface and so might cause knock-on changes in level 3.  You can expect to spend in the order of a few hours on each routine, and the responsibility falls to the individual programmer implementing the routine.  

The final level of design is that of each routine's internals.  This involves laying out the detailed functionality of the individual routines, and the responsibility again falls to the individual programmer.  Methods for achieving this can include writing pseudocode, looking up algorithms, or programming.  

<hr />
<h3>References</h3>
Brooks, F. P., Jr. (1987) ""No Silver Bullets - Essence and Accidents of Software Engineering"" <i>Computer</i>, <b>4</b> 10-19
Dijkstra, E. (1972) ""The Humble Programmer."" <i>Communications  of the ACM</i>. <b>15</b> 10 859-66
Dijkstra, E. (1989) ""On the Cruelty of Really Teaching Computer Science."" <i>Communications of the ACM</i>. <b>32</b> 12 1397-1414"
Code Complete: Design in Construction (Part 3 - Design Building Blocks: Heuristics),"Ok, so this is my first post on <em>Code Complete</em> for a little while; it turned out the busy period at work lasted a good couple of weeks longer than I thought it would!  It was a quite a while back that I made these notes (mid-June, in fact), so if the post seems less coherent or I've got something obviously wrong, please leave a comment.  Here be dragons.

This post deals with design heuristics.  We've already touched on what heuristics are in the <a href=""http://www.codebork.com/2009/05/13/code-complete-software-development-metaphors.html"" title=""Code Complete: Software Development Metaphors"">Software Development Metaphors</a> post, so you might want to refresh your understanding before reading on.  Inside, I will cover McConnell's description and critical evaluation of the most common design heuristics.  These can be viewed as smaller steps in a larger process, or as individual methods to use at different stages of the design process.

<strong>Note: this is a long post!</strong>

[img_assist|nid=103|title=Chocolate Heuristics|desc=Copyright &copy; 2006 <a href=""http://www.flickr.com/photos/maadmob/"" title=""maadmob on Flickr"">maadmob</a>|link=url|url=http://www.flickr.com/photos/maadmob/|align=center|width=400|height=330]
","
Ok, so this is my first post on <em>Code Complete</em> for a little while; it turned out the busy period at work lasted a good couple of weeks longer than I thought it would!  It was a quite a while back that I made these notes (mid-June, in fact), so if the post seems less coherent or I've got something obviously wrong, please leave a comment.  Here be dragons.

This post deals with design heuristics.  We've already touched on what heuristics are in the <a href=""http://www.codebork.com/2009/05/13/code-complete-software-development-metaphors.html"" title=""Code Complete: Software Development Metaphors"">Software Development Metaphors</a> post, so you might want to refresh your understanding before reading on.  Inside, I will cover McConnell's description and critical evaluation of the most common design heuristics.  These can be viewed as smaller steps in a larger process, or as individual methods to use at different stages of the design process.

[img_assist|nid=103|title=Chocolate Heuristics|desc=Copyright &copy; 2006 <a href=""http://www.flickr.com/photos/maadmob/"" title=""maadmob on Flickr"">maadmob</a>|link=url|url=http://www.flickr.com/photos/maadmob/|align=center|width=400|height=330]

The first heuristic covered is by-the-book Object Orientation, <strong>finding real-world objects</strong> that model the system's concepts.  This is not stated in the sense that ""oh, the Peregrine Falcon is a perfect description of the order processing subsystem: efficient and graceful"", but rather that an order is placed for a customer; has a number of lines on the invoice, each of which is for a stock item, and a total price; and might be processed by a particular member of staff.  What I've done in that last sentence is to identify some of the common objects (customer, order, item, employee) and attributes (lines, total price).  Furthermore, we can look at the actions that can be done to each object (""place the order"", ""charge the customer"", ""ship the order""), and what each object can do others (an Account Manager might place the order on behalf of the customer).  At a greater level of detail, we can determine the visibility of the object's members (e.g., the customer might not be able to view the item's identifier; the employee might not be able to view the customer's credit card details; the order can view the totals of all the order lines), and from that define each object's public interface. 

The next heuristic is to <strong>form consistent abstractions</strong> across the system you are designing.  This is accomplished by defining good class interfaces, and providing larger programming abstractions.  McConnell illustrates this last point with the metaphor of building a house: a builder will install doors after the carpenter has crafted them from the base materials (e.g., wood, steel, glass, etc.).  The builder works with the larger abstraction of ""door"" over the smaller abstraction of ""large sheet of glass"".  

The third heuristic, <strong>encapsulating implementation details</strong>, is an effective technique for managing complexity.  If the encapsulation is done well, it prevents you from looking at the complexity, and then the old adage of ""out of sight, out of mind"" starts to take hold.  If your encapsulation is poor, you end up having to manage the implementation detail (and therefore more of the complexity) on a daily basis.  

The OO idea of inheritance is a powerful one, and as such McConnell suggests it be treated with care and respect.  In fact, he goes a bit further than that to suggest that <strong>it should only be used when it simplifies the design</strong>.  There was recently an example at work that bears this out.  In my component, we have two Change Password operations, one for each of before and after the user has logged in.  The ""external"" (pre-login) Change Password class inherits from the ""internal"" (post-login) class, which has saved a fair bit in terms of duplication of code, etc.  However, there's not a great deal of inheritance used elsewhere in the component, and so tracing the flow of execution between the two is non-obvious and can be quite tricky, not least because both classes are called <code language=""java"">ChangePassword</code>!

McConnell's fifth heuristic covers the idea of <strong>information hiding</strong>, specifically secrets.  This has benefits in that again it emphasises hiding complexity, this time based on the iceberg principle (i.e., that the vast majority of the inner workings of the class, and therefore its complexity, should be behind the public interface).  Information hiding will help you at all levels of the design, be it working out how the high-level components of the system fit together, or how a subsystem processes input from a file.  

[img_assist|nid=102|title=Complexity should be 7/8 below the surface too|desc=|link=node|align=center|width=219|height=299]

There are some barriers to information hiding, however.  Most notably, these include circular dependencies and perceived performance penalties from creating more classes, hitting more methods, etc.  In reality, these days, the performance penalty caused by information hiding is negligible in all but the most esoteric of situations.  Other behaviours that can cause difficulties with information hiding are excessive distribution of information (where too many diverse bits of the system know about the same bit of information), and class data being mistaken for global data.  

However, information hiding has proven to be useful in many systems implemented, and McConnell states that it has a unique ability to produce effective design solutions.  You should, therefore, get into the habit of asking <strong>""What should I hide?""</strong>.

The sixth heuristic is to <strong>identify areas likely to change</strong>.  McConnell proposes a three-step process for achieving this:
<ol>
  <li>Identify items likely to change</li>
  <li>Separate items that are likely to change</li>
  <li>Isolate items that seem likely to change</li>
</ol>

There are a number of items that are likely to change.  Examples include business rules (e.g., keeping up with changing tax regulations, etc.), hardware dependencies, non-standard language features, status variables, areas that are difficult to design and construct, and more.  Separating these items involves compartmentalising them into their own classes, or grouping them into a class with similar items.  To achieve step 3, isolation of items likely to change, you need to ensure the public interfaces are insensitive to the potential changes.  This does involve anticipating areas of change, which is no mean feat, but I am starting to find that this is the sort of thing that experience teaches you.

<strong>Reducing coupling</strong>, and keeping it loose, is an excellent design heuristic, and something that <a href=""http://www.pragprog.com/"" title=""The Pragmatic Bookshelf"">the Pragmatic Programmers</a> describe at some length.  Taking their analogy, a tightly-coupled system is like a helicopter: moving the joystick requires adjustments to the rudder and other controls to keep the craft balanced.  Everything is inter-related, and a small change here could affect something else in a non-obvious, or trivial, or confusing way.  A loosely-coupled system, however, provides small, direct, visible and flexible relations.  The size of a coupling is measured by the number of connections, whilst the visibility is measured by the prominence of the connection.  Flexibility describes how easily the connections change.  

McConnell lists a small number of examples of the types of coupling (in ascending order, loosely-coupled to tightly-coupled):
<dl>
  <dt><strong>Simple-data-parameter:</strong></dt> <dd>all data are of primitive 
types and passed through parameter lists</dd>
  <dt><strong>Simple-object:</strong></dt> <dd>instantiates the object</dd>
  <dt><strong>Object-parameter:</strong></dt> <dd>if A requires B to pass it a C then B needs to know about C too</dd>
  <dt><strong>Semantic:</strong></dt> <dd>A uses some semantic knowledge of another module's inner workings</dd>
</dl>

I found these slightly confusingly-named, but well-explained.  Hopefully the summarised explanations above show that the concepts aren't too tricky to understand.  

McConnell then covers the idea of <strong>using common patterns</strong> in your designs.  These can reduce complexity through their ready-made abstractions, and can reduce errors by institutionalising details of common solutions.  There is a huge amount of value in being able to say ""we're writing a Database Abstraction Layer here, the Bridge pattern describes this situation well"", both in terms of easing your own understanding of the problem and the implementation.  In and of themselves, they provide heuristic value by suggesting design alternatives, and can streamline communication by moving the conversation to a higher level.  It's much easier to talk about an interface that's implementing the Fa&ccedil;ade pattern by referring to a Fa&ccedil;ade than it is by frequently and repeatedly describing the interface as ""something the simplifies the use of the underlying subsystem by grouping the most common and useful functionality into the single interface"".  

Last of all, McConnell covers a small selection of other design heuristics.  I'm not going to cover them in any detail here, but instead simply list them with a short description.  

<dl>
  <dt><strong>Aim for strong cohesion:</strong></dt> <dd>i.e., highly-focussed classes and modules</dd>
  <dt><strong>Build hierarchies:</strong></dt> <dd>improve organisation</dd>
  <dt><strong>Formalise class constructs:</strong></dt> <dd>preconditions and postconditions</dd>
  <dt><strong>Assign responsibilities</strong></dt>
  <dt><strong>Design for test:</strong></dt> <dd>TDD can help, but it's not the be-all and end-all</dd>
  <dt><strong>Avoid failure:</strong></dt> <dd>consider ways the system might fail, e.g., threat modelling</dd>
  <dt><strong>Choose binding time consciously</strong></dt>
  <dt><strong>Make central points of control:</strong></dt> <dd>One Right Place</dd>
  <dt><strong>Consider using brute force</strong></dt>
  <dt><strong>Draw a diagram</strong></dt>
</dl>"
Code Complete: Brief Interlude,"I have a couple of new posts waiting in the wings for this series.  They're both quite lengthy, so they're taking a bit of time to write.  

Things have also got considerably busier at work in the last week or so, and it's proving nigh on impossible to dedicate the time to reading and absorbing the material.  As such, there will likely be a bit of a gap in my postings on this series after I've cleared out the back-log.  

If you've been enjoying the series so far, and/or if you have any feedback on how to improve it, etc., please leave me a comment or two.  
","I have a couple of new posts waiting in the wings for this series.  They're both quite lengthy, so they're taking a bit of time to write.  

Things have also got considerably busier at work in the last week or so, and it's proving nigh on impossible to dedicate the time to reading and absorbing the material.  As such, there will likely be a bit of a gap in my postings on this series after I've cleared out the back-log.  

If you've been enjoying the series so far, and/or if you have any feedback on how to improve it, etc., please leave me a comment or two.  

'til next time...  Happy coding :-)"
Superuser.com Private Beta,NULL,"It was <a href=""http://blog.stackoverflow.com/2009/07/super-user-semi-private-beta-begins/"" title=""Super User Semi-Private Beta begins"">announced</a> a few days ago that the <a href=""http://superuser.com/"" title=""Super User"">Super User</a> private beta has begun.  Instructions on accessing it are available from <a href=""http://blog.stackoverflow.com/2009/07/super-user-semi-private-beta-begins/"" title=""Super User Semi-Private Beta begins"">the Stack Overflow blog announcement</a>.
<!--break-->
Jeff Atwood <a href=""http://blog.stackoverflow.com/2009/05/the-stack-overflow-trilogy/"" title=""The Stack Overflow trilogy"">described a while back</a> how he always envisaged a trilogy of Stack Overflow sites, using the trilogy of the original <em>Star Wars</em> films as a touchstone.  Super User is, therefore, the <em>Return of the Jedi</em> of the trilogy, and <a href=""http://blog.stackoverflow.com/2009/05/the-stack-overflow-trilogy/"" title=""The Stack Overflow Trilogy"">comes with the appropriate health warning Ewoks</a>:
<blockquote>
Superuser.com  <strong>For computer enthusiasts and power users</strong>

One word: <em>ewoks</em>. But also, Leia in a bikini. Still canon, but little odder than the earlier movies. In other words, things are going to get a little .. crazy .. in the finale.
</blockquote>

This is a (mostly) no-holds barred community site, where any question on computers is fair game.  

Oh, and if you link your various accounts together, provided you have >200 rep on the other site, you get a +100 bump in rep.  You can also import your profile details from the other sites.  

Why not check it out?  And <em>do</em> play nicely with the Ewoks!"
Small update,"That busy period at work that I mentioned hasn't really ended, and by the sounds of things it won't be any time soon.  As such, it's likely that my Code Complete series is going to be put on hold for a while longer as we ride this current and move on.
","That busy period at work that I mentioned hasn't really ended, and by the sounds of things it won't be any time soon.  As such, it's likely that my Code Complete series is going to be put on hold for a while longer as we ride this current and move on.

I also went away for three weeks which didn't help my blogging any :-)  I got back yesterday from a week in Northumberland, preceded by two weeks in Bordeaux.  This was a rather longer stay than intended: on the eighth day (of ten) one of the cars we took abroad broke down and had to be towed away to be mended by the ""local"" Seat dealership.  The other car went home as planned with just under half the group that had to be back at work the following day, whilst the rest of us stayed behind to wait the car's repair.  The problem turned out to be worse than originally thought.  More than just a snapped fan belt and broken air conditioning both were replaced over the course of two days.  By the time we got back to the UK late on the evening of Friday 28 August, approximately 48 hours late, we'd developed a serious case of cabin fever, and we were borderline hysterical.  Needless to say I needed a holiday to recover from the holiday, and so I spent another twelve hours driving to Northumberland.  I had a very relaxing week amongst the bleak and remote scenery there, and I'm now just about rested enough to go back to work!

I'll try to get my blog back on track on other topics and home projects that are hopefully of interest to you, dear reader."
O (2001),NULL,"[3.5/5]

Over the last couple of weeks, I've found myself watching a couple of Shakespeare re-tellings, including <a href=""http://www.imdb.com/name/nm0525303/"" title=""Baz Luhrmann on IMDB"">Baz Luhrmann's</a> rather excellent 1996 version of <em>Romeo and Juliet</em>.  <em>O</em> doesn't quite measure up to Luhrmann's visionary re-telling of star-crossed lovers, but in many respects it's rather unfair to compare the two as they deal with their source material in very different ways: Luhrmann and <a href=""http://www.imdb.com/name/nm0668902/"" title=""Craig Pearce on IMDB"">Craig Pearce</a> update the surroundings, dress and props for the late twentieth century, but leave the dialogue in Shakespearean English; in <em>O</em>, <a href=""http://www.imdb.com/name/nm0433994/"" title=""Brad Kaaya on IMDB"">Brad Kaaya</a> provides a fresh environment for the tale of <em>Othello</em> in his screenplay.
<!--break-->
Set in an American high school and based around the school's basketball team, Kaaya and director <a href=""http://www.imdb.com/name/nm0625789/"" title=""Tim Blake Nelson on IMDB"">Tim Blake Nelson</a> work with the somewhat clichéd metaphor of sport as war.  <a href=""http://www.imdb.com/name/nm0001616/"" title=""Mekhi Phifer on IMDB"">Mekhi Phifer</a> plays Odin (Othello), the school's basketball star.  Odin is almost single-handedly taking his school's team to the major leagues, and professional representatives are showing an interest.  The team's coach, Duke Goulding (<a href=""http://www.imdb.com/name/nm0000640/"" title=""Martin Sheen on IMDB"">Martin Sheen</a>), loves Odin like a son, which makes his own son Hugo (Iago, played by <a href=""http://www.imdb.com/name/nm0001326/"" title=""Josh Hartnett on IMDB"">Josh Hartnett</a>) jealous.  

From the very outset we get a sense of Hugo's jealous nature, and after the opening soliloquy, we find Roger (Roderigo) complaining to Hugo about how Odin is dating the beautiful Desi Brable (Desdemona, played by <a href=""http://www.imdb.com/name/nm0005466/"" title=""Julia Stiles on IMDB"">Julia Stiles</a>), daughter of the Dean (Brabantio).  Hugo is upset because Odin has publicly recognised the support of his team-mate Cassio (<a href=""http://www.imdb.com/name/nm0005080/"" title=""Andrew Keegan on IMDB"">Andrew Keegan</a>) at a recent awards ceremony, and so they hatch a plot to break Odin and Desi apart.  It is not long, however, before things take a more sinister edge, with Odin being accused of forcing himself upon Desi and Hugo nearly being caught in the frame.  

Odin gives Desi a scarf that belonged to his grandmother as a token of his affection.  After the close call in his first plot, Hugo realises he has to ""up his game"", and drives a wedge between Odin and Cassio, eventually leading to doubt in Odin's mind about Desi's faithfulness to him.  He calls on his friend Emily (Emilia), Desi's room-mate, to steal the scarf for him, and passes it to Cassio to give to the real object of his affection, Brandy (Bianca).  Eventually, Odin is so consumed with rage and jealousy that, fuelled further by drugs, he kills Desi and later commits suicide.  Hugo is arrested, claiming in a closing soliloquy that he will have his day in the spotlight.  

The opening and closing soliloquies, narrated over internal shots of the school's bird loft, neatly but obviously reflect the film's tagline that ""Everything comes full circle"".  The re-telling of <em>Othello</em> with contemporary themes and surroundings works, but I was unconvinced by the setting of the film in a school; I can't help but wonder if the remarkably faithful adaptation of the plot from the source material is a contributing factor.  Also, American teenage films and TV shows often leave me quite cold; the American high-school experience is completely alien to me, and from what I see through the lens, it seems as if no-one has ever had a good experience of high-school.  This is one of the reasons I gave up on <em>Heroes</em> and never got into <em>Smallville</em>, for example.  

Many of the themes and scenes from the play are well-handled, and the closing sequence is really weighed down by the gravity of what Hugo has done to his team and people who might once have considered him a friend.  While I liked the idea of taking the story out of its original setting, I found Kaaya's use of a sports team in place of the original army a little lazy; given how little actual basketball made it into the film, almost any other scenario could have been presented, potentially even something non-competitive.  The key to the plot is conveying the themes of jealousy and racism, and only really the first of these actually came through.  

In short, this is an interesting update to Shakespeare's play that makes the plot accessible to an audience that might otherwise be disinterested in Shakespeare.  By setting it in a high-school environment, however, it really becomes ""<em>Othello</em> for teenagers"" which will inevitably put off older audiences."
William Shakespeare's Romeo and Juliet,NULL,
Code Complete: Design in Construction (Part 4 - Design Practices),NULL,"Wow, it's been quite a while since I updated my <em>Code Complete</em> series, and I've got quite the backlog to wade through now!  Looking at the last save date on this post, it's been sat around for three months waiting to be written, so I'm sorry for being so slack, and I'll get on with writing it now...

This post covers some key design practices, and is the last post on Chapter 5, Design in Construction.
<!--break-->
The simplest design practice is to <strong>iterate</strong> your designs.  Your second attempt at solving a problem is nearly always better than the first: this is somewhat because <a href=""http://www.codebork.com/coding/2009/06/11/code-complete-design-construction-part-1-design-challenges.html"" title=""Code Complete: Design in Construction (Part 1 - Design Challenges)"">design is an example of a ""wicked problem""</a>, i.e., one that can be clearly defined by solving it in whole or part.  Note also that iteration requires the critical appraisal of the design after each iteration, allowing for kinks, mistakes, and omissions to be corrected.  

Another option available to you in design is the notion of divide and conquer.  This is a useful tool for decomposing a complex system into different areas of concern, and will help introduce greater modularity into your system, making it more loosely-coupled.  All good stuff.  If you run into a dead end when dividing your system, fence it off and start a new iteration using the knowledge that you've gained from the first attempt.

Useful with divide and conquer are top-down and bottom-up decomposition.  Top-down decomposition starts at a high level of abstraction, drilling down into the detail in successive applications of the technique; you should keep decomposing until the next level is ""obvious"" and therefore easier to code than decompose further.  The advantage of this approach is that it makes complex systems easier to deal with: it introduces fewer details at any given time, and allows you to defer construction details until it is most appropriate to deal with them.  

Bottom-up decomposition works in the opposite direction, starting with the specifics and building successive generalisations on top of them.  This is useful when the top-level abstraction is poorly-defined or -understood; sometimes it's just easier to start with the specifics.  It also provides a couple of advantages in construction: utility functionality is identified earlier (thereby producing a compact and well-factored design); and code reuse becomes more obvious and easier to consider.  However, it is very hard to use exclusively, and sometimes you can't build a coherent system from the pieces identified.  As a result, you should bear in mind the following tips when using bottom-up design:
<ul>
  <li>Keep asking yourself what you <em>know</em> the system should do</li>
  <li>Identify concrete objects and responsibilities from that question</li>
  <li>Identify common objects and group them using subsystem organisation, packages, composition within objects or inheritance (whichever is most appropriate for the scenario).</li>
  <li>Continue with the next level up, or skip to the top and try to work down.</li>
</ul>

Sometimes, however, you just have to try something out to know if it will work, and this is where experimental prototyping has a role to play.  It can be very easy to slip into making a polished prototype, or at the very least, something more than that which is most definitely must be, which is <strong>the absolute minimum required <em>throwaway</em> code.</strong>  For example, when assessing database performance, create the right number of tables called Table1, Table2, etc., with the right number of columns each called Column1, Column2, &hellip; , and fill the rows with <em>junk</em> data.  In order to achieve the best results from prototyping, the question forming the basis of the scenario must be specific.  ""Will X work?"" is not specific enough; ""Will X support Y under assumption Z"" is.  Finally, the code created must be throwaway; if there is any belief that it will be re-used, the developer(s) will end up producing an implementation and not a prototype.  

Collaborative design practices are very useful tools to keep on your belt; the value of an extra pair of eyes is not to be underestimated.  There are many ways of utilising a collaborative approach ranging from the informal (bouncing ideas around with a co-worker, whiteboarding) to the formal (pair programming, formal inspections).  If you don't have a team to collaborate with, it's possible to get some of the benefits of the collaborative process by using self-reviews: produce a design, put it away for a week, and then come back and appraise it.  You will be looking at it afresh, and will be able to evaluate it as another might.  

McConnell poses the important question of how much design is sufficient for a project in this chapter, and answers it with the following table:

<table style=""text-align: center"">
  <thead style=""font-weight: bold; border-top: 2pt solid silver; border-bottom: 2pt solid silver"">
    <td style=""text-align: left; border-right: 1pt solid silver"">Factor</td>
    <td style=""border-right: 1pt solid silver"">Level of Detail Needed in Design Before Construction</td>
    <td>Documentation Formality</td>
  </thead>
  <tbody>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Design/construction team has deep experience</td>
      <td style=""border-right: 1pt solid silver"">Low Detail</td>
      <td>Low Formality</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Design/construction team has deep experience but is inexperienced in the application's area</td>
      <td style=""border-right: 1pt solid silver"">Medium Detail</td>
      <td>Medium Formality</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Design/construction team is inexperienced</td>
      <td style=""border-right: 1pt solid silver"">Medium to High Detail</td>
      <td>Low-Medium Formality</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Design/construction team has moderate-to-high turnover</td>
      <td style=""border-right: 1pt solid silver"">Medium Detail</td>
      <td>-</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Application is safety-critical</td>
      <td style=""border-right: 1pt solid silver"">High Detail</td>
      <td>High Formality</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Application is mission-critical</td>
      <td style=""border-right: 1pt solid silver"">Medium Detail</td>
      <td>Medium-High Formality</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Project is small</td>
      <td style=""border-right: 1pt solid silver"">Low Detail</td>
      <td>Low Formality</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Project is large</td>
      <td style=""border-right: 1pt solid silver"">Medium Detail</td>
      <td>Medium Formality</td>
    </tr>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Software is expected to have a short lifetime
(weeks or months)</td>
      <td style=""border-right: 1pt solid silver"">Low Detail</td>
      <td>Low Formality</td>
    </tr>
    <tr style=""border-bottom: 2pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver"">Software is expected to have a long lifetime
(months or years)</td>
      <td style=""border-right: 1pt solid silver"">Medium Detail</td>
      <td>Medium Formality</td>
    </tr>
  </tbody>
</table>

There are a couple of caveats, of course.  First, the terms ""Medium detail"", ""low formality"", etc., are a little vague (particularly the documentation measures), so require guess-work (ideally grounded in experience) to decide what constitutes ""high detail"".  Additionally, two or more of these factors might be relevant to your project, and sometimes they may contract each other (e.g., a small, long-lived, mission-critical application).  In these scenarios, you need to use your judgement to decide how far to go.  

Finally, McConnell describes ways of capturing your design work.  These range from using formal design documentation methods, such as Class-Responsibility-Collaboration Cards (CRC cards, produced using index cards, listing the class name, its responsibilities and classes that co-operate with it) through to using wikis to capture design discussions and decisions, and inserting design documentation into the code itself (e.g., using file/class comments, which are particularly powerful if coupled with a comment-parsing system such as JavaDoc or Doxygen).  Other suggestions include writing email summaries to the team and archiving them publicly; using a digital camera to photograph whiteboard drawings; using UML diagrams at appropriate levels of detail; and saving design flip charts (which can be used instead of re-writing all that information into a formal document)."
Code Complete: Working Classes (Part 1 - Class Foundations: Abstract Data Types),NULL,"Another day, another <em>Code Complete</em> blog post.  I might even clear two today, looking at the number of notes I have for this one.  

The <a href=""http://www.codebork.com/2009/09/29/design-practices.html"" title=""Code Complete: Design in Construction (Part 4 - Design Practices)"">last post</a> rounded off the contribution to the discussion around the problems of designing software.  A new chapter means a new topic, and chapter 6, titled ""Working Classes"", deals with tips on, and the issues around, creating classes in your program.  First up is the topic of Abstract Data Types: what they are, why you should be interested, and how you should use them.
<!--break-->
An Abstract Data Type (ADT) is a collection of data and operations that work on that data.  As a result of this, ADTs allow you to work in the problem domain (""stock control system"") rather than the implementation domain (""PHP and MySQL"").  The use of ADTs provides a number of benefits:
<ul>
  <li>Hide implementation details</li>
  <li>Changes don't affect the whole program</li>
  <li>Make the interface more informative</li>
  <li>Easier to improve performance</li>
  <li>Program is more obviously correct</li>
  <li>Program becomes more self-documenting</li>
  <li>Don't have to pass data all over your program</li>
  <li>Able to work with real-world entities rather than with low-level implementation structures</li>
</ul>

ADTs can be defined from any system, be they relatively simple or highly complex.  For example, an ADT for a light would be very simple, with only two operations (switch on, switch off), and a private field for storing the current state of the light.  

McConnell provides a number of guidelines for defining ADTs.  First off, he suggests <strong>building low-level data types as ADTs, not from primitive data types</strong>; even simple items should be considered ADTs.  A list representing a set of billing records, for example, should be treated as a set of billing records, and not as a simple list.  Using the example of the light above, you might be tempted to dispense with the ADT and use a boolean value to indicate whether the light is on or off, reading and writing that boolean directly.  Using the ADT, however, improves the readability of your code, and improves its factoring: even using a boolean named <code>lightOn</code>, it is more intuitive to call <code>light.turnOn()</code> and <code>light.turnOff()</code> than it is to code <code>lightOn = true</code> and <code>lightOn = false</code>.  You also get the benefits of using the ADT, such as encapsulating the current state of the light and abstracting away the implementation details.  

The second tip is to <strong>treat common objects, such as files, as ADTs.</strong>  This is made quite simple by .NET and the JDK thanks to the various IO classes such as File, although this is less true of some of the scripting languages (ones dealing with file handles, such as Perl and PHP, spring to mind).  

Consider <strong>layering ADTs similar to the Operating System and/or the language abstractions.</strong>  For example, the OS doesn't make you position the read/write head at a specific phsyical address, and the language provides ADTs so you don't have to make tricky OS calls.  Endeavour to create your ADTs at a similar <em>or higher</em> level of abstraction, as is appropriate to the situation, and utilise layering to make your programming easier.  

Finally, <strong>ensure you refer to an ADT independently of the medium on which it's stored.</strong>  For example, you might be tempted to refer to a really large ADT as ""Large ADT File"", and have a Read() method.  This undoes the abstraction a bit and exposes more information than is needed.

<em>Exercise for the reader:</em> Using the above guidelines, see if you can define an ADT for a liquidiser (a blender for any American readers), an elevator, or a cruise control system.  

Of course, in non-OO languages such as C, you'll need to add in support for handling multiple instances of your ADT, such as self-rolled ""constructors"" and ""destructors"".  You will also likely need to introduce some kind of ID to distinguish between the different instances.  McConnell goes into more detail on this than I do here (I expect my readership is almost exclusively working in OO languages), so if you're interested in learning more, you will need to read the book!"
Code Complete: Working Classes (Part 2 - Good Class Interfaces),NULL,"This post represents the second instalment taken from chapter 6 of <em>Code Complete</em>, entitled ""Working Classes"".  This post covers the issues to consider when designing class interfaces, illustrated with code samples.  An important piece of information to keep in mind when reading this post is that McConnell is talking in terms of the public interface exposed by a <em>class</em> through its public members.  While an interface (as defined in Java or C#) also fits this bill, some of the advice given here is specific to the idea of a class interface, and not a standalone interface.

If you're after the executive summary (this is quite a long post, after all), there are only two things you must build into your class interfaces: <strong>good abstraction and good encapsulation.</strong>  Read on to find out more.
<!--break-->
<h2>Good Abstraction</h2>

A good abstraction is one that is self-consistent, programmatic and clearly-defined.  As such, <strong>a class's interface should offer a group of routines that clearly belong together.</strong>  For example, an <code>Employee</code> class should describe an employee's personal details and provide services to initialise and use an <code>Employee</code> object.  Listing 1 below provides an example of a class interface with good abstraction, while Listing 2 provides an example poor abstraction.  
<blockcode language=""csharp"">
public class Employee {
    public string Name { get; set; }
    public string Address { get; set; }
    public string HomePhoneNumber { get; set; }
    public string WorkPhoneNumber { get; set; }
    public TaxId TaxId { get; set; }
    public JobClassification JobClassification { get; set; }               

    public Employee() { }

    public Employee(string name, string address,
                    string homePhone, string workPhone,
                    TaxId taxId, JobClassification jobClass) {
        Name = name;
        Address = address;
        HomePhoneNumber = homePhone;
        WorkPhoneNumber = workPhone;
        TaxId = taxId;
        JobClassification = jobClass;
    }
}
</blockcode>
<span class=""inline inline-center caption""><strong>Listing 1:</strong> A Class Interface with Good Abstraction</span>

Internally of course the class may have additional routines, data, etc. to support the publicly-available methods and data, but the user of the class doesn't need to care or even know about these.  Compare this with the following class interface providing a poor abstraction; there is so much wrong with this, McConnell awards it a Coding Horror badge:

<blockcode language=""csharp"">
public class Program {
    public void InitialiseCommandStack();
    public void PushCommand(Command command);
    public Command PopCommand();
    public void InitialiseReportFormatting();
    public void FormatReport(Report report);              

    // ... etc.
}
</blockcode>
<span class=""inline inline-center caption""><strong>Listing 2:</strong> A Class Interface with Bad Abstraction</span>

To improve Listing 2, the routines should be refactored into separate classes, and the <code>Program</code> class should have a consistent abstraction with high cohesion, such as in Listing 3:

<blockcode language=""csharp"">
public class Program {
    public void InitialiseUserInterface();
    public void ShutdownUserInterface();
    public void InitialiseReports();
    public void ShutdownReports();               

    // ... etc.
}
</blockcode>
<span class=""inline inline-center caption""><strong>Listing 3:</strong> A Class Interface with Better Abstraction</span>

An extension to this idea is to use the <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html#SRP"" title=""SOLID Principles of OOD: Single Responsibility Principle"">Single Responsibility Principle (SRP)</a>, which ensures each class has internal cohesion in addition to good interface cohesion.  

<strong>The class interface should present a consistent level of abstraction.</strong>  For example, each class should implement one and only one <a href=""http://www.codebork.com/2009/12/24/code-complete-working-classes-part-1-class-foundations-abstract-data-types.html"" title=""Code Complete: Work Classes (Part 1 - Class Foundations: Abstract Data Types)"">Abstract Data Type</a>.  Mixing levels of abstraction in a class, such as providing methods to read a file and process the data read in, is poor design: it reduces cohesion, weakens the class's own abstraction, increases coupling, and results in a maintenance nightmare.  

When coding your class, <strong>be sure you understand what abstraction the class is implementing.</strong>  McConnell relays the anecdote of a project he once worked on that ended up wrapping a spreadsheet control rather than a simpler grid control, because the spreadsheet abstraction was closer to what the control needed to do.  The wrapper class exposed all <em>n</em>-hundred methods of spreadsheet, rather than simplifying to the needed grid plus <em>n</em> methods required to implement the same functionality ""super-grid"" functionality.  There was uproar at implementation time, but it proved to be the correct decision when it came to maintenance: the spreadsheet abstraction was a stronger one for the control, which meant the implementation was simpler to grasp than the equivalent grid implementation would have been.  

<strong>Ensure that your class's services are provided in pairs with their opposites.</strong>  As an example, if your class has an operation adding an item to a list, it will likely need one removing an item from the list as well.  Don't create these pairs willy-nilly, but do always check to see if you need the complementary operation when you create one.  

Don't be afraid to <strong>move unrelated information to another class.</strong>  (Again, this reflects on the SRP.)  Occasionally you will come across a situation where half the class's routines work with one half of the data, whilst the other half of the routines work with the other half of the data.  In situations like this, you should split the two halves into separate classes that have their own cohesive and consistent abstractions.  

<strong>Interfaces should be programmatic rather than semantic.</strong>  McConnell describes how an interface definition conceptually consists of two parts.  The <em>programmatic</em> part consists of the data types and other attributes that can be enforced by the compiler, whilst the <em>semantic</em> part is made from the assumptions of how the interface will be used (such as ""<code>MethodA()</code> must be called before <code>MethodB()</code>"").  This semantic part should be documented in the class/file comments, but <strong>it is important to keep interfaces only minimally dependent on documentation</strong>: often documentation goes unread, and comments tend to fall out of sync with the code they document.  A nifty trick is to use asserts or other similar techniques to make the semantic elements of the interface programmatic.

Speaking of changing code, you should <strong>beware of erosion of the interface's abstraction under modification.</strong>  Listing 4 provides an example of a class that has been modified without giving thought to the interface abstraction:

<blockcode language=""csharp"">
public class Employee {
    public FullName FullName { get; set; }
    public Address Address { get; set; }
    public PhoneNumber WorkPhoneNumber { get; set; }

    // ...

    public bool IsJobClassificationValid(JobClassification jobClass) {
        // ...
    }

    public bool IsPostCodeValid(Address address) {
        // ...
    }

    public bool IsPhoneNumberValid(PhoneNumber phoneNumber) {
        // ...
    }

    public SQLQuery CreateNewEmployeeQuery { get; }
    public SQLQuery ModifyEmployeeQuery { get; }
    public SQLQuery RetrieveEmployeeQuery { get; }
}
</blockcode>
<span class=""inline inline-center caption""><strong>Listing 4:</strong> Example of a Class Interface that's Eroding Under Maintenance</span>

In the real world, there is no logical connection between employees and routines that check valid post codes, etc. &mdash; unless of course you're employing people to manually validate post codes!  I think we can agree that's sufficiently unlikely.  Similarly, the database interaction methods do not belong here, because they are at a much lower level of abstraction than the employee class itself.  Utilising an <abbr title=""Object-Relational Mapping"">ORM</abbr> library like NHibernate<sup>*</sup> allows you to create an <code>Employee</code> from the database, abstracting away the mechanics of talking to the database.  

<strong>Don't add public members that are inconsistent with the interface abstraction!</strong>  Always ask yourself whether the member you are adding is consistent with the class's abstraction.  If it's not, find a better place to put it, creating a new class if necessary.  

It is important to <strong>consider abstraction and cohesion together.</strong>  These ideas are closely related: a class interface that presents a good abstraction usually has strong cohesion, although the inverse doesn't hold as strongly.  If you see a class that has weak cohesion and you can't work out how to correct it, ask yourself whether the class presents a good abstraction.

<h2>Good Encapsulation</h2>
As we saw in a previous post, <a href=""http://www.codebork.com/coding/2009/06/19/code-complete-design-construction-part-3-design-building-blocks-heuristics.html"" title=""Code Complete: Design in Construction (Part 3 - Design Building Blocks: Heuristics)"">encapsulation is a stronger concept than abstraction</a>.  Whilst abstraction helps manage complexity, encapsulation enforces the abstraction by preventing you from looking at the details.  McConnell takes no prisoners in pairing these together: <strong>either you have both abstraction and encapsulation or you have neither.  There is no middle ground.</strong>

So, how do we practice good encapsulation?  First off, we must <strong>minimise the accessibility of classes and their members.</strong>  Good object-oriented languages provide differing levels of accessibility, including <code>public</code>, <code>protected</code> and <code>private</code>.  .NET also provides two further access modifiers: <code>internal</code> (accessible by other members of the same assembly), and <code>protected internal</code> (accessible by other members of the same assembly, or derived classes in another assembly).  You should know the definitions of each of these accessibility levels inside out, so that you can fully grasp the implications of using each.  

One school of thought in utilising access modifiers is to lock down the class or member to the lowest workable level; however, this is not necessary if exposure is consistent with the abstraction.  It's worth keeping in mind that <em>hiding more is generally better than hiding less.</em>  Certainly you should <strong>never expose member data in public:</strong> this violates encapsulation in perhaps the most heinous way.  

McConnell also warns against putting private implementation details into a class's interface.  In modern languages like C# and Java, this isn't possible, and may in fact be specific only to languages where private implementation details can be exposed via the class's header file (e.g., C++ and Objective-C).  

When designing and implementing your classes, you should <strong>avoid making assumptions about the class's users.</strong>  Instead you should design and code to adhere to the contract specified by the class's interface.  You should also <strong>avoid friend classes</strong>.  These are classes that know about the internals of their friends.  Again, they violate encapsulation, and expand the amount of code you have to think about at any one time, increasing complexity.  They can very infrequently be used to manage complexity, such as in the <a href=""http://en.wikipedia.org/wiki/State_pattern"" title=""State pattern - Wikipedia, the free encyclopedia"">State pattern</a>.

<strong>Don't put a routine into the public interface just because it uses only public routines.</strong>  The fact that only public routines are used by a routine is irrelevant to whether it should be exposed in the interface.  If it's not consistent with the abstraction, don't expose it.

You should also <strong>favour read-time convenience to write-time convenience.</strong>  This was also touched on in <em>The Pragmatic Programmer</em>.  Source code is read many, many, many more times than it is written; favouring write-time convenience is a false economy.  

<strong>Be wary of semantic violations of encapsulation.</strong>  Semantic violations can be quite difficult to spot, so here's a run-down of some examples:
<ul>
  <li>Not calling <code>ClassA</code>'s <code>Initialise()</code> method because you know that <code>ClassA.PerformFirstOperation()</code> calls it automatically</li>
  <li>Not calling <code>database.Connect()</code> before calling <code>employee.Retrieve(database)</code> because you know <code>Retrieve()</code> will automatically connect to the database if there isn't already a connection</li>
  <li>Not calling <code>ClassA.Terminate()</code> because you know <code>ClassA.PerformFinalOperation()</code> calls it automatically</li>
  <li>Using a reference to <code>ObjectB</code> created by <code>ObjectA</code> even after <code>ObjectA</code> has gone out of scope because you know <code>ObjectA</code> keeps <code>ObjectB</code> in static storage and <code>ObjectB</code> will still be valid.  This to me sounds like a great route to creating memory leaks.</li>
  <li>Using <code>ClassB.MAXIMUM_ELEMENTS</code> constant instead of using <code>ClassA.MAXIMUM_ELEMENTS</code> constant because you know they're both set to the same value.</li>
</ul>

The problem with semantic violations of encapsulation is that they make the client code depend on the private implementations, not the public interface.  What happens if, in the last example above, the value of <code>ClassB.MAXIMUM_ELEMENTS</code> changes?  At best, you will notice an obvious bug, but more likely some subtle behaviour will have been introduced into the application that will be hard to reproduce.  

Always <strong>watch for coupling that is too tight.</strong>  Ensure you minimise the accessibility of your classes and their members, and avoid friend classes because they're tightly coupled (by definition).  Make data private rather than protected in a base class so that derived classes are less tightly coupled to the base class.

Finally, be sure to <strong>observe the <a href=""http://en.wikipedia.org/wiki/Law_Of_Demeter"" title=""Law of Demeter - Wikipedia, the free encyclopedia"">Law of Demeter</a></strong>, also referred to as the Principle of Least Knowledge.  This is succinctly defined as 

<blockquote cite=""http://en.wikipedia.org/wiki/Law_Of_Demeter"">
Each unit should have only limited knowledge about other units: only units ""closely"" related to the current unit.
</blockquote>

Any given class should make as few assumptions as possible in its communications with other entities.  Formally, for any method <em>M</em> on object <em>O</em>, <em>M</em> may reference:
<ul>
  <li><em>O</em></li>
  <li><em>M</em>'s parameters</li>
  <li>anything created within <em>M</em></li>
  <li>any direct component of <em>O</em></li>
  <li>global variables accessible by <em>O</em> in the scope of <em>M</em></li>
</ul>

The result of these restrictions is that your code should only use one dot.  For example <code>a.Method()</code> obeys the rule, but <code>a.nother.Method()</code> violates it.  

<hr />
<sup>*</sup> Other ORM libraries are available."
Code Complete: Working Classes (Part 3 - Design and Implementation Issues),NULL,"There are a number of design and implementation issues to consider when working with classes, not least dealing with inheritance.  This third post in the series of four on ""Working Classes"" covers my take on Steve McConnell's thoughts on containment, inheritance, member functions and data, and constructors.
<!--break-->
<h3>Containment</h3>

McConnell defines containment in terms of ""has a"" relationships; for example, a car ""has a"" steering wheel, an employee ""has a"" name and a phone number.  These can be primitive data elements or more complex object types.  These kinds of ""has a"" relationships should only be implemented through private inheritance as a last resort, i.e., if you can't achieve it via containment, or if containment creates tight coupling with the base classes.  A need to implement ""has a"" relationships through private inheritance tends to point to design problems, so if you find yourself in this situation, it's worth revisiting your high-level design before going ahead.

A further piece of advice for containment is to be critical of classes that contain more than seven or so members (based on the idea that <a href=""http://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two"">working memory is limited to seven plus or minus two items of information</a>).  If the members are mainly object types, then aim for the lower end of this range, as they are complex types, containing their own members; however, if the members are mostly, or entirely, primitive types, then the rule may be relaxed to the upper end of the range if needed.  
 
<h3>Inheritance</h3>

If containment is described in terms of A ""has a"" B, then inheritance is described in terms of A ""is a"" B; i.e., one class is a specialisation of another class.  Scott Meyers, author of <em>Effective C++</em> amongst other titles, regards this as <strong>the single most important rule in object-oriented programming</strong>.  

""Is a"" relationships should be implemented via public inheritance such that the new class ""is a"" more specialised version of the base class.  The derived class must adhere completely to the interface contract defined by the class; if it cannot, inheritance is the wrong technique.  Inheritance adds complexity, however, so it is a dangerous technique; as such it must be designed for and documented, or otherwise prohibited.  If a class is not designed to be inherited from, be sure to mark it as such using <code language=""java"">final</code> or <code language=""csharp"">sealed</code>, or whatever mechanism your language makes available for this.  

When designing for inheritance, you should ensure you adhere to the <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html#LSP"" title=""SOLID Principles of OOD"">Liskov Substitution Principle</a>.  In essence, this means that you shouldn't inherit from a base class unless the derived class really and truly ""is a"" more specific version of the base class.  The Pragmatic Programmers neatly summarise this as ""The subclass must be usable through the base class interface without the need for the user to know the difference""; i.e., all routines defined in the base class should mean the same thing in the derived classes.  See the code listing below for an example:

<blockcode language=""csharp"">
public class Account {
    public void RecordTransaction(Transaction transaction) {
        // ...
    }
}

public class CurrentAccount : Account {
    // ...
}

public class SavingsAccount : Account {
    // ...
}

public class Program {
    public static void main(String[] args) {
        // ...

        Account currentAccount = new CurrentAccount();
        Account savingsAccount = new SavingsAccount();

        currentAccount.RecordTransaction(transaction);
    }
}
</blockcode>

Given the base class of <code>Account</code> and the derived classes of <code>CurrentAccount</code> and <code>SavingsAccount</code>, a programmer should be able to call Account.RecordTransaction() on either derived class without caring what subtype it is.  If this principle holds, much of the complexity introduced by inheritance is done away with: suddenly you need only deal with the generic attributes and not the implementation details.  Compare an <code>Account.InterestRate</code> getter implementation in a <code>SavingsAccount</code> with that of a <code>LoanAccount</code>: if you were worrying about the implementation details, you would likely need to remember to flip the sign.

When inheriting, be sure that you inherit only that which you want to inherit, i.e., look after your children!  You may want your kids to inherit the three-storey Kensington townhouse, but you probably don't want to saddle them with the mortgage too.  As mentioned above, class contracts apply to the inheritance relationship too, so don't pass on too much detail and baggage to the derived classes.  

The following table details variations on inherited routines.

<table style=""text-align: center"">
  <thead style=""font-weight: bold; border-top: 2pt solid silver; border-bottom: 2pt solid silver"">
    <td style=""text-align: left; border-right: 1pt solid silver; padding: 2px;""></td>
    <td style=""border-right: 1pt solid silver; padding: 2px;"">Overridable</td>
    <td style=""padding: 2px;"">Not Overridable</td>
  </thead>
  <tbody>
    <tr style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver; padding: 2px;""><strong>Implementation: Default Provided</strong></td>
      <td style=""border-right: 1pt solid silver; padding: 2px;"">Overridable routine</td>
      <td style=""padding: 2px;"">Non-overridable routine</td>
    </tr>
    <tr  style=""border-bottom: 1pt solid silver"">
      <td style=""text-align: left; border-right: 1pt solid silver; padding: 2px;""><strong>Implementation: No Default Provided</strong></td>
      <td style=""border-right: 1pt solid silver; padding: 2px;"">Abstract overridable routine</td>
      <td style=""padding: 2px;"">N/A - doesn't make sense</td>
    </tr>
  </tbody>
  <caption style=""caption-side: bottom; margin-top: 3px; font-weight: bold"">Variations on Inherited Routines</caption>
</table>

By way of explanation, ""abstract overridable"" indicates that a derived method should inherit the interface but not the implementation, such as with Java's <code language=""java"">AbstractMap.entrySet()</code> method.  This is indicated in Java and C# with the <code language=""csharp"">abstract</code> keyword.  ""Overridable"" indicates that the derived method should inherit the interface and the implementation, whilst still allowing the method to be overridden.  Examples include <code language=""java"">Object.getHashCode()</code> and <code language=""java"">Object.equals()</code>; this is the default behaviour for Java, but in C# overridable methods must be decorated with the <code language=""csharp"">virtual</code> keyword.  ""Non-overridable"" methods are those for which derived methods inherit the interface and the implementation, but where overriding of the implementation is disallowed.  There are no obvious examples in J2SE or .NET, but this is indicated by decorating your method with the <code language=""java"">final</code> (Java) or <code language=""csharp"">sealed</code> (C#) keywords.  Beware of inheriting a method's implementation just because you're inheriting the interface: there are strategies to avoid it.  If you want to use a method's implementation but not its interface, consider using containment instead to wrap the implementation.

To reduce complexity, you should avoid overriding a non-overridable member function.  This can cause confusion: methods look polymorphic when in fact they're not.  For example, if <code>A.print(String message)</code> is private, don't create <code>B.print(String message)</code> or <code>B.print(int i)</code> when <code>B</code> extends <code>A</code>.  Also, be sure to move common interfaces, data and behaviour to the highest possible level in the inheritance tree.  The abstraction should guide you here; if moving something up another level breaks the higher object's abstraction, leave it where it is.

McConnell goes on to describe some things to look out for and avoid when using inheritance.  First of these is to be suspicious of classes with only one instance.  This often indicates the design confuses objects with classes; i.e., a new derived class is created instead of instantiating an existing object.  The Singleton pattern is a notable exception to this rule.

You should also be suspicious of base classes with only one derived instance.  This can indicate pre-emptive design, i.e., trying to anticipate future needs, often without fully understanding what those future needs are.  This can be avoided by adhering strongly to <abbr title=""You Ain't Gonna Need It"">YAGNI</abbr>, and not creating any more inheritance structure than is necessary.

Furthermore, be suspicious of classes that override a routine and do nothing inside the derived routine.  This is usually an error in the design of the base class: e.g., class <code>Cat</code> has method <code>Scratch()</code>, but some cats are de-clawed so can't scratch.  One possible solution to this scenario is to create a new subclass <code>ScratchlessCat</code> that overrides <code>Scratch()</code> to do nothing.  This presents you with a number of problems, however.  First, it violates the abstraction presented by <code>Cat</code> by changing the semantics of its interface; second, what happens when you find a cat without a tail, or a de-clawed cat without a tail, or a cat that doesn't drink milk?  You'll end up with a horribly complex inheritance hierarchy.  Finally, the code becomes confusing to maintain because the interfaces and behaviour of the ancestor classes tell you nothing about the behaviour of their descendants.  The way to fix this scenario is to move the claws abstraction out of the <code>Cat</code> class and into its own class, and then use containment.

Try to avoid deep inheritance trees; inheritance brings much complexity with it, and this quickly mounts up when you introduce multiple levels of inheritance.  Perhaps unsurprisingly, deep inheritance trees are frequently associated with increased fault rates.  McConnell quotes Arthur Riel's 1996 book <em>Object-Oriented Design Heuristics</em> as suggesting six is a good maximum, but believes himself that this suggestion is ""grossly optimistic"", proposing instead that this is better applied to the total number of subclasses than the number of levels in the inheritance tree.  

Prefer polymorphism to extensive type-checking.  Polymorphism can effectively replace ""is"" or ""instanceof"" checks when possible; indeed an entire online campaign, the <a href=""http://www.antiifcampaign.com/"">Anti-If Campaign</a> has been spawned to address this very issue.  

Make all data private, not protected.  As Joshua Bloch describes in his 2001 book <em>Effective Java</em>, inheritance breaks encapsulation: when you inherit from an object, you obtain privileged access to that object's protected routines and data.  To preserve encapsulation, provide protected accessors instead of protected fields.

Finally, multiple inheritance should be <strong>very</strong> sparingly used.  Indeed, use it only after carefully considering the alternatives and weighing the impact on system complexity and comprehensibility.

In summary:
<ul>
  <li>If multiple classes share common data but not behaviour, create a common object those classes can contain.</li>
  <li>If multiple classes share common behaviour but not data, derive them from a common base class that defines the common routines.</li>
  <li>If multiple classes share common data and behaviour, inherit from a common base class that defines the common data and routines.</li>
  <li>Inherit when you want the base class to control your interface; contain when you want to control your interface.</li>
</ul>

<h3>Member Functions and Data</h3>
McConnell goes on to offer some advice on member functions and data.  First up is to keep the number of routines in a class as small as possible.  Research suggests that higher numbers of routines per class are associated with higher fault rates. Other factors can be more significant, though, such as deep inheritance trees, a large number of routines called within a class, and tight coupling between classes.  For best results, balance the number of routines against these other factors for best results.  

Try to disallow implicitly generated member functions and operators you don't want, such as private constructors, read-only properties, etc.  Also try to minimise the number of different routines called by a class; collectively known as ""fan out"" along with the number of classes used by a class, this positively correlates with fault rates.  Furthermore, try to minimise indirect routine calls to other classes, for example <code>account.ContactPerson.DayTimeContactInfo.PhoneNumber</code>.  The <a href=""http://en.wikipedia.org/wiki/Law_of_Demeter"">Law of Demeter</a>, also known as the principle of least knowledge or ""the law of one dot"", covers this quite neatly.  Simply put, there's no point in directing the wheels to turn when instead you want to drive forward.  

Finally, try to minimise collaboration with other classes.  This is related to the idea of coupling, but McConnell here enumerates some specific kinds of class interaction that should be minimised as much as possible: the number of kinds of objects instantiated, the number of different direct routine calls on instantiated objects, and the number of routine calls on objects returned by other instantiated objects.  

<h3>Constructing and Copying objects</h3>
As much as possible, all member data should be initialised in all the objects constructors.  This is an inexpensive defensive programming practice that can protect you from initialisation errors later on.  It is not clear, however, whether or not this means that initialising fields at declaration is wrong, as is supported by Java and C# for example.  The singleton pattern should be enforced through the use of a private constructor, so that member data is still initialised in the constructor, but only the class itself may create a new instance of the object.  

Finally, McConnell includes a note on deep copying vs. shallow copying of classes.  Deep copies (where a new object is instantiated and the member data of the source is copied into it) are simpler to code and maintain; shallow copies (where a new instance is created to reference the original instance) require code for reference counting and ensuring safe copies, comparisons and deletes.  <a href=""http://en.wikipedia.org/wiki/Object_copy"" title=""Object Copy"">Wikipedia has a good explanation of the differences.</a>  McConnell suggests that deep copies should be preferred to shallow copies until it's proven that shallow copies are needed (for performance gains).  It's worth noting, however, that modern languages with good garbage collection handle the reference counting issue capably for you, and in fact make shallow copying an easier and more natural operation than deep copies (which require implementations such as C#'s <code>MemberwiseClone()</code> method)."
Code Complete: Working Classes (Part 4 - Reasons to Create a Class),NULL,"This fourth and final post in the mini-series on Working Classes covers the reasons to create a class.  These reasons are many and varied, but fall quite neatly into a number of groups.  These groups are: modelling concepts, managing complexity, hiding information, building for the future, and classes to avoid.  This post should be rather shorter than the recent ones, and hopefully more digestible as a result.  

I'm also experimenting with a new method of writing up my notes that I hope will make this post more coherent; I feel that some of my recent posts in the <em>Code Complete</em> series have been almost a sequence of short paragraphs and possibly haven't hung together too well.  I'm interested to know whether you think this post is an improvement, dear reader.
<!--break-->
<h3>Modelling Concepts</h3>
It is best practice to create a new class for each real-world or abstract object that you need to model in your application.  For example, if you model different forms of vehicle, create a separate class for each vehicle type, such as motorcycle, car, ambulance.  You can use an abstract vehicle class at the top of an inheritance hierarchy to model common properties such as registration plate.  Similarly, if you are modelling abstract objects, such as a shape, introduce a new class (and possible inheritance hierarchy) for this too, even though it doesn't exist in the real world.  

The <a href=""http://www.codebork.com/2009/12/24/code-complete-working-classes-part-1-class-foundations-abstract-data-types.html"" title=""Code Complete: Working Classes (Part 1 - Class Foundations: Abstract Data Types)"">earlier post on Abstract Data Types</a> provides more information on good ways to go about defining classes of this type.

<h3>Managing Complexity</h3>
New classes can help you manage your application's complexity in two ways: by reducing complexity, or by isolating it.  <strong>The single most important reason to create a class is to reduce complexity.</strong>  Classes allow you hide information so you won't need to think about it, which is a major win for reducing complexity in your application.  

Isolating complexity into a new class brings some benefits, too.  For example, errors are easier to spot if they are localised to one particular area, and it is easier to try several designs and keep the best one.  This allows you to experiment with different algorithms, or different implementations ""under the hood"", whilst keeping the interface the same.  

You should not be afraid to create a new class (but keep in mind the points about eliminating irrelevant classes below).  

[img_assist|nid=136|title=|desc=|link=none|align=center|width=400|height=297]

<h3>Hiding Information</h3>
As described in <a href=""http://www.codebork.com/coding/2009/06/19/code-complete-design-construction-part-3-design-building-blocks-heuristics.html"" title=""Code Complete: Design in Construction (Part 3 - Design Building Blocks: Heuristics)"">an earlier post on design</a>, information hiding is a powerful technique for managing complexity.  Create new classes to hide implementation details, limit the effects of changes you might make to an area of your system, and hide global data.  

Implementation details are best encapsulated in the routine or class hosting them; leaky abstractions bring implementation details to the fore, which then need to be managed carefully.  Create a new class to wall off certain implementation details.  

New classes can also be created to isolate the areas of your code likely to change.  Designing your application so that the areas most likely to change are the easiest to change will give you greater flexibility in future when the change comes along, and the effects of those changes can be mitigated through the design at the outset.  Maybe you expect a requirement to support multiple back-end databases (as is common with a number of web-based applications, for example); you would need to source or build yourself a database abstraction layer to ensure that the database system used was fully isolated from the rest of your application in order to prevent a large amount of re-work down the line.  

Some applications require global data or state to be maintained through the lifetime of an application (or HTTP request in a web application).  If this applies to your application, hide it behind a class interface; this affords you a number of benefits, such as allowing you to change the structure of your data without changing the program, and monitoring access to the data. Furthermore, and perhaps most importantly, <em>it makes you think about whether the data is truly global.</em>

<h3>Building for the Future</h3>
I'll admit this is a bit of a catch-all category for the remainder of McConnell's suggested reasons for creating a class.  They do seem to hang together under this heading, thought.  

You may want to introduce a new class to streamline parameter passing.  If you're passing lots of data around, then a different class organisation might work better.  This is one advantage of the <a href=""http://en.wikipedia.org/wiki/MVVM"" title=""MVVM on Wikipedia"">Model-View-ViewModel (MVVM)</a> pattern: into your view model class goes a bunch of related data and behaviour to be presented in your view, that perhaps doesn't form a coherent model of a concept (abstract or concrete).  For example, a ViewModel for the Facebook homepage might include your name, your news feed, your upcoming events, your suggestions, etc.; none of these would be in the ""User"" model, which would include your name, email address, gender, etc.  The ViewModel makes it much easier to pass around this collection of related data rather than passing each separate item as an individual parameter.  

If you need a central point of control for something, a pivot around which two components of the system revolve and interact, create a new class embodying that relationship.  For example, a class in this category might maintain knowledge of the number of entries in a table, or control of devices such as files, printers or database connections.  An example of this is the Repository pattern I have <a href=""http://www.codebork.com/coding/2009/01/24/mocking-databases.html"" title=""Web Frameworks Evaluation II.iii: Mocking Database Connections in Unit Tests"">previously covered</a>.  

[img_assist|nid=137|title=Actors Michael J. Fox and Christopher Lloyd in the movie Back to the Future.|desc=|link=none|align=center|width=399|height=268]

Classes are, of course, great for facilitating reusable code, which is something for which any good developer should be striving.  Code put into well-factored classes can be more easily re-used, so don't be afraid to refactor your code into new classes if appropriate (many refactoring patterns result in the creation of new classes).  McConnell also suggests moving code to its own class if it might be used in another program or module; approach this with some caution, however, particularly if you're in an environment adhering to the principle of <abbr title=""You Ain't Gonna Need It"">YAGNI</abbr>, as this might be considered a violation.  

New classes will help you plan for a family of programs.  This links back to the ideas listed in Information Hiding above: programs will change, so isolate the parts you expect to change by putting them into their own classes; your classes can then be modified without affecting the rest of the program.  Classes can also help you package related operations together, although you have the option of using packages/namespaces for a similar purpose if your language supports them.  

<h3>Classes to Avoid</h3>
So, if those are all the good reasons for creating a class, what sort of classes should you avoid creating?  What are <em>bad</em> reasons for creating a class?  

First up is the omniscient, all-knowing and all-powerful god class.  A good indicator of a god class is the use of a large number of accessors from another class, manipulating its data.  This is an indication that your class interfaces are not as strong as they might be.  

Irrelevant classes should be eliminated.  These are classes that are missing one of the two components of classes: behaviour or data.  If a class contains data, but no behaviour, consider moving the fields into other classes.  If the class does logically hang together as an entity, such as the classic property bag objects you occasionally find in an application of  the <abbr title=""Model-View-Controller"">MVC</abbr> pattern, you might want to consider converting them into structs if your language supports it and it is appropriate to your scenario.  Structs have different semantics in C# from classes: they are treated as value types rather than reference types.  

The other form of irrelevant class, those that contain behaviour but no data, are often named after verbs.  This is an anti-pattern: a class should model a concept, abstract or concrete, which by definition cannot be a verb and must be a noun.  The behaviour in classes such as these should be turned into a routine on another class.  

<h3>Conclusion</h3>
Well, that rounds off this post, and the mini series on Working Classes.  The next post will bring us on to Chapter 7 of <em>Code Complete</em>, which covers high quality routines.  I welcome any feedback you might have on this post, the mini-series, or on the <em>Code Complete</em> series as a whole; please leave me a comment or drop me an email."
Visual Studio Build Progress in the Windows 7 Task Bar: Creating a Small VS 2010 AddIn,NULL,"<a href=""http://www.codebork.com/science-and-technology/2009/02/24/windows-7.html"" title=""Windows 7 write-up on CodeBork"">Windows 7</a> introduces a whole bunch of <a href=""http://www.codebork.com/science-and-technology/2009/03/21/windows-7-aero-snap.html"" title=""Windows 7 Aero Snap"">cool features</a>, as I've mentioned here previously.  Sometimes the simplest ones are the most effective, however, and there's nothing simpler or more effective than the in-taskbar progress indicator.  If you haven't seen this already, when you copy a file in Windows Explorer, download a file using IE, or perform any of a number of similar actions that might take a while to complete, the progress information is displayed in the task bar icon for that application.

[img_assist|nid=116|title=Windows 7 Taskbar Progress|desc=This screenshot also illustrates the ""Icon and Text"" display mode|link=none|align=center|width=400|height=234]
<!--break-->
I've also been playing around with the beta 2 release of Visual Studio 2010 recently.  It is very slick and amazingly solid for a beta release. I'm starting to expect all pre-release software from Microsoft to be this solid!  Most importantly it is <em>fast</em>, and a huge improvement over the dog-slow and borderline unusable first beta.  

I was so surprised that VS 2010 doesn't display build progress in the Windows 7 task bar that I decided to investigate.  First of all, I tweeted <a href=""http://www.hanselman.com/"" title=""Scott Hanselman's blog"">Scott Hanselman</a> (Softie and blogger extraordinaire) and <a href=""http://weblogs.asp.net/scottgu"" title=""Scott Guthrie's blog"">Scott Guthrie</a>, VP of a bunch of MS products including ASP.NET MVC, Silverlight, WinForms, WPF, Visual Studio Tools for WPF... You get the idea :-)

[img_assist|nid=113|title=|desc=|link=none|align=center|width=505|height=166]

And Scott H was good enough to get back to me almost immediately, pointing me to one of the guys on his team at MS:

[img_assist|nid=114|title=|desc=|link=none|align=center|width=504|height=92]

Well, after a couple of days I got bored of waiting (Pete didn't already have an example), so I decided to look at it myself.  It turned out to be surprisingly easy, although it took me quite a long time to discover the correct APIs and iron out the bugs.  The full project is available on <a href=""http://github.com/alastairs/buildprogress/"" title=""alastairs's buildprogress at master"">GitHub</a> (although all the code of interest is covered in this post), and you'll need VS2010 to build and run the add-in, along with the <a href=""http://code.msdn.microsoft.com/WindowsAPICodePack"" title=""Windows API Code Pack"">Windows API Code Pack</a>, which provides a wrapper to the COM interfaces to many of the Windows 7 features.  As far as I know, this is intended only to be a stop-gap between the current 3.5.1 version of .NET and .NET 4.0 due to RTM sometime next year.  Beta 2 of .NET 4.0 already includes APIs for working with the Windows 7 task bar, but I didn't use them as I struggled to find documentation on them originally, and they're all WPF-based.  My WPF skills are, well, non-existent, so my attempts this evening to port the plugin to these ""native"" APIs sadly failed.

When you create a new Visual Studio AddIn project, you get a <code>Connect</code> class auto-generated with a bunch of default event listeners and doc comments illustrating their use.  You also get a public constructor with no arguments that Visual Studio will call when creating the AddIn.  I didn't need to do anything here, so it's left blank (and could, in fact, probably be deleted so the .NET default constructor is called).

<blockcode language=""csharp"">
namespace BuildProgress
{
    public class Connect : IDTExtensibility2
    {
        // Place your initialization code within this method.
        public Connect() { }
</blockcode>

The first event listener in the file is the <code>OnConnection</code> listener.  This event listener runs when Visual Studio <em>loads</em> the AddIn (contrast with <em>creating</em> the AddIn above).  The listener takes a reference to the Visual Studio instance running the AddIn, the connection mode, a reference to the AddIn itself, and finally an array of custom arguments (passed by reference).  Mostly I ignored these; the generated code contained the first two assignment statements below, caching the references to VS and my AddIn, and of course the reference to VS comes in very handy.  My next step is to subscribe to a number of build events by adding an event handler to the event definitions, such as <code>OnBuildBegin</code>.  For non-.NET readers, the reason += is used here is because event subscription is additive - my <code>OnBuildBegin</code> event handler is added to a list of subscribers to the <code>OnBuildBegin</code> event.

<blockcode language=""csharp"">
        // Receives notification that the Add-in is being loaded.
        public void OnConnection(object application, ext_ConnectMode connectMode, 
                                 object addInInst, ref Array custom)
        {
            _applicationObject = (DTE2)application;
            _addInInstance = (AddIn)addInInst;

            _applicationObject.Events.BuildEvents.OnBuildBegin += OnBuildBegin;
            _applicationObject.Events.BuildEvents.OnBuildProjConfigDone += 
                        OnBuildProjConfigDone;
            _applicationObject.Events.BuildEvents.OnBuildDone += OnBuildDone;
        }
</blockcode>

Next we declare a bunch of private fields to help out with displaying the progress and a flag to indicate whether or not one or more of the projects failed to build successfully (more on that flag in a bit...!).  

The <code>OnBuildBegin</code> event takes two parameters, build scope and build action<sup>1</sup>.  For the purposes of this AddIn, I don't care about the values of these parameters, so I ignore them.  First of all, I clean up for the new build by removing any existing icon overlay, turning off the progress indicator and resetting the failed build flag to false.  

I then do some simple calculations to work out how many progress points the indicator should move per project.  I found the ""number of projects"" property easily discoverable via Intellisense.  The Windows API Code Pack <code>TaskbarManager</code> class (a <a href=""http://en.wikipedia.org/wiki/Singleton_pattern"" title=""Singleton Pattern on Wikipedia"">Singleton</a>) requires a maximum value for the progress bar, so I set this to (number of projects) x (number of percentage points per project) >= 100.  Finally, I call through to update the progress bar indicator.

<blockcode language=""csharp"">
        // Generated code
        private DTE2 _applicationObject;
        private AddIn _addInInstance;

        // Used to display progress
        private int _projectProgressPercentagePoints;
        private int _nextProgressValue;
        private int _maxProgressValue;

        // Did one of the projects fail to build?
        private bool _buildErrorDetected;

        private void OnBuildBegin(vsBuildScope scope, vsBuildAction action)
        {
            // Reset any previous build state to ensure we start from scratch
            TaskbarManager.Instance.SetOverlayIcon(null, string.Empty);
            TaskbarManager.Instance.SetProgressState(
                        TaskbarProgressBarState.NoProgress);
            _buildErrorDetected = false;
            
            // Now make some calculations as to what ""progress"" is.  The progress 
            // is approximately a percentage: 
            // (_numberOfProjects * _projectProgressPercentagePoints may be 
            // greater than 100, e.g., if _numberOfProjects is 3 or some other 
            // non-factor of 100).  
            var _numberOfProjects = _applicationObject.Solution.Projects.Count;

            _projectProgressPercentagePoints = 
                        (int)Math.Ceiling((decimal)100 / _numberOfProjects);
            
            // Set the initial progress values and kick-start the progress 
            // updating
            _nextProgressValue = 0;
            _maxProgressValue = 
                        _projectProgressPercentagePoints * _numberOfProjects;

            UpdateProgressValue(false);
        }
</blockcode>

This next block is the first of the two most interesting blocks (save the best 'til last and all that jazz).  Each time a project finishes building, the <code>OnBuildProjConfigDone</code> event is fired, with a bunch of arguments.  I simply call through to update the progress bar indicator, and pass into that method whether or not the build failed (i.e., was not a success).  <strong>Slight digression:</strong> I don't know why MS have used these long parameter lists for passing around the event arguments.  We are advised when coding for .NET to create a class that derives from the <code language=""csharp"">EventArgs</code> base class, and wrap all the necessary arguments in our derived class.  For example, here we would have <code language=""csharp"">BuildProjConfigDoneEventArgs</code> defining a <code language=""csharp"">Project</code> property, a <code language=""csharp"">Success</code> property, etc.

Anyway, digression over.  <code>UpdateProgressValue()</code> checks the next value falls within the displayable range for the progress bar, and updates the progress value.  

Here's one of the really cool bits of the AddIn: like the new-style progress bars introduced with Vista, the taskbar progress indicator has a number of different states that affect how it displays.  Normally, it displays as a green bar; when an error occurs it can be made to turn red; while yellow indicates that the work item has paused.  If a build error occurred, I set the build failure flag mentioned earlier to true, and set the progress indicator state to Error.  For the remainder of the build, the progress bar will track in red rather than green, providing instant visual feedback that there is a problem with the build!

Finally, we increment the <code>_nextProgressValue</code> by the appropriate number of percentage points so that it is ready for the next project build to finish.  

<blockcode language=""csharp"">
        private void OnBuildProjConfigDone(string Project, string ProjectConfig, 
                            string Platform, string SolutionConfig, bool Success)
        {
            UpdateProgressValue(!Success);
        }

        private void UpdateProgressValue(bool errorThrown)
        {
            if (_nextProgressValue < 0)
                _nextProgressValue = 0;

            if (_nextProgressValue > _maxProgressValue)
                _nextProgressValue = _maxProgressValue;

            TaskbarManager.Instance.SetProgressValue(
                        _nextProgressValue, _maxProgressValue);
                        
            if (errorThrown)
            {
                _buildErrorDetected = true;
                TaskbarManager.Instance.SetProgressState(
                            TaskbarProgressBarState.Error);
            }

            _nextProgressValue += _projectProgressPercentagePoints;
        }
</blockcode>

Ok, last method now.  The <code>OnBuildDone</code> event listener is fired when the entire build completes; importantly, it fires <em>after</em> the <code>OnBuildProjConfigDone</code> event.  This takes the same arguments as <code>OnBuildBegin</code>, and again we ignore them.  

This is the second cool feature of the AddIn.  The Windows 7 task bar supports icon overlays, i.e., adding small icons to the task bar icon to indicate some kind of state change in the application.  If you've used TortoiseGit or TortoiseSVN then you'll recognise this concept; a tech preview of Outlook 2010 I have used utilises this mechanism to indicate when you have received new mail.  Here I display a simple tick or cross to indicate whether or not the build completed successfully.  The overlay persists until you run a new build, so you have an immediate and persistent visual indication of whether or not your last build succeeded.  These icons are stored in a resources (.resx) file in the solution, and are retrieved via the Resources API.  

Finally I do a short <code language=""csharp"">Thread.Sleep()</code> to ensure the progress bar briefly displays at 100% before being reset.  The class name has to be fully-qualified (with its namespace) because another assembly used for AddIns defines a interface called <code>Thread</code>.  

<blockcode language=""csharp"">
        private void OnBuildDone(vsBuildScope scope, vsBuildAction action)
        {
            if (_buildErrorDetected)
            {
                TaskbarManager.Instance.SetOverlayIcon(
                        (Icon)Resources.ResourceManager.GetObject(""cross""), 
                        ""Build Failed"");
            }
            else
            {
                TaskbarManager.Instance.SetOverlayIcon(
                        (Icon)Resources.ResourceManager.GetObject(""tick""), 
                        ""Build Succeeded"");
            }

            // Add in a small delay so that the progress bar visibly reaches 100%
            System.Threading.Thread.Sleep(100);
            TaskbarManager.Instance.SetProgressState(
                        TaskbarProgressBarState.NoProgress);
        }
    }
}
</blockcode>

That's all there is to it!  Now for some commentary on the Visual Studio AddIn framework and the Windows API Code Pack.  

The Visual Studio AddIn framework is easily explorable via Intellisense, and this will often quickly lead you to the correct property or method for your situation (for simple AddIns, at least).  However, it is <em>massive</em>, and there's a wealth of documentation available that would need to be read for any serious AddIn development.

I found the error messages thrown whilst debugging the AddIn unhelpful to say the least, with ""Unknown Exception""s and cryptic error codes appearing all over the place.  Googling the error codes didn't help much, either, and I found that this is not even a beta-quality issue: both VS 2005 and 2008 RTM'd with these same useless error dialogs.  

Given the amount of effort the Visual Studio team has put into ""dogfooding"" the newer MS technologies such as WPF and <a href=""http://www.codeplex.com/MEF"" title=""Managed Extensibility Framework"">MEF</a>, I was suprised to find that creating a WPF-enabled AddIn is not trivial.  Packaging AddIns manually is nigh-on impossible: there is some information available about the packaging format, but not so much on the format of files included in the package, so I was only able to get halfway towards a properly-packaged AddIn.  

I created this AddIn without the benefit of the full VS2010 Beta 2 SDK (I didn't have it installed when I embarked on the project), so it may be that some of the issues I mentioned above are resolved if you bother to use that.  Certainly the packaging problems will go away, from what I've heard.  

The Windows API Code Pack is a useful interim step whilst we wait for .NET 4.0 to reach RTM.  However, I came across a number of issues with apparent non-determinism in the taskbar API that cause some funny behaviour, such as the icon overlay not always being removed when I run my clean-up code, and the icon overlay appearing before the progress indicator has reached its maximum value.  I might be unfairly blaming this on the taskbar API, of course, as it could also be a result of the event listeners running out-of-order.  Of course such things are incredibly difficult to debug accurately or successfully.  

Another thing that stumped me for quite a while was that I couldn't cache <code language=""csharp"">TaskbarManager.Instance</code> in my code at all; it had to be called explicitly.  This may be a result of the particular implementation of the Singleton pattern that the developers used for the <code>TaskbarManager</code> class, but it took me a while to work it out.  I thought about it some today, and discussed it with my friend and colleague <a href=""http://www.analysisuk.com/"" title=""Steve's Personal Blog"">Steve</a> (who's much more knowledgeable on .NET than I), and we couldn't come up with a sensible answer.  Maybe it's one for StackOverflow and <strike>Jon Skeet</strike> Tony the Pony&hellip;

Finally, and most annoyingly, Unit Testing was a no-go, both because of this point and because the <code>TaskbarManager</code> class is closed for <abbr title=""Dependency Injection"">DI</abbr>.  There is nothing I can feed my AddIn to stand in for the TaskbarManager class during a Unit Test because it does not implement any interfaces, so no mocks can be created.  This goes for the WPF API also.  As such, debugging was slow and painful.  If you know of a way to create unit tests in a situation such as this, I would be <em>very</em> interested to hear of it.  

If you happen to be using VS2010, please <a href=""http://github.com/alastairs/buildprogress/"" title=""My AddIn on GitHub"">download the AddIn from GitHub</a> and give it a go.  Feature requests and bug reports are all welcome, and can be logged on GitHub.  

<hr />
<sup>1</sup> Scope indicates how much of the solution is being built: a project, a batch or the whole solution; action indicates what build action is being performed: build, clean, deploy or rebuild all."
Stack Overflow Dev Days - London,NULL,"On Wednesday 28 October 2009, Joel Spolsky and Jeff Atwood brought their Stack Overflow sideshow to London's Kensington Town Hall, and I was lucky enough to be one of the ~1000 people attending.
<!--break-->
<h3>Registration and Opening Keynote</h3>

The day opened early, with registration starting soon after 8am, and we were soon gulping copious amounts of coffee, breakfast, and collecting bags of swag (free FogBugz book, Super User and Server Fault stickers, etc., etc., etc.) from the various exhibitors before Joel's opening keynote.  After an excellent and incredibly geeky send-up of <a href=""http://en.wikipedia.org/wiki/Scrubs_%28TV_series%29"" title=""Scrubs on Wikipedia"">Scrubs</a> set in the Fog Creek offices, Joel spoke on simplicity vs power.  Ribbing <a href=""http://37signals.com"" title=""37Signals' website"">37Signals</a>, one of the darlings of Web 2.0, more than a little for their ""oversimplified"" user interfaces (""Hey, anyone can create an HTML page and slap a &lt;textarea /&gt; on it!""), he demonstrated how many products start out as simplified versions of another product, and soon expand to fit user's requirements.  Of particular note was that old customer line, ""if you include feature x, we'll sign now"" (ok, so I paraphrased a bit, but you get the picture :-) that seems to drive product complexity like no other factor on Earth.  As features are added, so the UI becomes more cluttered, it's harder to find feature y or complete task a, and smart developers start thinking, ""I could knock up a simplified version of this overnight and make my fortune!"".  All this has happened before and all this will happen again.  

The crux of Joel's keynote was that there's nothing wrong with losing a bit of simplicity to add extra power to your product.  Coming from an ex-Program Manager of Excel this is not unexpected, but please: in all things, moderation.  The power in Microsoft's Office suite necessitated a drastic overhaul of their UI paradigms for Office 2007 because users were stuck in nested menu hell, concentrating more on how to accomplish their tasks than they were on what they wanted to accomplish!

<h3>Python</h3>

Michael Sparks from the BBC's R&D arm spoke about Python, using <a href=""http://norvig.com/spell-correct.html"">Peter Norvig's spelling corrector</a> as the basis for his talk.  This was quite an impressive talk in many respects, simultaneously illustrating the power of Python (the code was ~21 lines of code, all function definitions) and walking through Norvig's surprisingly simple implementation of a complex software component.  The corrector is used on Google's search pages, and uses Bayes' theorem to guess the word that you meant based on a corpus of words that it might have been, and a couple of rules indicating how you might have mis-spelled the word.  These rules are things like transposed letters, a missing letter, an added letter, etc.  Google, of course, uses as its corpus of knowledge <a href=""http://www.youtube.com/watch?v=iDbyYGrswtg"" title=""Moss introduces Jen to a new concept in business technology: The Internet."">The Internet</a>.

<h3>Android</h3>

Reto Meier of Google spoke eloquently on the Android platform.  This was a very interesting talk, and illustrates how Google gets the idea of helping developers in a way that Apple just doesn't.  Android is completely Java-based; Apple require developers to code in Objective-C (more on this later).  The Android framework looks like it might be the simplest framework for mobile development, and the devices are proving to be hot contenders to the iPhone (see The Register's reviews of the <a href=""http://www.reghardware.co.uk/2009/08/10/review_phone_htc_hero/"" title=""HTC Hero review on Reg Hardware"">HTC Hero</a> and the <a href=""http://www.reghardware.co.uk/2009/05/06/review_smartphone_htc_magic/"" title=""HTC Magic review on Reg Hardware"">HTC Magic</a>, for example). 

The Android SDK includes APIs to call into a number of services (including location, search and multimedia) and sensor measurements.  The sensors include accelerometers, orientation sensors, a compass, and even a thermometer.  Like most mobile platforms these days, Android also supports home-screen widgets. 

Unlike the iPhone, Android supports background processes, allowing for multi-tasking.  Therefore, you can listen to music whilst reading your emails, something the iPhone is infamously unable to even with its own iTunes app.  Partially as a result of the multi-tasking support, it also supports inter-process communications, allowing applications to share data.  For example, you can create an ""Intent"" in a contacts application that exposes a Geo URL (maybe retrieved from the contact's address), and open it in Google Maps.  To desktop users, this is nothing new: ShellExecute and its ilk have been around since the dawn of time almost. 

New features for Android version 2.0 include Bluetooth support (this was removed from the 1.0 release), aggregation of contact information from different sources, a camera API (effects, flash mode, focus mode, etc., etc.).

The Android development environment is Eclipse, with some specific plug-ins.  A nice feature of this is the ability to create a completely custom Android softphone in the emulator, defining the SDK version and hardware parameters from whether a camera is installed right down to the capacity of the SD card (and whether or not there is even an SD card present).  The virtual devices are significantly slower than hardware devices, but Meier said that most apps run tolerably on the virtual devices. 

Interestingly, the Android's localisation support looks very good: the developer provides separate resources files for each locale, they're compiled into the app, and the appropriate file is loaded based on the phones locale settings.  This approach also works for supporting different hardware configurations.

Sadly, the iPhone continues to reign supreme, even amongst the developers; when asked, fewer than 1% of the attendees owned an Android phone.  

<h3>jQuery</h3>

This was mostly an introduction to jQuery - what it is and how to use it - but it also covered the slightly more advanced topic of jQuery plugin development.  Presented by Remy Sharp of the Full Frontal JavaScript Conference.

If you've done any web development in the last year or two, you've probably come across jQuery; if you haven't, it's a framework for JavaScript that takes the pain out of JavaScript development by abstracting away the various browser quirks and differences, and provides a nice API for manipulating the various bits of HTML on your page.  This API is based around the CSS selectors, so it allows developers to reuse existing knowledge.  For example, the following example applies row striping to all tables on the page:

<blockcode language=""javascript"">$('table tr:nth-child(odd)').addClass('odd');</blockcode>

There's a useful sandbox site available at www.visualjquery.com for trying out your jQuery code. 

The basic execution model jQuery uses is ""Find or create something, then do something"".  To facilitate this model, the jQuery object supports chaining; i.e., each method on the jQuery object modifies the jQuery object and returns it.  Iteration is implicit within the API, such as via the each() function.  It's also important to note that the selectors fail silently, and that they are evaluated from right to left.  For example

<blockcode language=""javascript"">$('a[title][hash*=""foo""]');</blockcode>

searches first for all elements with the hash attribute containing a value like ""foo"" (the values ""foobar"" ""myfoo"" and ""myfoobar"" will all be matched), then restricts that set to the elements that also have the title attribute applied, and finally restricts that set to just anchor elements.

Furthermore, you can contextualise the query by passing in a second selector; this can dramatically improve performance.  For example

<blockcode language=""javascript"">$('.header', '#main');</blockcode>

looks within in the tree rooted at the element with id ""main"" for elements with the header class applied. 

Functions that deal with some kind of value are generally accessors, i.e. they can set and retrieve the value.  Finally, there are two types of search: find() implements a Depth-First Search (vertical) on the HTML tree, whilst filter() implements a Breadth-First Search (horizontal). 

Check out http://codylindley.com/jqueryselectors to see these in action.  

<h3>FogBugz 7</h3>

Joel gave us a demo of <a href=""http://www.fogcreek.com/FogBUGZ/"" title=""FogBugz"">FogBugz 7</a>, and the related sales pitch.  It's a pretty neat issue tracking system, and a big improvement over FogBugz 6 from what I gather.  However, I found the user management features are sorely lacking to the point that managing more than ten, maybe twenty, users must be incredibly frustrating, and there are few permissions to lock down the actions a user can take on a bug report at any stage of its lifecycle.

<h3>Stack Overflow Careers</h3>

Jeff Atwood introduced this new job hunting service from the Stack Overflow team, currently in beta.  The twist is that employers come to Stack Overflow Careers to find new employees, rather than posting jobs and prospective employees applying for them.  

There's an introductory offer currently running for developers where you get 3 years' membership for $29 (<strong>expires 9 November 2009, tomorrow!</strong>).  Pricing for employers is still being decided, but I think this is where Joel and Jeff intend to make the serious money so it won't be cheap.  This seems also to be aimed at avoiding recruitment agencies and lower-standard employers:

<blockquote>We believe that <span style=""color: red;"">every professional programmer should have a job they love</span>, and current sites like Monster, DICE, craigslist, and so forth do a woefully inadequate job of matching professional programmers with the type of employers who understand the true value of programmers who <a href=""http://www.joelonsoftware.com/articles/HighNotes.html"" title=""Hitting the High Notes from Joel on Software"">hit the high notes</a>.</blockquote>
 
That's a fairly half-baked explanation of a pretty cool concept, so if you want to find out a bit more, check out <a href=""http://blog.stackoverflow.com/2009/10/introducing-stack-overflow-careers/"" title=""Introducing Stack Overflow Careers"">the announcement from which the above paragraph is taken.  Joel has also done <a href=""http://www.joelonsoftware.com/items/2009/11/05.html"" title=""Upgrade your Career"">an announcement</a> of the service, in his own inimitable style :-) 

<h3>Qt</h3>

The only key takeaway from this presentation was ""avoid programming for Symbian and Qt at all costs"".  Architecturally, it looks like quite a good SDK (and it runs on desktops as well as Symbian phones), but the developer support and build quality of the SDK are both very poor, to the point where the Nokia guy, Pekka Kosonen, couldn't get his demos working.  Luckily Pekka was quite candid about the problems facing Nokia and what his talk lacked in quality he more than made up for in humour!  There are plans under foot to improve the situation, but we're talking years until they've caught up with Apple. 

Amusingly, the Qt freebie (a bag) was about as useless as the product itself, as the tags on the zips kept falling off!

<h3>iPhone</h3>

Ok, so I'll start this off with a disclaimer: what follows is all my own opinion, formed during the talk; it may not match your own experiences :-) 

I like Apple's products - I couldn't live without my iPod Nano - and I still use my old G4 iBook I had at University, even if it is slowly packing up.  OS X is a good operating system, although it sounds like Snow Leopard would be best shuffled under the carpet (or maybe turned into one :-) 

I don't (currently) own an iPhone because it's waaaaaaaaaaay out of my price range, but having played with a couple and compared it with other devices, it does seem that the iPhone 3GS is the best-available smart phone today (in spite of its faults, like the camera, and the lack of support for background processes, and...)

However, it's painfully obvious that Apple's focus is not development.  Whilst the XCode IDE seems like quite a good product, the Objective-C language takes all those conventions and improvements in language design that have been built up over the last n years and throws them out the window.  There's an old joke that the only valid measurement of code quality is the number of WTFs/minute scored when reading it: 

[img_assist|nid=117|title=Code Quality|desc=The only valid measurement of code quality is WTFs/minute|link=url|url=http://www.osnews.com/story/19266/WTFs_m|align=center|width=500|height=471]

Note even the good code has a couple :-)  The problem with Objective-C is that the programming language is itself a source of WTFs.  Take this code snippet for example:

<blockcode language=""objc"">
@interface SomeClass : NSObject
{
    -(void) setName:(NSString*) newName;
}
</blockcode>

Intuitively, you might expect to be defining an interface, a high-level abstraction to a class.  Instead, that @interface keyword is completely misleading, and the code snippet instead defines a class (an interface is defined using the @protocol keyword, apparently).  Furthermore, this is a header file; the implementation of the class is stored separately, just like C/C++.

Another snippet, this time to initialise, use and dispose of an object:

<blockcode language=""objc"">
SomeClass* c = [[SomeClass alloc] init];
[c setName: @""elephant""];
[c release]
</blockcode>

What's that?  Manual memory management?  Pointers?  <strong>*Shudder*</strong>. If this were supposed to be a C++ equivalent, I'd be less worried.  However, Objective-C is pitched as a fully modern language up there with C# and Java and it won't even manage memory for you?  Modern languages are supposed to abstract away these concerns to make developers more productive. 

Apple have done themselves no favours by adopting the Objective-C language; whilst the Android talk required no introduction to Java to discuss the topic, Phil Nash spent twenty minutes providing a basic introduction to Objective-C before we could even get on to the iPhone-specific stuff.  This meant that we didn't cover much in terms of the device's capabilities at all, unlike the Android talk. 

I had high hopes for this talk and genuinely wanted to learn about iPhone development, but instead I got a tutorial on Objective-C.  For me at least, iPhone development appears to be a big bag of FAIL.

[<strong>Note</strong>: I think I've fucked up the code samples, as GeSHI is failing to correctly syntax-highlight them.]

<h3>Jon Skeet</h3>

Jon Skeet, Googler, MVP, and most importantly the top user on Stack Overflow with a whopping reputation of 111,451 [at time of writing - Ed.] gave a thoroughly entertaining talk rant on the problems humanity has created for developers.  You can read the <a href=""http://msmvps.com/blogs/jon_skeet/archive/2009/11/02/omg-ponies-aka-humanity-epic-fail.aspx"" title=""OMG Ponies!!! (Aka Humanity: Epic Fail)"">full transcript</a> on his blog, and a <a href=""http://vimeo.com/7403673"">video</a> is also available.

Topics covered included rounding errors, the different semantic interpretations of numbers (e.g., £5.50 vs 5.50kg), line breaks, the Turkey test, time and time zones.  If you fancy a laugh, check it out :-)

<h3>How not to design a scripting language</h3>

Paul Biggar from Trinity College Dublin filled the ""Graduate Student"" slot at the London event.  His slides (with speaker notes) are available here. There's not a great deal to say on this talk, as it was entirely theoretical.  Some of the theory had practical implications of course, with Paul focussing on Python (for no good reason; he could equally have chosen Ruby, for example). 

<h3>Yahoo! Developer! Tools!</h3>

I'll say this about Christian Heilmann, Developer Evangelist at Yahoo!: he's an excellent presenter, but it's impossible to take notes because he moves at lightning speed.  His slides also contain little to no content, consisting mostly of <a href=""http://icanhascheezburger.com/2009/11/03/funny-pictures-pumpkins/"">lolcats</a> :-)  What follows is therefore based on my memory of the talk, which is fading quickly. 

Christian gave an excellent overview of Yahoo!'s developer tools, including YUI (a JavaScript library similar in intent to jQuery; he did some not-too-subtle jQuery bashing in the process) and Grids (for creating CSS layouts using a visual designer).  The most powerful tool he demonstrated, however, was <a href=""http://developer.yahoo.com/yql/"">YQL</a>, the Yahoo Query Language.  This tool is effectively SQL for The Internet; it pulls together the various different APIs exposed by the multitude of applications available on the web and allows you to cross-reference and ""mash-up"" the content returned.  Heilmann's own <a href=""http://www.wait-till-i.com/"">homepage</a> is completely constructed from YQL queries and the Yahoo! Grid tool.  Here's a simple YQL query to retrieve my TweetStream:

<blockcode language=""sql"">select * from twitter.user.status where id='alastairs'</blockcode>

It's also worth mentioning that having seen Christian talk at the <abbr title=""Future of Web Apps"">FOWA</abbr> Tour in Cambridge on YQL, little of this talk was new to me.  After about the third or fourth time, the content might get a bit dull (even if he is an entertaining speaker).  

<h3>Conclusion</h3>

The London Dev Day was a huge success, and for me personally very interesting. I picked up a number of topics about which I know very little (Python, jQuery) or nothing (iPhone, Android).  I'm a big fan of having a breadth of knowledge as well as a depth of knowledge, and that is one of the things the Dev Days idea promotes most strongly. I will definitely be going again next year, and I would strongly recommend it to anyone even vaguely interested in going."
NMock 2 vs Moq,NULL,"You may remember from <a href=""http://www.codebork.com/coding/2008/08/24/nmock-framework.html"" title=""NMock Framework"">my previous blog post on mocking frameworks</a> that I'm a bit of a fan of this kind of tool.  They're great for simplifying unit testing, and can also help guide you in to writing better, more loosely-coupled code via <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html#DIP"" title=""Dependency Inversion Principle: one of the SOLID Principles of OOD"">Dependency Injection</a>.  

Last night I spent some time porting some of my unit tests for a side project I'm working on from <a href=""http://www.nmock.org/"">NMock</a> 2 to <a href=""http://code.google.com/p/moq/"">Moq</a> 4 (currently in beta).  My reasons for switching were three-fold: first, NMock can only mock interfaces which was causing problems with testing some parts of my code that relied on elements of the ASP.NET MVC framework; second, NMock is unable to support some of the newer features of .NET as it was written to target .NET 2.0 (whilst each version of Moq is matched to the respective version of .NET, so the Moq 4 beta can utilise the C# 4.0 goodies like dynamic typing); last, the NMock project doesn't seem to be as active as it once was, with the last official build released in January 2008.  

As a result of this switch, I'm in more of a position to critically evaluate NMock and Moq than I was when I wrote my original post.  Here's a summary of my thoughts and experiences.
<!--break-->
Moq (which can be pronounced either as ""Mock"" or as ""Mock-You"") is a relatively new mocking framework on the scene, with v1.0 released in December 2007.  It has been designed specifically to integrate deeply with C# 3.0 and take advantage of the new language features in this version, such as implicit typing, lambda expressions and LINQ expression trees.  This enables you to create mock objects, assign them behaviours and record expectations about them in a simple way; it also means that you get full Intellisense support when creating your mock objects.  Moq's other advantage is that it eschews the Record/Replay syntax favoured by <a href=""http://ayende.com/projects/rhino-mocks.aspx"">Rhino Mocks</a>, which I find quite opaque and unintuitive.

NMock, whose last build was released in January 2008 (coincidence?), was originally started for .NET 1.0, and was updated for .NET 2.0 which shipped in ~2005.  It does not take advantage of the C# 3.0 language features, and so relies on a ""fluent"" syntax for creating stubs and recording expectations.  This makes reading the  expectations and stubs quite natural (the syntax is <em>very</em> close to English), but it is not the most natural way to program.  Additionally, there is no Intellisense support for the interfaces being mocked, as the framework relies quite a bit on magic strings.  

""Installing"" Moq is really very simple: just download a zip file, extract it to some location, and reference Moq.dll (the only binary included) in your VS project.  The fact that it's completely contained within one binary makes it much simpler to manage the dependency on this framework in comparison with, say, <a href=""https://www.hibernate.org/343.html"" title=""NHibernate homepage"">NHibernate</a>: at only 285KB, it can be added to your project without bloating your project.  This all holds for NMock as well, of course.

The Moq paradigm is quite different from NMock's.  In NMock, you specify a local variable typed to your interface, and create a mock implementation (carried out by NMock via Reflection), as follows:

<blockcode language=""csharp"">
    Mockery mockFactory = new Mockery();
    var mockRepository = mockFactory.NewMock<IRepository>();
    var controller = new MyController(mockRepository);
</blockcode>

The advantage of this is that you can use your mock objects directly in your unit tests, provided you have written the code under test to adhere to the <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html#DIP"" title=""Dependency Inversion Principle: one of the SOLID Principles of OOD"">Dependency Inversion Principle</a>.  In Moq, the objects that you create are mocks, not implementations of your interface, so you have to do a smidgeon extra juggling to pass the mocked object around for use.  However, the power gained from this small sacrifice is quite immense: you can change the behaviour of mocks to be more ""strict"" (throwing an exception for any un-mocked members, as is the NMock behaviour); to act as ""partial"" mocks (i.e., calling into the base class's implementation for any un-mocked members); and more. Here's a simple example of how to create and use a mock object:

<blockcode language=""csharp"">
    Mock<IRepository> mockRepository = new Mock<IRepository>();
    var controller = new MyController(mockRepository.Object);
</blockcode>

Note how we have to use the <code language=""csharp"">Object</code> property on the mock object to access the actual mocked object.  

In NMock, you create stubs and expectations as follows:

<blockcode language=""csharp"">
    var id = 1;
    var user = new User { Username = ""joebloggs"", Role = ""user"" };
    Stub.On(mockRepository).Method(""GetById"").With(id).Will(Return.Value(user));
    Expect.Once.On(mockRepository).Method(""SomeOtherMethod"").WithNoArguments();
</blockcode>

Note the magic string indicating which method to stub, the similarity between the stub and the expectation, and the need to know of the internal workings of the GetById method.  If you don't record an expectation on a method and it is invoked on the mock object, an exception is thrown and your unit test fails.  These three things are NMock's greatest weaknesses, and it was only when I moved to Moq that I realised the importance of the second one.  

NMock conflates the ideas of a stub and an expectation, which can cause confusion for the developer.  According to the <a href=""http://www.nmock.org/cheatsheet.html"">NMock cheatsheet</a>, Stub is equivalent to Expect.AtLeast(0).  As a result, the API is not as clean as it might have been: stubs automatically create an expectation, and you create stub implementations when recording expectations.  

Moq clearly separates these ideas, and additionally removes the need for the mock factory (NMock's <code language=""csharp"">Mockery</code> class).  This allows a developer to get up and running with Moq more quickly than they might with NMock.  Handily, Moq does support the notion of a mock factory (via the cunningly-named <code language=""csharp"">MockFactory</code> class), which provides for creating many mocks with the same settings (i.e., ""strict"", ""partial"", etc.).  Creating a stub can be accomplished as follows:

<blockcode language=""csharp"">
    var id = 1;
    var user = new User { Username = ""joebloggs"", Role = ""user"" };
    mockRepository.Setup(repo => repo.GetById(id)).Returns(user);
</blockcode>

We can verify our expectations as follows:

<blockcode language=""csharp"">
    mockRepository.Verify(repo => repo.GetById(id), Times.Exactly(1));
</blockcode>

In both cases, the <code language=""csharp"">repo => repo.GetById(id)</code> syntax is a C# lambda expression.  Under the hood, this is a kind of anonymous delegate, and actually ends up creating the same IL as an anonymous delegate would.  This particular lambda is equivalent to the following method:

<blockcode>
    public User Anonymous(IRepository repo) {
        var id = 1;
        return repo.GetById(id);
    }
</blockcode>

It's important to note, however, that Moq uses lambdas to create expression trees to match on, and doesn't actually execute the lambda in the example above.  That is to say, Moq looks for method calls that match the pattern of ""GetById on an object of type IRepository with argument of type int and value id"".  

One hurdle I ran into when porting from NMock to Moq was an unhelpful and weird exception: my mocks were trying to convert objects of incompatible types, e.g., converting an int to a User object.  The reason for this is because I had created fake objects for use in my project, and was then referencing the property on the object in my lambda.  For example:

<blockcode language=""csharp"">
    var user = new User { Id = 1, Username = ""joebloggs"", Role = ""user"" };
    mockRepository.Setup(repo => repo.GetById(user.Id)).Returns(user);
</blockcode>

This messes up the expression tree matching that I mentioned above, so it's important to make sure you pass the right thing into your mock.  For example, the following code works as expected.  

<blockcode language=""csharp"">
    var userId = 1;
    var user = new User { Id = userId, Username = ""joebloggs"", Role = ""user"" };
    mockRepository.Setup(repo => repo.GetById(userId)).Returns(user);
</blockcode>

One annoyance is that if you want your stubbed method to return <code language=""csharp"">null</code>, you can't use the <code language=""csharp"">Returns()</code> method as simply as you'd like: a compiler error is thrown complaining about an ambiguity between the overloads of <code language=""csharp"">Returns()</code> and <code language=""csharp"">Returns<T>(Func<U>)</code>.  As such, you have to use a simple lambda expression as a parameter to the <code language=""csharp"">Returns()</code> method like so:

<blockcode language=""csharp"">
    mockRepository.Setup(repo => repo.GetById(userId)).Returns(() => null);
</blockcode>

The <code language=""csharp"">() => null</code> lambda is equivalent to the following method:

<blockcode language=""csharp"">
    public User Anonymous() {
        return null;
    }
</blockcode>

Other than this, however, Moq is a bit of a joy to use.  Because of the use of lambdas, you get Intellisense support on your own API (e.g., when typing out <code language=""csharp"">repo.GetById()</code>), which obviously speeds things up immensely."
An Education,"[4.5/5]

This moving story of a teenager growing up in 1960s London charms, startles, amuses, shakes and delights in equal measure.
[img_assist|nid=120|title=An Education|desc=L-R Dominic Cooper, Rosamund Pike, Peter Saarsgard, Carey Mulligan|link=none|align=center|width=400|height=266]
","
[4.5/5]

I can pretty safely say that <em>An Education</em> is the best British film you will see this year, and is definitely one of the best films of any origin that you will see in 2009.  It is beautifully shot, slickly written, and astoundingly well-acted across the board, it neatly captures the experience of growing up in London in the 1960s.  

<a href=""http://www.imdb.com/name/nm1659547/"">Carey Mulligan</a>, who as Sally Sparrow lit up Stephen Moffat's excellent episode of <em>Doctor Who</em>, <em>Blink</em>, in 2007, is otherwise relatively unknown: her most high-profile film role before <em>An Education</em> was perhaps as Kitty Bennett in Joe Wright's 2005 adaptation of <em>Pride and Prejudice</em>.  Needless to say, she's been quite busy in the meantime with other TV roles, and some stage work as well.  

Mulligan plays Jenny Mellor, a bright, pretty, sixteen year-old school girl with her eyes set on an English degree at Oxford.  Her father (<a href=""http://www.imdb.com/name/nm0000547/"">Alfred Molina</a>) is pushy, encouraging her in her academic activities and her hobbies, and very set in his views of what is best for his daughter, and what young people should be doing.  Her mother (<a href=""http://www.imdb.com/name/nm0786806/"">Cara Seymour</a>) is a stereotypical quiet-and-slightly-downtrodden 1960s housewife, wanting the best for her daughter and supporting her husband.  One day on her way home from school in the pouring rain, Jenny is offered a lift by a charming youngish man, David (<a href=""http://www.imdb.com/name/nm0765597/"">Peter Saarsgard</a>, who at times has more than a little of Kiefer Sutherland about him).  

David has a flash maroon Bristol sports car and appreciates music and art: he offers Jenny an exciting ray of light in her otherwise dull and grey existence.  Soon he is starting to distract Jenny from her school-work, drawing her into his world of concerts and champagne and fine clothes and jazz clubs and fancy restaurants with his friends Danny (<a href=""http://www.imdb.com/name/nm1002641/"">Dominic Cooper</a>) and Helen (a sparkling <a href=""http://www.imdb.com/name/nm0683253/"">Rosamund Pike</a>) and they quickly fall in love.  Jenny becomes uneasy about some aspects of David's life and his business dealings with Danny as she learns more about his world, but he distracts her with gifts and trips to Oxford and Paris.  Jenny's parents are as enamoured with David as she is herself, and when David proposes to Jenny her father gives them his blessing and explains how its worked out well for Jenny and that she needn't bother with Oxford any longer.  And then David turns out not to be who he said he was.  

[img_assist|nid=120|title=An Education|desc=L-R Dominic Cooper, Rosamund Pike, Peter Saarsgard, Carey Mulligan|link=none|align=center|width=400|height=266]

<a href=""http://www.imdb.com/name/nm0394984/"">Nick Hornby</a>'s (<em>High Fidelity</em>, <em>About a Boy</em>) screenplay is well-written, moving seamlessly between scenes and dwelling on the more touching or startling moments with precisely the right amount of emphasis.  It crackles and sparkles with wit: whilst this is not a comedy by any stretch of the imagination, you will find yourself laughing regularly until the final scenes.  <a href=""http://www.imdb.com/name/nm0771054/"">Lone Scherfig</a>'s direction provides some memorable scenes and paces the on-screen action beautifully: never do you feel the film is dragging nor rushing ahead.  An example is the re-styling of Jenny in the vein of Audrey Hepburn for the trip to Paris; whilst Mulligan is transformed into the very image of Hepburn, the moment that takes your breath away is the styling of 1960s Paris after Hepburn's own 1964 film <em>Paris - When it Sizzles</em>.

There are some stand-out performances, as well.  Carey Mulligan is utterly superb in this respect, bringing an amazing mix of naïveté, youthful willfulness, vulnerability and braggadocio to a very complex role.  As such,  I except and greatly hope we will be seeing more of her in the future.  Similarly, Rosamund Pike's portrayal of the beautiful but <em>very</em> ditsy Helen cannot go unmentioned.  Pike reportedly wanted the smaller role <a href=""http://www.imdb.com/title/tt1174732/trivia"">because ""no one ever lets me be funny""</a> and she really excels in it, bringing (misplaced) snobbishness, airheaded gazes and no-questions-asked acceptance of Danny's and David's business in equal measure.  Dominic Cooper, who was also excellent in the film of Alan Bennett's <em>The History Boys</em> in 2006, and in other roles since, revels in the fun of the role whilst bringing a small but necessary amount of compassion to Jenny's situation later into the film.  <a href=""http://www.imdb.com/name/nm0000668/"">Emma Thompson</a>, in a role that amounts to little more than a cameo, is as excellent as ever playing the matriarch of a headmistress at Jenny's school.  

In short, this really is a must-see film on all counts.  If you miss it at the cinema, be sure to pick it up next year when it's released to DVD!

<hr />

<h3>Spoiler alert!</h3>

Not a spoiler in the usual sense, that I'm going to give away crucial plot details, but the information below may still spoil the experience of the film for you, which is why I have separated this from the main body of the review.  It changed my experience of the film for me; more on that in a moment.  

This is, in fact, a true story, based on the memoirs of the journalist Lynn Barber (formerly of The Observer, now with The Telegraph).  You can read an excerpt from the book of the same title <a href=""http://www.guardian.co.uk/culture/2009/jun/07/lynn-barber-virginity-relationships"" title=""My harsh lesson in love and life"">here on The Guardian's website</a> (it also provides a detailed synopsis of the plot of the film); an interview with Barber following the publication of her memoirs is available <a href=""http://www.telegraph.co.uk/culture/books/5507121/Lynn-Barber-I-know-Ive-done-a-bad-thing.html"" title=""Lynn Barber: 'I know I've done a bad thing'"">on the Telegraph's website</a>.  What surprises me a bit is that the film is not being sold as a true story at all: there is no mention of the fact on the posters, and I haven't seen any interviews about the film mentioning it.  

I had read the extract from Barber's memoirs back in June when it was published; I don't remember the film being mentioned in the article at that point, but it wasn't something that would necessarily stick in my mind.  It was with some shock, then, that I realised about a third of the way into the film that some of the plot details were remarkably familiar.  When it all slotted into place, that Jenny, David, and every other character in the film were in fact real people and every scene had played out nearly fifty years ago, and that I suddenly knew how this story was going to end, it became a somewhat uncomfortable film to watch.  I spent the rest of the film feeling slightly sickened by what I was watching on screen &mdash; seeing a real person's life crumble &mdash; and I was no longer able to enjoy the film in isolation, to feel as detached from the subject matter as I might have otherwise.  As a result of this, the film <em>has</em> left more of an impression on me, but I'm in two minds as to whether or not that's a good thing; as much as I loved this film and would love to watch it a second time, I think I would struggle to do so."
Harry Brown,"[4/5]

This bleak and, at times, disturbing revenge flick sees Michael Caine deliver another strong performance in a story that might be considered over-egged.

[img_assist|nid=122|title=Harry Brown|desc=Even in his 70s, Michael Caine is still bad-ass|link=none|align=center|width=400|height=267]
","
[4/5]

<em>Harry Brown</em> was declared by <a href=""http://www.shortlist.com/"" title=""ShortList homepage"">ShortList</a> to be the best British film of 2009.  I don't feel that it can justifiably be given that accolade in the same year that the wonderful <a href=""http://www.codebork.com/films/2009/11/26/education.html"" title=""An Education review on CodeBork"">An Education</a> played in cinemas.  Whilst both films include an impressive cast of British actors; some stand-out performances; have well-shaped, absorbing plots; and are brilliantly directed; <em>An Education</em> is quite simply the better film.  But enough with the comparisons.

Michael Caine plays Harry Brown, an ex-serviceman living on a sink estate in Elephant and Castle.  Gangs of ""feral youths"" roam the estate terrorising its inhabitants.  The film opens with an initiation ritual involving a gun, apparently filmed on a mobile phone; next we see a young mother murdered as she pushes her toddler through the estate.  Other scenes involve sexual harrassment of a young lady whilst her boyfriend is physically initimidated, violent weapons, drugs.  The police, whilst mostly well-intentioned, are ineffectual.  When they do arrest one or more of the gang members, they are unable to get any information out of them (""No comment"" being the stock answer) and are forced to release them again.  If this film were a colour, it would be pitch black.  It shamelessly plays to the ""broken society"" fears of Middle England, and with a plot like that you can see why <em>The Daily Mail</em> proclaimed this to be ""finally a film that really matters"".  

Over the course of the film, Brown suffers two bereavements, and is pushed over the edge into vigilantism.  D.I. Frampton (played rather excellently by Emily Mortimer) is the only one who recognises the latest deaths on the estate are the work of Brown, but unsurprisingly is unable to convince her superintendent of this fact and has no evidence to charge him or even arrest him.  With an impending operation to clear up the estate, the superintendent only has his eyes on one thing: personal glory.  Ultimately, and utterly appallingly, Brown is betrayed by his sole remaining friend.  

[img_assist|nid=122|title=Harry Brown|desc=Even in his 70s, Michael Caine is still bad-ass|link=none|align=center|width=560|height=373]

Caine's performance is rock-solid, and can be considered to extend his roles from the 60s and 70s (there are parallels to <em>The Italian Job</em> and <em>Get Carter</em>, for example). Emily Mortimer and Charlie Creed-Miles put in strong supporting performances as the CID policeman tasked with policing the estate, such as their roles are.  The film really revolves around Caine's performance; no other character comes anywhere close in terms of screen time or development.  

The script is visceral, and the cinematography fits the story: mostly shot at night, <em>Harry Brown</em> is as visually dark as it is metaphorically.  Director Daniel Barber's vision is sound and well-realised, and intensely gripping; Barber especially knows how to work tension in his audience.  It can be forceful &mdash; as the film reached its climax, I felt myself shrinking into my seat, utterly oppressed &mdash; but this is no bad thing, and is effective in a film like this.  

However, in assessing the film as a whole, I keep coming back to the plot, and I can't help but feel that it drags the film down in my estimation.  It is <strong>relentlessly</strong> bleak, and its vision of a dysfunctional, broken society seems over-wraught, almost as if writer Gary Young was using it as an exercise in hyperbole.  At the climax, it is <em>so</em> appalling and repugnant that it loses some of its effect; you suddenly find yourself outside the film, looking in.  It essentially destroys the last vestige of believability Young and Barber are trying to maintain.  

I know I said this about <em>An Education</em> &mdash; albeit for differing reasons, and I have since changed my mind &mdash; but even a month after seeing this film I am not convinced it will be one that ends up in my DVD collection.  That said, <em>Eastern Promises</em> is sat on my shelf, so there may be hope for it yet."
Daybreakers,"[3.5/5]

This mongrel transcends the vampire genre, but the final descent into unmitigated gore ruins an otherwise interesting premise.

[img_assist|nid=124|title=Daybreakers|desc=We're the ones holding the crossbows|link=none|align=center|width=400|height=268]
","
[3.5/5]

Generally, I don't think I'm a huge fan of ""genre"" flicks.  Ok, except maybe Sci-Fi, but even then the best ones are the ones that do something clever with the genre, like last year's <em>Moon</em>.  <em>Daybreakers</em> could have been that film in the vampire genre, but narrowly misses.

[img_assist|nid=124|title=Daybreakers|desc=We're the ones holding the crossbows|link=none|align=center|width=560|height=375]

Having thoroughly enjoyed the first season of HBO's <em>True Blood</em> on Channel 4 before Christmas, I'm quite vampired up.  I can see the appeal of the <em>Twilight</em> movies to their target audience, but they are just too young, too tame, too teenage to spike my interest.  So it was with some interest that I sat down to <em>Daybreakers</em>, an interesting mongrel of genres.  

Billed as <em>The Matrix</em> meets <em>28 Days Later</em>, there is quite a lot of science fiction in this vampire film, directed by the Australian twin brothers Michael and Peter Spierig.  Set in 2019, the human race has been turned into vampires, and in the space of ten years has managed to adapt to their new circumstances.  Life has been completely reworked to run at night: cars come with ""day-driving mode"" where thick UV shutters cover the windows and the driver navigates by camera; condominiums have shuttered underground parking and no windows; subways have been converted to subwalks; coffee shops serve blood; humans are farmed.  Yes, that's the <em>Matrix</em> parallel: here humans are farmed for their blood and not to produce electricity.  

There is a problem, however: the number of humans in the farms is dwindling, mostly due to old age and lack of supply, and vampire mega-corp Bromley Marks &mdash; headed up by a devilish Sam Neill &mdash; is looking for solutions to the problem, notably a blood substitute.  This is an idea that likely won't be strange to most Americans and Australians: agricultural giants (or Big Farma, see what I did there?!?) of both countries have been punting substitutes for a while to shore up their business against future crop shortages.  

The hunger is making Earth's once human inhabitants more savage, and the malnutrition is starting to cause physical deformities and loss of mental focus.  Some vampires have become desperate enough to feed off their own kind, if not themselves, with terrible and appalling results.  The problem increases over the course of the film as a world blood shortage develops, leaving the price of blood too high for the average family of four. 

Edward Dalton (the rather bland Ethan Hawke) is a haemotologist working on the blood substitute at Bromley Marks.  It's a thankless task: the most promising sample resulted in the violent (and comical) death of the test subject, and to make matters worse the CEO Charles Bromley (Neill) seems to be more interested in pumping up the price of the real thing.  One night, Dalton encounters some humans (Claudia Karvan, Willem DaFoe) with a possible cure for the vampire disease, and sets to work productising it.  It's slow work, and in the meantime the human safehouse is compromised.  As the film closes, an alternative method of propagating the cure has been discovered, and with it the bodies start to pile up, torn apart by the ravenous vampires.  

It is this climax that mostly turned me off the film.  The descent into violence and gore rather sullied an otherwise intelligent and well-developed idea.  The script is fairly average: the characters are mostly under-developed, and as a result the cast struggles to make much of the material.  The notable exception here is Willem DaFoe who manages to put together something from very little; Ethan Hawke is as wet he was in <em>Gattaca</em> (which was otherwise an excellent film).

Perhaps part of the problem is that there is too much crammed into <em>Daybreakers</em> for a 110-minute film: is it sci-fi, is it vampire, is it fantasy, is it making political statements?  It valiantly attempts all four with some success, but it sacrifices the characters at the altar of pace.  We get some fleeting glimpses into the depth of some of the characters (Dalton and Bromley most notably), but there is no real coherent arc to them; they end the film much as they started it.

Onto the niggles. Throughout the movie, I was bugged by the inconsistencies in the vampire myth espoused by <em>Daybreakers</em>.  For example, the explanation for vampire's well-known dislike for sunlight was due to the UV rays.  Given that the fluorescent lights used to light their offices, subwalks and apartments also contain a high UV component.  Details like this shouldn't really be explained, because there's not really a way to do so in a satisfactory, consistent matter; that's why this falls into the sci-fi/fantasy pigeonhole.

Also, why was no attempt to contain the outbreak of vampirism made?  National borders could have been shut, quarantine measures could have been implemented&hellip;  I find it a leap of faith too far to believe that the infection spread quite <em>that</em> quickly.  The blatant product placement of Chrysler also bugged me, as it left me wondering whether Dodge, VW, etc., had all been (literally) devoured in the infection.

In short, <em>Daybreakers</em> is an entertaining and above-average vampire flick that could have caused a serious re-think of expectations for the genre (and still might), but it ultimately left me feeling a little unfulfilled and wishing it could have done more."
Windows Live Writer,NULL,"<p>I’ve heard many good things about <a title=""Download Windows Live Writer"" href=""http://download.live.com/writer"">Windows Live Writer</a> as a blogging tool, so I thought I’d give it a go.&#160; Drupal’s great an’ all, but entering HTML manually to format your post is so 2001.&#160; So, here’s my first post from Live Writer!</p>  

<p>[img_assist|nid=128|title={ Insert Obligatory Image Here }|desc=Image © 2007 <a title=""MichaelMaggs&#39; user profile on Wikimedia Commons"" href=""http://commons.wikimedia.org/wiki/User:MichaelMaggs"">MichaelMaggs</a>|link=node|align=center|width=400|height=267]</p>
<!--break-->
<p>I’ve literally just got it installed, but my impressions are good so far (we’ll see how they change when I hit “Publish”…): Live Writer was an easy download and install – no surprise there, although it did try and give me the rest of the Live tools too – and I just had to add and enable the Drupal <a title=""Windows Live Writer BlogAPI project page on Drupal.org"" href=""http://drupal.org/project/wlw_blogapi"">Windows Live Writer BlogAPI module</a>.&#160; The <a title=""Windows Live Writer BlogAPI documentation"" href=""http://www.masterthebusiness.com/2008/09/11/windows-live-writer-and-drupal-6/"">documentation</a> is detailed and easy to follow, and although it is a little out-of-date it’s still correct.&#160; </p>  <p>Sadly, there’s no obvious list of implemented features which is disappointing considering the ambitions of the project maintainers.&#160; Some of the proposed features are listed on the setup documentation, and it seems at least some of them have been done; for example, the tags I have already defined are provided as options in Live Writer, and file uploads are apparently working (if you see a picture above, then they are!). </p>  <p>Anyway, here goes nothing…</p>

<hr />

<p><strong>Update 15 January:</strong> since posting this yesterday, I have tidied up the formatting to correct the rendering of the post.  Live Writer posts HTML to Drupal (unsurprisingly), but my default Input Format for blog posts severely limits the number of HTML tags that can be used.  As a result, the &lt;img&gt; tag and all the &lt;p&gt; tags were stripped out of the saved post before they were displayed, which meant that the post appeared as a single blob of text.  I've flipped the Input Format over to Full HTML for this post to fix the problem.  </p>"
Rachel Getting Married,"[4/5]

An excellent and intimate portrayal of internal conflict, addiction and loss, featuring a superb lead performance from Anne Hathaway.  

[img_assist|nid=129|title=Rachel Getting Married|desc=Left to Right: Anne Hathaway as Kym, Rosemarie DeWitt as Rachel. 
Photo by Bob Vergara © 2007 Sniscak Productions, INC. Courtesy Sony Pictures Classics. All Rights Reserved.|link=node|align=center|width=400|height=268]
","
[4/5]

For many critics, <em>Rachel Getting Married</em> was the film where Anne Hathaway finally proved her acting chops.  Previous turns had either been too small (<em>Brokeback Mountain</em>) or the film wasn't of a high enough quality to pass fair judgement (the wonderfully silly and actually quite enjoyable <em>Get Smart</em>; <em>Becoming Jane</em>).  Her performance in <em>The Devil Wears Prada</em> was strong, but seemingly not strong enough to make them sit up and take notice.  I'm not sure what it was they were missing or waiting for, but they seemed to find it here.

Hathaway plays Kym, a recovering drug addict, who has come home from rehab for her sister Rachel's wedding.  Contrary to your expectations from the title, the film is less about Rachel (Rosemarie DeWitt) than it is about Kym, and the backdrop of the wedding might be little more than a plot device to get the characters all into one place together.  However, weddings tend to be emotional and high-stress events for all involved (sometimes even the guests!), so the setting is not inappropriate given how the plot unfolds.  We learn of a tragedy, resulting from Kym's addiction, that hit the family in the past; the mum (Debra Winger) and dad (Bill Irwin) broke up sometime after this event, although remain on amicable terms.    

Kym, as might be expected, is a damaged girl and this is highlighted in her petulance, selfishness and attention-seeking behaviour.  At the wedding rehearsal dinner, when a number of people have given speeches in honour of the happy couple, Kym contributes her own that focuses more on her experiences as an addict in rehab.  It's clear that she was unhappy that the attention wasn't on her, and that she decided to fix that.  Unsurprisingly, this later provokes a row between Kym and Rachel.  

Underlying this, however, is a kind and loving personality that is struggling to come to terms with her addiction, the tragedy and rehab itself.  Director Jonathan Demme's capable hands shape and nurture the viewer's empathy for Kym: she doesn't want to hurt anyone, but she wants people to understand what she is going through.  Just because she's out of rehab doesn't mean she's completely recovered.  

[img_assist|nid=129|title=Rachel Getting Married|desc=Left to Right: Anne Hathaway as Kym, Rosemarie DeWitt as Rachel. 
Photo by Bob Vergara © 2007 Sniscak Productions, INC. Courtesy Sony Pictures Classics. All Rights Reserved.|link=node|align=center|width=400|height=268]

Incorporating and addressing the issues that it does, <em>Rachel Getting Married</em> tells a very human story, reflecting all aspects of human nature both ugly and beautiful.  A confrontation with her mother represents a low-point for Kym, whilst the kitchen competition between Sidney, Rachel's fiancé (Tunde Adebimpe), and her dad provides a light-hearted moment of familial warmth and very real fun.  The wedding turns out to be as joyful an event as it should be, and again we see the best of being human.  

No review of this film can be written without mentioning Anne Hathaway's performance.  She brings great depth to Kym, playing the complex emotions excellently and rendering the character's internal conflicts as raw, difficult and painful.  She strikes a good balance between the less pleasant components of Kym's personality and the kinder parts, allowing us to warm to and even identify with her Kym.  Hathaway really got inside the head of her character, and as a result put in a thoroughly convincing performance.  

However, it's also worth making mention of Rosemarie DeWitt's supporting role, and I firmly believe DeWitt hasn't received proper recognition for her performance: it's likely that she was simply eclipsed by Hathaway.  Rachel has to weigh up her own preferences and desires in regard to her wedding and reconcile them with her affection and empathy for Kym and her situation.  Hathaway and DeWitt have an excellent on-screen chemistry that makes their sororal relationship more than believable and breathes life into their shared scenes &mdash; in particular the arguments and the later more touching sequences.

The direction on the whole is good and it is shot in a very intimate, almost documentary style; indeed, some shots are seen through a hand-held camera.  The cinematography is spare, utilising very long sequences without cuts, and in scenes like the rehearsal dinner this creates a sense of great intimacy for the viewer, as though they are a fly on the wall.  The flip side, however, is that the pacing is sometimes a little slow leaving the viewer's attention wandering.  The afore-mentioned kitchen competition, for example, is rather drawn out considering the amount of plot and character development resulting from it.

This film is well worth watching, and I would suggest it is one to be shared with friends or a partner.  If you're into buying DVDs like me, I would strongly recommend this as a keeper; you're not going to watch it hundreds of times, but it's likely you'll want to see it more than once.  If you're more into renting-and-returning, get this on your list, and crack open a bottle of wine to accompany it."
ReelCritic - A New Film Blog,NULL,"As you may have already gathered from my Twitter feed, I've been working recently on ReelCritic, a new blog catering solely for my film reviews.  For a sneak peek at the design of the new site, pop on over to http://www.reelcritic.co.uk/.  Everything you find at that address is a non-functional mock-up, so please don't try to leave comments, etc.  There's only one review available, <a href=""http://www.reelcritic.co.uk/2010/01/16/rachel-getting-married.html"" title=""Review: Rachel Getting Married""><em>Rachel Getting Married</em></a>.  I'm currently working to port this design over to Drupal's theme engine so that I can get the blog fully up and running.
<!--break-->
The rationale for this change is to separate my posts more than is currently possible.  This blog was originally set up as a home for posts about programming, with the aim of learning more about programming and developing my skills.  The posts on other subjects were to be mere sidelines to the main theme, but it has become clear from my posting patterns over the last few months that this blog really has two main themes and as such doesn't really have space for both: the film reviews have, to an extent, pushed out the coding posts.  

This also opens further opportunities for me: for example, I intend to expand the scope of ReelCritic to cover my musings on film in general, and also TV.  Serendipitously, it also feeds back into my aims of learning more about programming, as I have been sharpening my CSS and HTML saws over the last couple of weeks preparing the new design, which all in all I'm quite chuffed with.  All the graphics were sourced from the ever-reliable iStockPhoto.  My intention is to make the film reel horizontally scrollable via a neat bit of jQuery that I'm yet to find/write.

[img_assist|nid=131|title=Reely Good|desc=Screenshot of ReelCritic's new design|link=node|align=center|width=400|height=241]

I can't really mention this without also mentioning the fantastic resource that is <a href=""http://www.doctype.com/"" title=""Web design Q&A - Doctype.com"">DocType.com</a>.  Essentially, this is <a href=""http://www.stackoverflow.com/"" title=""Stack Overflow"">Stack Overflow</a> for web design, but it also comes with the nifty feature of being able to supply a link to your problematic page and displaying a screenshot of the page next to your question.  Furthermore, it caches the CSS and HTML from your page at the time of posting, and displays them in syntax-highlighted pretty print.  The community is as friendly and helpful as that around Stack Overflow, albeit seemingly a bit smaller.  Judging by the design and features of the site, I can't help but think it is based on Stack Exchange, the software behind Stack Overflow, but some of the differences, such as the screenshot feature, make me think the similarity is just coincidence.  

In addition to the blog, I have started a new Twitter account for ReelCritic, which you can find <a href=""http://twitter.com/reel_critic"" title=""Alastair Smith (reel_critic) on Twitter"">@reel_critic</a>.  If you've enjoyed my reviews, you will likely enjoy the <a href=""http://twitter.com/reel_critic"" title=""Alastair Smith (reel_critic) on Twitter"">@reel_critic</a> tweet stream.  

My long term plan is to put together an aggregator site running at http://www.alastairsmith.me.uk/ to serve as my homepage that pulls in data from both CodeBork and ReelCritic, as well as my other online activities (e.g., Picasa, Twitter, <a href=""http://www.github.org/"" title=""GitHub - Social Coding"">GitHub</a>).  If there's anything you'd particularly like to see on any of my sites, leave me a comment or <a href=""/contact"" title=""Contact Alastair"">send me an email</a>."
Moving Home,NULL,"I find the whole meta-blogging thing a bit tiresome, really, but I thought I should probably do a quick update as CodeBork is moving home!  There are two main things you will notice: 
<ol><li>it should be <em>much</em> snappier than it was before, having the full benefit of a fatter pipe than an ""up to 8Mbps"" ADSL connection</li>
<li>Some of the images have disappeared.  They've been transferred successfully, I just need to win an argument with Drupal to get them to display.</li></ol>
<!--break-->
If you're interested in the gory details, CodeBork is now hosted with <a href=""http://mediatemple.net/"" title=""MediaTemple homepage"">MediaTemple</a> via their <a href=""http://mediatemple.net/webhosting/gs/"" title=""MediaTemple Grid-Service (gs) hosting"">Grid-Service (gs) package</a>.  This is an interesting service that offers you not just one machine, but a whole grid of them, allowing your site to scale when needed.  So, on the tiny off-chance that my blog should make it to the front page of <a href=""http://www.digg.com/"">Digg</a>, or be posted to <a href=""http://news.ycombinator.com/"">Hacker News</a>, and thereby receive thousands of hits a minute, I know my site will be able to handle it. 

The package is priced at a fixed $20/month, and offers 100GB storage, 1TB bandwidth allowance, and up to 100 unique sites may be hosted from the one package.  <a href=""http://www.reelcritic.co.uk/"" title=""Film views, reviews and news"">ReelCritic</a> will be going live from the same account in a few days, hopefully.  MediaTemple do a 30-day free trial if you are interested in playing around with the service.

This, along with developing the new ReelCritic site, has been taking up most of my spare time recently, which is why I haven't managed to get anything obvious done, like continuing my <a href=""http://www.codebork.com/category/coding/code-complete"" title=""Code Complete series on CodeBork"">Code Complete series</a>.  I hope to get back on to that soon.

<strong>Update:</strong> Apologies to anyone seeing this post twice.  I re-restored my database backup to resolve the problems I was having and it has mostly worked.  I now just have to fix up the images."
"NxtGen Cambridge: RDBMS, NoSQL and CouchDB",NULL,"If you've been following the developer hangouts in the last few months, you've probably heard at least a little bit about NoSQL and document databases. You may also have read how they're the best thing since sliced bread, and that NoSQL will be your new <abbr title=""Best Friend Forever"">BFF</abbr>.  Contrariwise, you may have read some of the <abbr title=""Fear, Uncertainty and Doom"">FUD</abbr> surrounding the subject and have *cough* a less rose-tinted view of the things.  

Last night, I attended the Cambridge <a href=""http://www.nxtgenug.net/ title=""NxtGen User Group"">NxtGenUG</a> meeting on this very topic, and I intend to distil some of what I learnt in this post; mostly it'll be me riffing around the stuff that Neil covered.  The talk was delivered by Neil Robbins, whose delivery was always energetic, interesting and informative.  The 100mph demo at the end of the talk was both simple and powerful.  He's a great speaker even if his slides were a bit wordy (with one or two being excessively so).  <a href=""http://www.twitter.com/NeilRobbins"" title=""Neil Robbins' Twitter feed"">Check him out on Twitter.</a>
<!--break-->
<h3>Background and Context</h3>
Neil began with a bit of background, which proved useful in providing some context to the new NoSQL movement.  <abbr title=""Relational Database Management Systems"">RDBMS</abbr> such as Oracle, SQL Server, et al. are excellent at mapping relational data, but much real-world data is not relational; in fact, the class of problems that RDBMS has grown in number, and is likely to continue to do so.  

The classic customers-plus-orders example that you get in many ""Databases 101""-type courses is not relational, as best exemplified by the OrderLine entity that is always added when normalising data.  Does an order line exist in the real world?  Well, ok, <em>sort of</em> &mdash; there are lines on an invoice, for example &mdash; but it's a trick that needs to be learnt: it's not intuitive.  This is also one of the reasons that denormalisation is sometimes required to juice extra performance from a database.  For some applications, such as Twitter, it makes no sense to enforce the relational model on the data, because the data is network-oriented and only semi-structured.  

When you come to program your system in a good OO language like C#, Java, etc., you quickly encounter the Object-Relational impedance mismatch: your objects don't look like your entities.  For example, your Order object might have some form of collection of products and quantities, and that collection will likely look nothing at all like your OrderLine entity.  You can get around this in a couple of ways: code your objects to look more like your entities (and subvert good OO programming practice in the process), or use an <abbr title=""Object-Relational Mapper"">ORM</abbr> such as NHibernate.  

You've probably heard me mention ORMs and NHibernate before, and they're a great way to solve some problems, but they add a huge amount of complexity in the process.  For example, NHibernate requires you to map your objects to your entities via an XML file, or via C# in the case of Fluent NHibernate.  When I've worked with NHibernate, I've found this to be a massive overhead in the process, <em>particularly</em> for new projects which are still evolving.  

Which brings me neatly on the next point: RDBMS do not cope well with schema evolution.  When you add a new column to a table, existing rows in the table utilise either NULL or the default value for the column (if a default has been specified).  This means the table then has to be updated with information for the new column; if the new column is a foreign key, for example, this can quickly get very laborious.  

Scalability is an interesting problem, particularly as applied to RDBMS.  Vertical scaling &mdash; buying more memory, CPU, etc. &mdash; is very easy, but quite expensive: server hardware does not come at commodity prices.  Horizontal scaling &mdash; splitting the system across separate nodes, either by data or function &mdash; gives greater flexibility, but is much more complex.  Constraints that you might have been able to rely on the RDBMS to enforce now must be incorporated into the application tier so they are consistent across the various nodes.  The two-phase commit technique used in distributed transactions to enforce consistency <em>reduces</em> the overall availability of the system: the figure is the product of the availability of all the individual nodes.  

Instead of aiming for high consistency as RDBMS do, document databases opt for something called eventual consistency.  This concept states that, across all the nodes in the system, at some point in the future the data will be consistent.  As it turns out, this actually the natural order of things in many scenarios: Neil's example was of a system with an important paper step such as filling out a form.  When the form has been completed, the system is inconsistent: the form has not yet been entered into the computerised portion of the system.  It is not until the form has left the agent's briefcase, been passed to the data entry clerk, and entered into the electronic system that the system is consistent overall.  

As it turns out, NoSQL implementations differ in their aims, so it's important to pick the correct one for your situation.  For example, MongoDB's <abbr title=""Unique Selling Point"">USP</abbr> centres around performance: it's blazingly fast.  There is some commonality between the implementations, however: they are all aimed at <em>large</em> (huge, enormous, gigantic) datasets; they all target commodity hardware; they all aim for high availability.  Here are the different classes of implementation:
<dl>
  <dt>Key-value stores</dt><dd>Hadoop, Redis, Voldemort, Dynamo</dd>
  <dt>Network-oriented</dt><dd>Neo4J</dd>
  <dt>Object</dd><dd>db4o</dd>
  <dt>Columnar</dt><dd>Google BigTable, Cassandra</dd>
  <dt>Document</dt><dd>CouchDB, MongoDB, Riak, Terrastore</dd>
</dl>

No doubt other implementations are available.

Map-Reduce is another common feature of many of these kinds of databases.  This, simply put, is parallel processing of your data.  Work is pushed to the nodes (rather than data being pulled to them), which then emit results.  This is the Map part of the operation.  The Reduce part is an aggregation, which filters the results.  <a href=""http://code.google.com/edu/parallel/mapreduce-tutorial.html"">Check out Google's explanation.</a>

<h3>CouchDB</h3>
<a href=""http://couchdb.apache.org/"">CouchDB</a>, a project of the <a href=""http://apache.org/"">Apache Software Foundation</a>, is an example of a document database.  A document is no more strictly-defined than as a ""self-contained thing"".  It can be an invoice; it can be an insurance quotation or policy; it can be an event or a business card.  There are no set fields, and documents can have as many or as few fields as required, and the fields can be of complex types.  Document databases are schemaless, so you could store an invoice, insurance quotation, and a business card in the one store.  

In CouchDB, documents are represented in JSON.  This seems to be well on its way to becoming a standard in this space: MongoDB uses a binary-serialised form of JSON called BSON (I wish they'd called it BiSON).  As such, documents look a little like this:

<blockcode language=""javascript"">
{
    ""key"": ""value"",
    ""key"": {
        ""objectType"": ""value"",
        ""integer"": 4
    },
    [""arrayItem1"", ""arrayItem2""]
}
</blockcode>

CouchDB is very simple, and has a RESTful API for interacting with it.  Neil pointed out that the API is not strictly RESTful, but I thought it a bit of a tangent so I won't detail it here (interesting as it was).  It handles multiple versions using something called <a href=""http://books.couchdb.org/relax/appendix/btrees"">append-only B trees</a>; essentially these are tree data structures to which nodes can only be added.  Each node represents a version of the document, with the leaf node(s) representing the latest version(s).  When necessary, the database can be compacted, which causes the trees to be ""wound up"": that is, the tree is traversed to the leaf node(s), conflicts are resolved, and the tree becomes rooted at the latest version.

Those optional plurals are important: CouchDB is designed for parallel execution and as such is implemented in Erlang, which has excellent concurrency support.  A further consequence of this implementation detail is that it is designed to crash!  This is better than it sounds: when you get a return value back from CouchDB, you are guaranteed that your data has been persisted to disk.  If something goes wrong in one of the CouchDB processes, it simply hard resets: no lost data.

Further good stuff includes attachments (for binary streams) and its unbelievably <em>tiny</em> size.  However, and this may put some people off trying it out over something like MongoDB, ad hoc queries are not supported.  You can only query via defined views (luckily there are quite a few built in), and the command line interface requires <a href=""http://curl.haxx.se/"">curl</a> or a similar library/program that allows you to fire off web requests.  There is a GUI supplied with CouchDB, called Futon (available via your favourite web browser at http://127.0.0.1:5984/_utils), but the following curl examples should really help you understand what's going on under the covers.  Interestingly, Futon is stored in an internal CouchDB database as a design document.  

<blockcode language=""bash"">
$ curl -X GET 127.0.0.1:5984/_all_dbs
</blockcode>

This issues an HTTP GET request to port 5984 running on localhost to retrieve <code>_all_dbs</code>.  A JSON-serialised list of all the databases in CouchDB is returned.

<blockcode language=""bash"">
$ curl -X PUT 127.0.0.1:5984/invoicing
</blockcode>

This creates the invoicing database in CouchDB.  A JSON-serialised status message is returned.  Note the use of the PUT method here.  It's important to understand the difference between PUT and POST; check out <a href=""http://jcalcote.wordpress.com/2008/10/16/put-or-post-the-rest-of-the-story/"">this blog post on the subject</a> and the Wikipedia entry on <a href=""http://en.wikipedia.org/wiki/Idempotence"">Idempotence</a>.  

<blockcode language=""bash"">
$ curl -X POST 127.0.0.1:5984/invoicing -d @'myfile'
</blockcode>

This inserts a record into the invoicing database, using the contents of the myfile file as the document.  Note therefore that myfile must contain JSON-serialised data.  This JSON is transmitted in the body of the POST request.  A JSON-serialised status, id (a GUID) and revision id (rev, made up of a revision number and a GUID) is returned.  

It's important to note here that <strong>old revisions are not stored indefinitely.</strong>  When the compacting operation I mentioned earlier is executed, the old revisions are lost.  CouchDB cannot, therefore, be used as the basis of a version control system (for example) unless you guarantee that the compacting operation never runs.

<blockcode language=""bash"">
$ curl -X GET 127.0.0.1:5984/invoicing/_all_docs
</blockcode>

This returns information about all the documents stored in the invoicing database.  You must append the <code language=""bash"">?include_docs=true</code> query string in order to retrieve the data.  In practice, however, you are highly unlikely to ever need to retrieve information on all the documents: remember, these systems are designed to work with millions or even billions of documents.

<blockcode language=""bash"">
$ curl -X PUT 127.0.0.1:5984/invoicing/<some GUID>?rev=<some rev ID>
</blockcode>

This updates the document in the invoicing database identified by &lt;some GUID&gt; at revision &lt;some rev ID&gt;.  As before, the rev is made up of a revision number and a GUID.  The rev query string is required here because you can only ever update a revision of a document.  In the process, you create a new revision of the document, and the new rev is returned in JSON form along with the other usual stuff.

To retrieve a document, simply use:
<blockcode language=""bash"">
$ curl -X GET 127.0.0.1:5984/invoicing/<some GUID>
</blockcode>

This will retrieve the latest revision of a document.  Presumably you can retrieve a specific revision by using the rev query string as before.  

The next example is quite interesting: using attachments to store binary data with your documents.
<blockcode language=""bash"">
$ curl -X PUT 127.0.0.1:5984/invoicing/<some GUID>/<some attachment name>?rev=<some rev ID> --data-binary @'invoice.pdf' -H ""Content-Type: application/pdf""
</blockcode>

Ok, so it's also more involved.  The &lt;some attachment name&gt; is an arbitrary name used by CouchDB to identify the attachment; it doesn't need to match the filename.  Again, we have to specify the rev as well as we are updating a revision of a document.  The <code language=""bash"">--data-binary</code> bit is an option to curl that indicates that the body of the HTTP request needs to be base64-encoded as it is binary data; the option requires an argument, in this case @'invoice.pdf'.  Likewise, <code language=""bash"">-H</code> is an option, which allows you to specify extra header fields manually.  In this case, we're supplying the MIME-type to indicate that our attachment is a PDF.  

The attachment can be retrieved in a couple of ways.  Appending the query string <code language=""bash"">?attachments=true</code> to a GET request for a document returns the base64-encoded representation of the attachment.  To view the attachment in a more useful format &mdash; in a PDF viewer in this case &mdash; add <code language=""bash"">/<some attachment name></code> to the GET request.

It's taken me WAAAY longer to write this post than I was expecting, so I'm going to wrap up with a couple of quick pointers to other resources.  You can find a <a href=""http://github.com/foretagsplatsen/Divan"">.NET library for CouchDB called Divan on GitHub</a>, and there's an excellent (and <em>free</em>!) online version of the O'Reilly CouchDB book available from http://books.couchdb.org/relax.  Do check out the further reading list below as well; there are some great posts there.  

<hr />

<h3>Further Reading</h3>
Dare Obasanjo, aka Carnage4Life, posts extensively on web topics:
<ul>
  <li><a href=""http://www.25hoursaday.com/weblog/2009/10/29/FacebookSeattleEngineeringRoadShowMikeShroepferOnEngineeringAtScaleAtFacebook.aspx"">Facebook Seattle Engineering Road Show: Mike Shroepfer on Engineering at Scale at Facebook</a>  This is a fascinating post all-round, and describes in some detail what Facebook had to do to get sufficient performance out of MySQL (including heavy use of <a href=""http://www.danga.com/memcached/"">memcached</a>).  They're now heavily investing in their own columnar storage system, called <a href=""http://incubator.apache.org/cassandra/"">Cassandra</a>.</li>
  <li><a href=""http://www.25hoursaday.com/weblog/2010/03/10/BuildingScalableDatabasesAreRelationalDatabasesCompatibleWithLargeScaleWebsites.aspx"">Building Scalable Databases: Are Relational Databases Compatible with Large Scale Websites?</a></li>
</ul>

Rob Conery, ex-Microsoft (working on ASP.NET MVC), has recently jumped on the NoSQL meme with MongoDB.  His blog is a custom engine powered by Mongo.  
<ul>
  <li><a href=""http://blog.wekeroad.com/2010/02/05/reporting-in-nosql"">Reporting In NoSQL</a></li>
  <li><a href=""http://blog.wekeroad.com/2010/02/06/nosql-a-practical-approach-part-1"">NoSQL – A Practical Approach, Part 1</a></li>
</ul>

<a href=""http://highscalability.com/"">The High Scalability Blog</a>.  A sort of best practice-oriented blog aiming to share information on how large sites build for scalability successfully."
"Action! Introducing ReelCritic, a new film blog.",NULL,"I've finally done it: <a href=""http://www.reelcritic.co.uk/"">ReelCritic</a> has launched!  The DNS changes are currently propagating through the Internet, so the link may not work for you straight away; you might want to give it another go in a few hours/tomorrow.
<!--break-->
Here's an excerpt from my <a href=""http://www.reelcritic.co.uk/2010/03/action.html"">introductory post</a>:
<blockquote>
Welcome to ReelCritic, a new film blog!

Here I will be mostly posting film reviews, but you can expect to see the odd opinion piece as well. Maybe even some news, who knows? This place is so new, I'm still working out what to do with it.

Some of you may have found this little blog from my software programming blog, CodeBork. You'll be pleased to hear that I have copied over the film-related posts from there to here, so you can find everything in one place. Sadly the comments have not be copied over, but this isn't a huge loss considering how few of them there are.
</blockquote>

As mentioned in <a href=""http://www.codebork.com/coding/2010/02/10/reelcritic-new-film-blog.html"">my earlier post on this new site</a>, the rationale for this is to give this blog more of a focus: previously the coding posts and the film reviews were jostling for space and my poor blog was suffering from something of an identity crisis.  So, if you found my film reviews and posts interesting but are not interested in the technical posts, you can now subscribe to ReelCritic instead!  As a result of all this, you will notice that I have removed the film-specific feed from the front page, and I will no longer be posting film reviews to this blog.  The film-specific feed has also been removed, so if you're subscribed to that you will probably want update your feed subscription to point to <a href=""http://www.reelcritic.co.uk/rss.xml"">the ReelCritic feed</a> instead.

As always, please feel free to leave me feedback in the comments below.  I hope you enjoy the new offering from <a href=""http://www.reelcritic.co.uk/"">ReelCritic</a>!"
Code Complete: High-Quality Routines (Part 1 - Introduction and Design Considerations),NULL,"Last time, <a href=""http://www.codebork.com/coding/2009/11/04/code-complete-reasons-create-class.html"" title=""Code Complete: Working Classes (Part 4 - Reasons to Create a Class)"">I rounded off the series-within-a-series on class design and usage, Working Classes</a>.  The next topic for dissection is routine design, creation and usage, and this topic will be handled over two posts.  This post will form a bit of an introduction and cover design considerations for high-quality routines through the classification of different kinds of routine cohesion.  So, without further ado, let's get cracking!
<!--break-->
A good place to start is to define what a routine is, and McConnell describes it as <strong>an individual method or procedure invokable for a single purpose.</strong>  Note that implicit in this definition is an assumption that the <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html#SRP"" title=""SOLID Principles of OOD"">Single Responsibility Principle</a> applies to methods as well as classes.  

McConnell then provides an example of a low quality routine, and suggests to the reader that they pick it apart and find as many errors in it as they can.  I've included my opinions and then McConnell's answers below; don't scroll too far if you want to try this exercise for yourself.  The routine is as follows (as printed in <em>Code Complete</em>, 2nd Edition):

<blockcode language=""cplusplus"">
void HandleStuff( COPY_DATA & inputRec, int crntQtr, EMP_DATA empRec,
    double & estimRevenue, double ytdRevenue, int screenX, int screenY,
    COLOR_TYPE & newColor, COLOR_TYPE & prevColor, StatusType & status,
    int expenseType)
{
int i;
for ( i = 0; i< 100; i++ ) {
    inputRec.revenue[i] = 0;
    inputRec.expense[i] = corpExpense[ crntQtr][ i ];
    }
UpdateCorpDatabase( empRec );
estimRevenue = ytdRevenue * 4.0 / (double) crntQtr;
newColor = prevColor;
status = SUCCESS;
if ( expenseType == 1 ) {
    for ( i == 0; i < 12; i++ )
        profit[i] = revenue[i] - expense.type1[i];
    }
else if ( expenseType == 2 ) {
        profit[i] = revenue[i] - expense.type2[i];
        }
else if ( expenseType == 3 ) {
        profit[i] = revenue[i] - expense.type3[i];
        }
</blockcode>

I picked out the following things as being bad practice or otherwise wrong:
<ul>
  <li><strong>Bad method name:</strong> <code>HandleStuff</code> is a very vague name that gives no hints as to what the routine is supposed to achieve.</li>
  <li><strong>No comments:</strong> There is no routine comment describing the purpose of the routine, its parameters and return value, nor are there any comments in the routine's implementation to describe trickier or non-obvious corners of the code.</li>
  <li><strong>Too many parameters:</strong>  This is a matter of taste, but it suggests to me that there may be scope to create an object to wrap the parameters, provided they are sufficiently closely-related.</li>
  <li><strong>Parameters are unrelated:</strong> This causes confusion over what the routine might be doing.  Indicates that the routine has more than one responsibility.  </li>
  <li><strong>Braces inconsistent and confusing:</strong> Along with the poor indentation, this is a major barrier to the routine's readability and therefore its comprehensibility.  </li>
  <li><strong>Reuse of loop variable i in different places in the routine:</strong>  This is particularly bad, as we will see when we get onto the chapter on variables.  Not only does i have little meaning outside of a loop, but also the routine becomes reliant on side-effects; i.e., that the loops complete at the expected iteration, that the variable is in the right state before the next loop begins, etc. </li>
  <li><strong>Has more than one responsibility:</strong> This routine updates a database, calculates estimated revenue, and sets a colour of some sort.  It therefore has no focus and sprawls.</li>
  <li><strong>Outputs data via parameters rather than returning value:</strong> This isn't bad practice in and of itself, but here it doesn't make a lot of sense, and is another symptom that the routine is trying to do too much.  At least one of the data items could be returned (perhaps the status)</li>
  <li><strong>Status is set to SUCCESS before the second half of the function has completed:</strong> Does status refer to a particular action in the first half of the routine, or the routine as a whole?  If the latter, there's still plenty of time for something to go wrong; if the former, it's poorly-named.</li>
  <li><strong>Using magic numbers in if statement:</strong> What is expenseType 1?  You might know when you wrote this code, but you may not three months down the line, and a new developer on your team most certainly won't.</li>
  <li><strong>using if statement over switch:</strong> If the language supports <code>switch</code>ing on an int, then this provides a more readable alternative to sequence of <code>if..else if</code></li>
  <li><strong>No checking of crntQtr value for valid data:</strong>  What happens if crntQrtr is 0?  Or 3.14159?  Or 14?</li>
</ul>
 
Here's McConnell's list of answers:
<ul>
  <li>Bad name</li>
  <li>No comments</li>
  <li>Bad layout.  Different layout strategies are mixed and matched.</li>
  <li>Input variable, inputRec, is changed (or it should be renamed so it's not classed as an input variable)</li>
  <li>Reads and writes global data (profit, crntQtr)</li>
  <li>No single purpose</li>
  <li>No defence against bad data</li>
  <li>Use of magic numbers</li>
  <li>Some parameters are unused (screenX and screenY)</li>
  <li>prevColour is passed as a reference parameter even though it isn't assigned a value</li>
  <li>Too many parameters (upper limit should be 7 for an understandable method), and they're laid out in an unreadable way</li>
  <li>Parameters are poorly ordered and undocumented.</li>
</ul>

So, with that in mind, what are the valid reasons to create a routine?  Well, as we saw last time with creating classes, <strong>the single most important reason to create a routine is to reduce complexity.</strong>  Again, this is an effective technique for hiding information so you don't have to juggle that along with the rest of the routine.  The deep nesting of an inner loop or conditional is a prime target for the Extract Method refactoring.  

Routines are a great tool for managing duplication, and indeed avoiding it in the first place.  If reducing complexity is the most important reason to create a routine, avoiding duplicate code is the most popular.  If you find yourself creating similar code in different routines, then you likely have an error in your decomposition: at this point, you can pull out the duplicate code, place a more generic version in a common base class, and move the specialised routines into subclasses; alternatively, you can move the common code into its own routine and let both methods call the new routine.  Furthermore, this saves space used by the duplicated code, making modifications easier because you only have to update one place, and making your code more reliable as there is only one place to check to ensure the code is correct.  

[img_assist|nid=139|title=|desc=Photo by <a href=""http://www.flickr.com/photos/jcse/"" title=""José Encarnação on Flickr"">José Encarnação</a>|link=none|align=center|width=400|height=300]

You can create new routines to support subclassing, as well.  Less code is needed to override a short, well-factored routine than a long and sprawling routine, because the original routine is clearly-defined and well-understood.  As a result, simple overridable routines can help reduce the chance of error in the subclass implementations.  

In any program, it's sensible to hide the order in which events happen to be processed from the wider world, because it shouldn't matter beyond the scope of the routine.  In a blog application, for example, you don't really want the code dealing with saving data to know that a connection to the database must be set up and opened, and an SQL statement prepared and populated with data, before the data can actually be saved to the database.  Instead, you can hide all this away in your <code>Save()</code> method, and ideally it would be factored out into separate routines on a dedicated class such as a <a href=""http://www.codebork.com/coding/2009/01/24/mocking-databases.html"">Repository</a>.  

If languages like C++ are your thing, routines can be used effectively to manage the pointer operations integral to these languages.  This technique is less appropriate to modern languages that abstract the pointer operations away from the developer's eyes, but it allows the developer to concentrate on the intent of the operation rather than its mechanics.  A similar argument holds for managing the portability of your system: using routines allows you isolate the non-portable capabilities of your system, such as database-specific functionality, from the rest of your system.  Such issues may not be applicable to your system, however.  

One of my personal favourite reasons to create a new routine is to simplify a complicated boolean test.  Understanding all the ins and outs of complicated boolean conditions is rarely necessary, and inlining them in your code presents a barrier to readability.  It's much easier to grok a complicated condition if it's wrapped in a method with a good, descriptive name: the finer details of the test are abstracted away and summarised nicely.  Furthermore, this emphasises the test's significance, encouraging your to expend extra effort to make the detail of the test clear and readable inside the function.  This is a technique I've used at work a few times recently, in both my own code and in code reviews, and it seems to be paying off well.  

Finally, you can introduce new routines to improve performance.  This is because it is easier to optimise the code in one place instead of several, and makes your code easier to profile.  

<strong>You should never introduce a new routine to ensure all routines are small.</strong>  This is a bit of an anti-pattern, and leads to arbitrary divisions of labour.  This is then usually compounded by the fact that such routines are difficult to name.  You very quickly end up with a mess.  
 
The biggest mental block to creating effective routines is a reluctance to create a simple routine for a simple purpose.  However, these make code more self-documenting.  Small routines can turn into larger routines, once newly-discovered errors are accounted for. Furthermore, many of the reasons to create a class are also good reasons to create a routine:
<ul>
  <li>Isolate complexity</li>
  <li>Hide implementation details</li>
  <li>Limit effects of change</li>
  <li>Hide global data</li>
  <li>Make central points of control</li>
  <li>Facilitate reusable code</li>
  <li>Accomplish a specific refactoring</li>
</ul>
 
<h3>Routine Cohesion</h3>
Cohesion, <a href=""http://portal.acm.org/citation.cfm?id=1661066.1661068"" title=""Structured Design, IBM Systems Journal, Volume 13, Issue 2"">introduced</a> by Wayne Stevens, Glenford Myers and Larry Constantine in 1974, is the workhorse design heuristic at the routine level.  Cohesion refers to how closely the operations within a routine are related, and can also be referred to as ""strength"".  The goal is to achieve maximum cohesion: each routine does one thing well and nothing else; this is known as functional cohesion.  The following forms of cohesion are considered poor, and prime for refactoring.  

[img_assist|nid=140|title=Cohesion|desc=Photo by <a href=""http://www.flickr.com/photos/nogood/"" title=""Yannig Van de Wouwer on Flickr"">Yannig Van de Wouwer</a>|link=none|url=http://www.flickr.com/photos/nogood/211866952/|align=center|width=399|height=266]

<strong>Sequential cohesion</strong> exists when a routine contains operations that must be performed in a specific order, sharing data from step to step, and don't make up a complete function when done together.
For example, in a routine that calculates age and time until retirement from a birth date, if the time to retirement is calculated from the result of the age calculation, the routine is considered sequentially cohesive. 
To make it functionally cohesive, create separate routines where each takes the birth date as a parameter.  Time-to-retirement could call the Age routine and each would still have functional cohesion.

<strong>Communicational cohesion</strong> is defined by operations in a routine that use the same data, but are otherwise unrelated.  For example, a given summary-printing routine also reinitialises the summary data; the operations can be split into separate routines, with the summary reinitialisation routine being called from the location where the summary is created in the first place. 

<strong>Temporal cohesion</strong> occurs when operations are combined into a routine because they're all executed together. <code>Startup()</code> and <code>Shutdown()</code> methods are prime examples of temporal cohesion.  Functional cohesion can be achieved by factoring out the individual jobs of the routine to separate routines and calling them from the original routine.
 
The following kinds of cohesion are generally unacceptable, and it is considered better to put in the effort to re-write them rather than put up with code that is poorly organised, hard to debug and hard to modify.

<strong>Procedural cohesion</strong> occurs when operations are done in a specified order (but don't necessarily share data).  A set of operations is organised into a specified order and the operations don't need to be combined for any other reason; a routine that gets an employee name, then an address, then a phone number would be considered procedurally cohesive.  To make the routine functionally cohesive, put the separate operations into their own routines, and make sure the calling routine has a single complete job (e.g., GetEmployee()).

<strong>Logical cohesion</strong> occurs when several operations are placed into a single routine, and one of the operations is selected by a control flag passed in as a parameter; i.e., only the control flow ties the operations together: they're otherwise separated out in a big if or switch statement together and the operations themselves are unrelated.  For example, a method called InputAll() can input customer names, timesheet info, or inventory data, depending on the flag passed in.  This can be improved by separating the operations into their own routines; if there is shared code between the operations, this is moved into a helper method and the operations are packaged into a class.  There is an exception, however: a routine containing nothing but a series of if or switch statements and calls to other routines.  McConnell terms this an ""event handler"", which is potentially a confusing term depending on your background.

<strong>Coincidental cohesion</strong> occurs when operations in a routine have no discernible relationship, and is also known as ""no cohesion"" or ""chaotic cohesion"".  Unfortunately this is not easily improved upon, and if you find yourself faced with a coincidentally cohesive routine, the only practical approach is to give it a deeper redesign and reimplementation.  

<h3>Conclusion</h3>
This post provided an introduction to key considerations for routines, including when to create new routines, when <em>not</em> to create new routines, and the concept of routine cohesion.  Part two will cover naming conventions for routines, and the use of routine parameters."
Code Complete: High-Quality Routines (Part 2 - Naming and Parameter Usage),NULL,"This second and final post on high-quality routines will cover good routine names and guidelines for parameter usage, as well as touching on routine length and special considerations for functions vs procedures.
<!--break-->
There are a number of things a routine name must achieve in order to be considered a good name.  First and foremost, <strong>they should describe everything the routine does, including all outputs and side effects.</strong>  This can lead to long and ""silly"" routine names, such as <code>ComputeReportTotalsAndOpenOutputFile()</code>, so the key here is to avoid programming with side effects rather than skimping on the routine name.  Furthermore, meaningless, vague, or wishy-washy verbs should be avoided, such as <code>DoWork()</code>, <code>HandleCalculation()</code>, <code>Process()</code>, etc.  ""Handle"" should only be used in the context of handling an event.  It can be the case that only the name is bad and routine itself is well-designed, but in most cases, the vague verb is a symptom of the vague purpose of the method. If this is the case, restructure the routine and its related routines so they have stronger purposes, and stronger names to boot.

Hopefully this point is self-evident, but don't differentiate routine names solely by number.  <code>ComputeReportTotals1()</code> and <code>ComputeReportTotals2()</code> are bad routine names: what's the difference between them?  Do they compute different totals, or operate on different reports?  Maybe they take different inputs?  This routine naming technique ruins what might otherwise be a good routine name by introducing ambiguity into the name, and uncertainty and doubt into its meaning.  

[img_assist|nid=142|title=|desc=|link=node|align=center|width=400|height=447]

Make the name as clear as possible, and as a logical follow-on from this point, make your routine names as long as necessary.  Whilst the optimum <em>average</em> length for a variable name is 9-15 characters, good routine names tend to be longer.  This can be bolstered by using a description of the return value for functions, such as <code>cos()</code>, <code>customerId.Next()</code>, <code>printer.IsReady()</code>, or <code>pen.CurrentColour()</code>; or by using a strong verb followed by an object for a procedure, such as <code>PrintDocument()</code>, <code>CalculateMonthlyRevenues()</code>, <code>CheckOrderInfo()</code>, <code>RepaginateDocument()</code>.  In OO languages, the object isn't strictly necessary, as the object itself is usually included in the call (<code>document.Print()</code>, <code>orderInfo.Check()</code>).  Names like document.PrintDocument() are redundant, and lose accuracy when carried through to derived classes.

Do use opposites precisely, and use naming conventions for opposites to aid consistency.  Common opposites include:
<table>
  <tr style=""border-bottom: 1px solid silver;"">
    <td style=""border-right: 1px solid silver; padding: 5px;"">Add/Remove</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Increment/Decrement</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Open/Close</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Begin/End</td>
    <td style=""padding: 5px;"">Insert/Delete</td>
  </tr>
  <tr style=""border-bottom: 1px solid silver;"">
    <td style=""border-right: 1px solid silver; padding: 5px;"">Show/Hide</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Create/Destroy</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Lock/Unlock</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Source/Target</td>
    <td style=""padding: 5px;"">First/Last</td>
  </tr>
  <tr style=""border-bottom: 1px solid silver;"">
    <td style=""border-right: 1px solid silver; padding: 5px;"">Min/Max</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Start/Stop</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Get/Put</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Next/Previous</td>
    <td style=""padding: 5px;"">Up/Down</td>
  </tr>
  <tr>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Get/Set</td>
    <td style=""border-right: 1px solid silver; padding: 5px;"">Old/New</td>
    <td style=""border-right: 1px solid silver; padding: 5px; background-color: #f8f8f8;"">&nbsp;</td>
    <td style=""border-right: 1px solid silver; padding: 5px; background-color: #f8f8f8;"">&nbsp;</td>
    <td style=""padding: 5px; background-color: #f8f8f8;"">&nbsp;</td>
  </tr>
</table>
 
Establish your own conventions for common operations.  Naming conventions are the easiest way to distinguish among different kinds of operations.  The following examples are all valid ways of retrieving an object's identifier &mdash; <code>employee.id.Get()</code>, <code>candidate.id()</code> or <code>dependent.GetId()</code> &mdash; but they should be standardised for your program.  
 
Regarding routine length, some research has been carried out into the optimum length for maintainability, comprehensibility, etc., but the results have so far proved inconclusive.  A significant percentage of object-oriented code is accessor routines, which are generally very short (often just one line), and generally should be kept as simple as possible, with extra processing handled in other routines.  Complex algorithms lead to longer routines, and these should be allowed to grow organically up to 100-200 lines: experience shows that routines of this size are no more error-prone than short routines.  Be careful with routines larger than ~200 lines, as they are harder to understand, and therefore maintain.  Issues such as <a href=""http://www.codebork.com/coding/2010/04/29/code-complete-high-quality-routines-part-1-introduction-and-design-considerations."" title=""Code Complete: High-Quality Routines (Part 1 - Introduction and Design Considerations)"">cohesion</a>, depth of nesting, the number of variables used, the number of decision points, and the number of comments needed to explain the routine, and other complexity-based considerations should dictate the length of the routine. 

[img_assist|nid=143|title=Enough with the names already|desc=Photo by <a href=""http://www.flickr.com/photos/jamescridland/"" title=""James Cridland's Flickr Photostream"">James Cridland</a>|link=none|align=center|width=400|height=219]

<h3>Routine Parameters</h3>
Interfaces between routines are very error-prone, because the syntax rules governing parameter lists are few in number and generally weak in force in comparison with classes.  The guidelines outlined in this section should help you minimise problems caused by routine interfaces.  

Putting parameters in input-modify-output order brings some semblance of order to what otherwise might be a somewhat random jumble of mostly-related data items: it implies the sequence of operations within the routine.  Alphabetical ordering &mdash; and random ordering in particular &mdash; are detrimental to understanding the routine from its interface, as the order makes no sense in the problem domain.  Furthermore, you should consider using different naming conventions for input, modify and output parameters to highlight the differences between them.  Status or error variables should always be placed last in the list.  As these are output parameters by definition, they should come at the end of the parameter list anyway, but they should be the very last items in the list, in the same way that <a href=""http://www.codebork.com/coding/2010/04/29/code-complete-high-quality-routines-part-1-introduction-and-design-considerations."" title=""Code Complete: High-Quality Routines (Part 1 - Introduction and Design Considerations)"">they should be set last in the routine</a>.  If several routines use similar parameters, always order those parameters consistently, as this aids remembering how to use the routines, and be sure to <strong>use all the parameters</strong>!  If you're not using a parameter in a routine, remove it.

Consider creating your own in and out keywords; this helps highlight which parameters have which role (input/modify/output).  C# has <code>out</code> (output only) and <code>ref</code> (input-output) keywords that fulfil this requirement (<code>ref</code> parameters must be initialised before passing in; <code>out</code> parameters cannot be; input parameters are implicit and the default), but Java doesn't support this at all: all parameters are pass-by-value.  Furthermore, Java doesn't support macros and preprocessing like C and C++ do, so you don't really have the option to add them in.  They <em>can</em> be added in C and C++, but the effect is only really to improve the documentation of the routine, as you still need to pass pointers and reference parameters.  There are some drawbacks to this approach: it extends the language in a way unfamiliar to most users (so you need to be disciplined in using your custom extensions consistently), nor is it enforceable by the compiler which could lull developers into a false sense of security when reading your code.

Don't use routine parameters as working variables, use local working variables instead.  This allows you to use the parameter value again later in the routine should you need to.

[img_assist|nid=144|title=Selfridge's Parametric Bridge|desc=Photo by <a href=""http://www.flickr.com/photos/ell-r-brown/"" title=""Ell Brown's Photo Stream on Flickr"">Ell Brown</a>|link=url|url=http://www.flickr.com/photos/ell-r-brown/3846029441/|align=center|width=400|height=300]

Do document your interface assumptions about the routine's parameters, both in the routine and the location from where the routine is called.  Even better than documenting the assumptions, if your language supports it, is to use assertions to enforce the assumptions.  Some examples of assumptions that should be documented are whether parameters are input-only, modified, or output-only; the units of numeric parameters (feet, inches, kilograms, etc.); the meanings of status codes and error values (if enums aren't used); the ranges of expected values; and specific values that should never occur.  

Try to limit the number of parameters to a routine to approximately seven.  This is a direct result of <a href=""http://en.wikipedia.org/wiki/The_Magical_Number_Seven,_Plus_or_Minus_Two"" title=""The Magical Number Seven, Plus or Minus Two (Wikipedia)"">the 7 ± 2 rule</a> proposed by George Miller in 1956, which theorises that the average capacity of the human brain's short-term memory is seven items, plus or minus two.  In modern and object-oriented languages, you can pass many items into a routine using a complex data type such as a class or a struct.  If you find yourself consistently passing the same set of arguments, however, the coupling between the two routines is too high.  The routines should be grouped into a class, with the frequently-used data promoted to class data, such as fields or properties.  

You should pass the variables or objects that the routine needs to maintain its interface abstraction.  Generally, there are two schools of thought on how much of an object to pass through.  The first school says that only the specific object members needed by the routine should be passed in, whilst the second school says that the whole object should be passed in for greater flexibility in future.  McConnell believes that, in reality, both are too simplistic, and instead you should pass in the data that matches the routine's interface abstraction.  So, for example, if the routine expects three specific data elements, and the data are coincidentally provided by a single object, and you happen to have the whole object to hand, you should pass that in.  If you have to create the object prior to calling the routine, the data should be passed into the routine as separate items: a routine that requires ""setup"" or ""teardown"" generally needs re-designing.  If you find you're frequently changing the parameter list to add new object members, pass in the object.

Ensure that the actual parameters passed into the routine match the formal parameters declared on the routine: a common mistake is to pass (for example) an int when a float is requested.  I'm not sure how much I agree with McConnell's recommendation on this point: strongly-typed languages provide more than a modicum of protection against most type-based mistakes, and no information is lost when an int is implicitly converted to a float.  Furthermore, whilst this advice applies to input-only parameters, it can become difficult to apply to parameters that are used for both input and output.

The final recommendation on parameter usage is to use named parameters, which allow formal parameters to be associated with actual parameters.  Some languages have had these for a while, such as Visual Basic, Objective-C and Python, and <a href=""http://msdn.microsoft.com/en-us/library/dd264739.aspx"" title=""Named and Optional Arguments (C# Programming Guide)"">C# has just recently had this capability added with the version 4 language specification</a>.  The benefit of named parameters is that it clears up any ambiguity over which actual parameter binds to which formal parameter: the relations between the two sets are laid out for all to see in the routine call.  

<h3>Functions vs Procedures</h3>
I most modern languages no distinction is drawn between functions and procedures at a syntactic level.  Semantically, a function returns a value whilst a procedure does not; a procedure in Java or C#, then, is a method with a <code>void</code> return type.  Two exceptions that prove this rule are VB and Delphi: both languages draw a syntactic distinction between functions and procedures.  

You should use a function if the primary purpose of the routine is to return the value indicated by the function name.  Returning a status result should ideally be done via an output parameter, not least because it promotes the better (but more verbose) programming practice of separating the routine call and the test on the status result; that is, it avoids statements of the form <code>if (Save(data))</code>.

<h3>Conclusion</h3>
In this post, we looked at considerations for naming routines, and guidelines for using parameters.  We also drew a (small) distinction between functions and procedures, and looked at some factors affecting routine length.  That rounds up another sub-section in this series, and with it, another chapter of <em>Code Complete</em>.  

Chapter 8 covers defensive programming techniques such as exceptions and other error-handling techniques, assertions, and protecting your program from invalid inputs; and debugging aids.  This will likely form another mini-series like the ""Working Classes"" series as there is a lot of information and advice to cover from this chapter."
PHP Web Deployment Using Git,NULL,"I've recently been working on a new version of my personal website, which can be found over at http://www.alastairsmith.me.uk/.  I wanted a light-weight site that would pull in data from my various presences across the web including my two blogs, GitHub, and Google Picasa; furthermore, as much as possible should be hosted on some form of Content-Distribution Network (CDN).  Google's CDN is serving up jQuery and jQuery UI quite nicely for me whilst minimising the amount of stuff I need to administer and maintain myself.  

It's generally a good idea when developing web applications to keep your in-development version separate from your production version; indeed this goes for many development scenarios, including in-house application testing before rolling out to users, desktop software development, and others.  The backbone of such a strategy is a good source control system, and whilst centralised source control like Subversion can handle this capably, a certain amount of friction is involved in setting it up.  Distributed version control systems, and Git in particular, can remove this friction.

Because my personal website is so small by design, the friction of setting up a centralised source control system for deployment scenarios for such a tiny application would have been great enough to put me off looking into it any further.  Luckily, I switched to Git as my version control tool of choice about a year ago, and haven't really looked back.  Here's how I accomplished a simple web deployment mechanism that can work for sites with a back-end component as well as a smart front-end.
<!--break-->
After a bit of Googling, I came across <a href=""http://stackoverflow.com/questions/279169/deploy-php-using-git/327315#327315"" title=""Deploy PHP using Git"">this Stack Overflow answer</a> which accomplishes everything I need in a very simple and frictionless manner.  It requires minimal set-up, and, once complete, deployment is as simple as running <code>git push production</code>.  <strong>How neat is that?</strong>

In essence, the solution is to check out your web application from your Git repository to your production web server, create a post-update hook that will update the web server's working copy after a push operation, and create a new remote configuration entry in your development copy.  You can then hack away in your dev copy, run all the necessary tests, and push it up to the production server when you're satisfied with it.  The post-update hook is the trickiest bit of the process, and even that has been completed for you and is available <a href=""https://git.wiki.kernel.org/index.php/GitFaq#Why_won.27t_I_see_changes_in_the_remote_repo_after_.22git_push.22.3F"" title=""Git post-update hook script to update a working copy after a push"">from the Git wiki</a>.

There is a further step alluded to in the comments on that answer, which is to ensure you lock down your .git directory on your production web server.  Exposing that directory and its contents can provide the entire source code for your application, which is likely not what you want, particularly from a security perspective.  I'd forgotten most of my Apache-configuration-fu so it took me a while to work out the correct .htaccess directives, but it would appear to be relatively simple: create an .htaccess file <strong>in your .git directory</strong> and enter the following code:

<blockcode>
deny from all
</blockcode>

You will also likely want to add the following code to an .htaccess <strong>in your application's root directory</strong> (i.e., the parent of the .git directory):

<blockcode>
Options -Indexes
</blockcode>

Similar access controls will need to be set in other web servers such as IIS; non-Apache configuration is left as an exercise for the reader.

The only down-side to this solution (which, realistically, is unavoidable) is that the new remote configuration needs to be specified on each working-copy you wish to develop in, but that is a quick and easy thing to define.  It may also cause you to forget to push to your origin repository regularly, as you push to production instead, but this isn't anything a little discipline can't solve."
New Laptop: Why I Went for a Mac (Again),NULL,"A couple of weeks ago, I successfully sold one of my old bassoons that I no longer play, which provided me with a tidy sum to make some purchases I'd been after for a while.  Top of the list was a new laptop, and after some searching around, and due consideration, I decided a MacBook Pro was the model for me.
<!--break-->
I already had an iBook G4 that I bought five years ago whilst at University, and it served me well during that time.  It coped - just - with Eclipse and I was happy dev-ing Java and PHP on that machine for my dissertation and fun for a few years.  I love OS X as an operating system: its focus on simplicity and usability produce results that are  second to none and its reputation is well-deserved.  The build quality of the machines is always superb: the fact that my iBook is still mostly usable five years later, in spite of its obsolete PowerPC processor and twentieth-century complement of RAM, is a testament to that.  Apple's focus on product design is legendary, and they get so many of the little things right as well as the big things: the mag-safe power cord is a great example of one of the little things, and the multi-touch trackpad is an inspired example of one of the big things.

I looked at netbooks and the vastly cheaper price did appeal to me: I have friends running Atom-based PCs with Vista or Windows 7 (Starter edition only), and both are happy with them.  However, as portable as netbooks are, the smaller screen put me off, particularly given that in this new widescreen world we have traded vertical space for horizontal.  This also put me off the 13"" MacBook Pro, which, whilst being about as portable as a netbook with a whole stable of extra horses under the hood, still didn't have sufficient screen real estate for my needs.  Furthermore, the build quality didn't seem as good, even on the better brands such as HP: there's a lot of plastic in these models.  

I settled on a 15"" MacBook Pro, for two reasons: screen size and sheer power.  The current 13"" MacBook Pro runs a Core 2 Duo at 2.5GHz, plus or minus .2 depending on your budget; the 15"" models run the new Nehalem chips under the guise of the i5 or, if you have serious cash, the i7.  The Core 2 Duo is an older architecture, though: as great as it was when it was released three years ago, Intel's new Nehalem architecture is seriously impressive, and <a href=""http://www.tested.com/news/apple-macbook-pro-15-inch-vs-13-inch-is-core-i5-worth-it/153/"" title=""Apple MacBook Pro 15-inch vs 13-inch: Is Core i5 Worth It?"">this post by tested.com</a> claimed a performance improvement of ~24% in the 15"" over the current 13"".  With much of my development work now done in Visual Studio 2010, my intention was always to run VS in Parallels desktop, and so the extra power would go a long way.  

The 15"" model also comes with Apple's proprietary GPU-switching technology that pair's Intel's integrated graphics chipset with an NVidia mobile chipset to give extra performance when you need it.  Having just remembered that Valve have recently released a Mac version of Steam, along with a number of big titles like Half-Life 2 and Portal, this turns out to be a great asset.  I don't think I'm going to have any free time now!

[img_assist|nid=147|title=MacBook Pro 15""|desc=|link=none|align=center|width=399|height=220]

The 15"" model can be fitted with one of three different screens: the standard glossy 1440x900 screen, or a high-res 1680x1050 screen in either glossy or matte anti-glare style.  I opted for the standard screen: the higher resolution would have been nice, but isn't essential; the standard resolution offers plenty of screen real estate.  The glossy screen is great for working on: the colours look amazing, really vivid and saturated.  The image is sharper than I thought possible.  As a result it also excels at movie playback, although the reflections from the glossy finish can be a little distracting in darker scenes.  The matte screen (observed on a 17"" MacBook Pro in-store) looked slightly washed out in comparison, but there were no reflections at all.  

The multi-touch trackpad is an inspired design choice, making a sometimes annoying utility an integral and essential part of the operating system experience.  The entire trackpad is the one button Macs are so famous for, meaning that I can click anywhere at all (although it is easiest at the bottom of the trackpad).  Gestures are supported in addition to the usual drag-a-finger-across-the-trackpad-to-move-the-cursor operation: I can scroll in windows vertically <em>and horizontally</em> using two fingers on the trackpad; a three-fingered swipe left or right moves backwards or forwards in the application respectively (e.g., the previous page visited in a web browser; the next page in a document; the next track in iTunes).  Pinch-to-zoom, familiar from the iPhone is supported in any application that has a zoom feature (Photoshop; iPhoto; Word/Pages; Safari/Firefox; etc.).  Two-fingered rotate is supported in photo-editing applications, etc. (i.e., where the application has a rotate function).  

OS X has advanced quite a lot since I last upgraded my iBook, which shipped with Panther.  I bought the upgrade to Tiger a year or so after purchase for no other reason than I needed a newer version of Java for my dissertation (thank you, Apple.) and the big feature of this release was Spotlight.  The iBook was never really powerful enough to make this a useful feature: both indexing and search were too slow.  Leopard came along in 2007 with new features like <a href=""http://en.wikipedia.org/wiki/Spaces_(software)"" title=""Mac OS X Spaces"">Spaces</a>, <a href=""http://en.wikipedia.org/wiki/Stacks_(software)"" title=""Mac OS X Stacks"">Stacks</a> and a new <a href=""http://en.wikipedia.org/wiki/Dock_(Mac_OS_X)"" title=""Mac OS X Dock"">Dock</a>.  Snow Leopard is primarily a performance release, with no major UI improvements or innovations.  

I think it is, however, Parallels Desktop, with which I am most impressed.  Parallels Desktop is a type 2 hypervisor which means that it runs virtual machines on top of a host operating system, in this case Mac OS X.  This has some drawbacks in terms of performance and security: the hypervisor has to abstract the virtual machines over the host operating system, adding an extra layer of processing; furthermore, if the host operating system is compromised by malware or a direct attack, the virtual machines are potentially compromised also.  Type 1 ""bare metal"" hypervisors that run in place of an operating system do not suffer these drawbacks: the virtual machines are abstracted directly over the hardware (offering better performance) and the virtual machines tend to be isolated in all senses from the hypervisor meaning that they cannot compromise the it and the hypervisor can be ringfenced from the big bad world.

[img_assist|nid=146|title=VS2010 on OS X|desc=So wrong it's right.|link=popup|align=center|width=400|height=250]

Parallels suffers the potential security problems of type-2 hypervisors (although the operating system difference between host and guest may help shield the guest from these problems), and by rights should suffer the performance hit as well, but I am utterly blown away by Parallels' performance on this machine.  Visual Studio, IE8 and PowerShell all run at speeds <strong>indistinguishable from native execution.</strong>  Similarly to Windows 7's XP Mode, Parallels' ""Coherence"" display view seamlessly integrates the applications in the virtual machine into the host operating system so that the experience is unified.  I can launch Visual Studio from the Dock or the Finder, and it runs directly on my desktop and not inside another window.  Closing the applications can be accomplished in the usual manner, or the virtual machine can be suspended, allowing you to start where you left off when you next open the application."
Event Notes,"Last night I attended <a href=""http://www.nxtgenug.net/"">NxtGenUG</a> Cambridge's September event, featuring Guy Smith-Ferrier speaking on Multi-Touch support in Windows 7 and WPF.  This was an excellent speech (as I've come to expect from NxtGen over the last year or so), and I've <a href=""http://www.evernote.com/pub/alastairsmith/publicnotebook"">posted</a> in a public <a href=""http://www.evernote.com/"">Evernote</a> notebook my notes from both Guy's talk and the nugget beforehand which provided an introduction to Kanban and Lean development.   
","Last night I attended <a href=""http://www.nxtgenug.net/"">NxtGenUG</a> Cambridge's September event, featuring Guy Smith-Ferrier speaking on Multi-Touch support in Windows 7 and WPF.  This was an excellent speech (as I've come to expect from NxtGen over the last year or so), and I've <a href=""http://www.evernote.com/pub/alastairsmith/publicnotebook"">posted</a> in a public <a href=""http://www.evernote.com/"">Evernote</a> notebook my notes from both Guy's talk and the nugget beforehand which provided an introduction to Kanban and Lean development.   

Also available there are my notes from last month's NxtGenUG Cambridge event, on cleaning up ASP.NET WebForms applications (less detailed), and from August's Guathon in London, which mostly covers tips and tricks for using Visual Studio 2010, including some neat tricks to make it play more nicely with <abbr title=""Test-Driven Development"">TTD</abbr>.  "
Plan 28: Building the Analytical Engine,"John Graham-Cumming, a prominent British programmer, has kickstarted a <a href=""http://plan28.org/"" title=""Plan 28: Building Charles Babbage's Analytical Engine"">plan to build Charles Babbage's Analytical Engine</a>.  John has put out a request for funding for the project: it is a truly mammoth undertaking and will not happen easily or cheaply.  John has estimated that it will take £500,000 to complete.  He is asking for only £10 (or dollars, or euros, or whatever your local currency is) from each donor.  Please <a href=""http://www.pledgebank.com/babbage"" title=""Pledge a contribution to Plan 28"">pledge</a> a tenner of your own and help get this vitally important and worthwhile project underway.  <a href=""http://www.codebork.com/science-and-technology/2010/10/20/plan-28-building-analytical-engine.html"">Read more about Babbage's Analytical Engine.</a>
","
<a href=""http://en.wikipedia.org/wiki/Charles_Babbage"" title=""Charles Babbage on Wikipedia"">Charles Babbage</a>'s <a href=""http://en.wikipedia.org/wiki/Analytical_engine"" title=""The Analytical Engine on Wikipedia"">Analytical Engine</a> was perhaps the first major advance in modern computing, yet the first plans date from 1837, an extraordinary 100 years before the first general-purpose computers were built.  Babbage's designs include items that are direct precursors of modern-day computer components, including a giant mechanical <abbr title=""Central Processing Unit"">CPU</abbr> (the modern-day's computer's ""brain""), which Babbage called the ""mill""; memory, called the ""store""; and persistent storage (disks) in the form of punched cards.  The Analytical Engine was to be powered by steam, and programmable like any modern computer (albeit using punched cards).  By way of reference, the first programmable computer ever built was the <a href=""http://en.wikipedia.org/wiki/Colossus_computer"" title=""Colossus computer on Wikipedia"">Colossus</a> housed at <a href=""http://bletchleypark.org.uk/"" title=""Bletchley Park website"">Bletchley Park</a> at the end of World War II.  

<a href=""http://en.wikipedia.org/wiki/Ada_Lovelace"" title=""Ada Lovelace on Wikipedia"">Countess Ada Lovelace</a>, daughter of the great poet Lord Byron, met and corresponded with Babbage a number of times during the period in which he was designing the Analytical Engine.  She provided some comprehensive notes on the design, including a program for calculating a series of Bernoulli numbers in the Analytical Engine.  The program has since been proven to run correctly in the Analytical Engine, had it ever been built, and as a result she is widely considered to be the first computer programmer.  <a href=""http://en.wikipedia.org/wiki/Ada_(programming_language)"" title=""Ada (Programming Language) on Wikipedia"">A programming language has been named in her honour.</a>

The Analytical Engine was to be the successor to Babbage's earlier <a href=""http://en.wikipedia.org/wiki/Difference_engine"" title=""Difference Engine on Wikipedia"">Difference Engine</a>, which was essentially a mechanical calculator.  Construction began on the Difference Engine, but funding was eventually withdrawn by the British government due to spiralling costs (sound familiar?).  Working models have been produced in the last 20 years, however, and one is currently housed at the Science Museum in London.  Seeing it in action is a marvel, and hints at some of the beauty inherent in mathematics.  The following clip, whilst brief, gives you some idea:

<object width=""480"" height=""385""><param name=""movie"" value=""http://www.youtube.com/v/aCsBDNf9Mig?fs=1&amp;hl=en_US""></param><param name=""allowFullScreen"" value=""true""></param><param name=""allowscriptaccess"" value=""always""></param><embed src=""http://www.youtube.com/v/aCsBDNf9Mig?fs=1&amp;hl=en_US"" type=""application/x-shockwave-flash"" allowscriptaccess=""always"" allowfullscreen=""true"" width=""480"" height=""385""></embed></object>

The pause in the rotation of the dials happens when a carry is calculated.  Note that the machine also features a printer, built to Babbage's designs.  

To date, the Analytical Engine has never been built in its entirety.  Babbage produced many revisions to his designs over the course of roughly 35 years, and whilst he was able to build a small portion of it, he sadly passed away before he saw either of his great inventions built and in operation.  Recently a prominent British programmer, <a href=""http://www.jgc.org/"" title=""John Graham-Cumming's website"">John Graham-Cumming</a>, has started a project to build the Analytical Engine for the first time.  Named <a href=""http://plan28.org/"" title=""Plan 28 homepage"">Plan 28</a> after the most complete of Babbage's plans for the Analytical Engine, the <a href=""http://www.pledgebank.com/babbage"">aims</a> are four-fold:
<ol>
  <li>To help digitize and make available in electronic form all of Charles Babbage's notes and plans associated with the Difference Engine and Analytical Engine.</li>
  <li>To fund the study of Babbage's Analytical Engine plans to determine what best constitutes a complete design for the Engine.</li>
  <li>To coordinate the building of a computer simulation of the Analytical Engine that shows its working in 3D with accurate physics.</li>
  <li>To build the Analytical Engine and donate it to a museum in Great Britain for public display.</li>
</ol>

So, this is where you come in.  If you're reading this, the likelihood is that you have some interest in computers. You most likely don't need me to tell you that the Analytical Engine is where modern computing was conceived, or that even now, nearly two hundred years after the idea was first born, it would be an incredible achievement to see his invention built.  You already have a grasp of the importance of this project, and what it could do for furthering public education in and understanding of computing.  All you need to do is <a href=""http://www.pledgebank.com/babbage"" title=""Pledge support for Plan 28"">sign the pledge</a> and spread the word via Twitter, Facebook, your own blogs, word of mouth..."
Inside the Mind of Ayende Rahien,"This evening I attended an event at <a href=""http://skillsmatter.com/""Skills Matter</a> entitled ""Inside the Mind of Oren Eini"". Oren is probably better known to you as Ayende Rahien, prolific blogger, Open Sourcer and general all-round legend in the .Net community. 
","This evening I attended an event at <a href=""http://skillsmatter.com/""Skills Matter</a> entitled ""Inside the Mind of Oren Eini"". Oren is probably better known to you as Ayende Rahien, prolific blogger, Open Sourcer and general all-round legend in the .Net community. 

I took a bunch of notes that I hope will be of use to others as well as myself, and they're <a href=""https://www.evernote.com/view.jsp?locale=default&shard=s35#v=t&n=d8d4dc0b-6d9b-4dd5-96b0-6b0e0d5f4c60&b=202a9f11-de39-4b32-a459-c2f4f98620e0&z=d"">available</a> in <a href=""http://www.evernote.com/pub/alastairsmith/publicnotebook"" title=""Alastair Smith's Public Notebook"">my public Evernote notebook</a>.

Keep an eye on Skills Matter's <a href=""http://skillsmatter.com/"">website</a> for a <a href=""http://skillsmatter.com/podcast/open-source-dot-net/building-software-ayendes-way"" title=""Podcast: Building Software Ayende's Way"">video</a> of the talk.  On an <em>entirely</em> unrelated note, I should probably get tethering working between my iPhone and MacBook Pro. Blogging via a phone is a bit painful!"
Agile Testing & BDD eXchange 2010,,"Back in November, I attended the Agile Testing and Behaviour-Driven Development Exchange (BDDX) at Skills Matter in London.  Here is my very delayed write-up of the event.
<!--break-->
After a mini nightmare catching a train to Skills Matter (I watched two trains leave because they were too full and the platform was jam-packed. I'm glad I don't have to deal with the Tube on a daily basis.), I arrived in plenty of time for an invigorating cup of coffee and an <em>amazing</em> carrot cake muffin.  After a small spot of networking, we started promptly at 10am with a keynote from <a href=""http://twitter.com/tastapod"" title=""Dan North on Twitter"">Dan North</a> on the subject of <a href=""http://blog.dannorth.net/2010/08/30/introducing-deliberate-discovery/"">Deliberate Discovery</a>.  This is the first talk from Dan North I have attended, and I have to say I was very impressed.  Dan is an engaging and persuasive speaker, and his talk dealt effectively with the concept of second order ignorance, or what Donald Rumsfeld famously termed ""unknown unknowns"".  Dan's key thesis was that even if you were to make a list of all the things that were likely to be holding your project up at any given moment, the largest, most important thing would not be on that list, because you simply don't even know that you don't know about it.  Deliberate discovery provides some techniques for coping with the pitfalls of the usual planning process, such as positive and confirmation biases, as well as helping you realise that <em>it's ok to have unknown unknowns</em>.  Dan's <a href=""http://blog.dannorth.net/2010/08/30/introducing-deliberate-discovery/"" title=""Introducing Deliberate Discovery"">blog post on the subject</a> covers much of the same material as his keynote, so between that and my notes you should have something approaching 100% coverage! &lt;/badGeekJoke&gt;

Lunch was bracketed by a a quartet of case studies at varying levels of interest and use.  The first, covering usage of SpecFlow in building a new product (a C# implementation of the Gherkin specification <abbr title=""Domain-Specific Language"">DSL</abbr>), could feed back directly into my own work and projects, and I think some useful information was covered here.  The second, from AutoTrader, was an amusing romp through adopting Agile practices in a PRINCE2/Waterfall environment, from a tester's perspective.  Much of what was covered in this talk would have been very useful to the guys at my last job.  There was also a demo of a new product from ThoughtWorks called <a href=""http://www.thoughtworks-studios.com/agile-test-automation"">Twist</a>, which includes some really powerful features, such as ReSharper/Eclipse-like refactoring of acceptance tests.  Currently Twist only supports Java products, but .NET and Ruby support is on the roadmap.  I urge you to check out this product, it looks amazingly powerful.  

The two post-lunch case studies were the low point of the day for me, particularly the data warehousing talk which seemed too specific to that company's scenario to be of general use.  It was also full of data warehousing jargon like ETL, OLAP Cubs, SSIS, etc., that just went right over my head.  The talk from the guys at <a href=""http://klarna.se/en"">Klarna</a>, however, was good.  They have <a href=""http://klarna.se/en/personal/about-us"" title=""About us | Klarna"">a very interesting business model</a> whereby they act as a middle man in eCommerce transactions: the customer pays nothing until the goods have arrived (Klarna pay the merchant in the meantime).  This was borne out of a case in Sweden a while back where an online retailer was making a bunch of money selling digital cameras; one Christmas they realised they could make more money if they didn't ship the goods, and a whole bunch of customers lost a whole bunch of money.  The company was founded by a couple of business guys and a techie who built the service in Erlang in one month.  In the last 5 years, their revenue has jumped nearly 14000%, the monthly income for October 2010 exceeded that for the whole of 2008, they have expanded to 380 employees, and now operate in 6 countries.  That kind of rapid growth and expansion brings a bunch of problems, not least that of getting the new starters up to speed on the product as quickly as possible.  

Overall, the day was very interesting.  One or two of the case study talks in the middle of the day could have been a bit livelier, and might have benefitted from being broken up more by conceptual talks."
PowerShell one-liner: extract all zip files,"I'm a big fan of <a href=""http://www.tekpub.com/"" title=""TekPub - High-quality screencasts for programmers."">TekPub</a> and its thorough video tutorials on a wide range of development topics from frameworks like ASP.NET MVC and Ruby on Rails to building apps for the various mobile devices, to their free series such as key development concepts.  
","I'm a big fan of <a href=""http://www.tekpub.com/"" title=""TekPub - High-quality screencasts for programmers."">TekPub</a> and its thorough video tutorials on a wide range of development topics from frameworks like ASP.NET MVC and Ruby on Rails to building apps for the various mobile devices, to their free series such as key development concepts.  

The videos are streamed from the TekPub website, or, if you buy an individual series or an annual subscription, you can download the videos to keep.  The downloads are zipped-up video files (.wmv or .mp4), and so each file needs to be extracted before it can be watched.  When I download the zip files, I like organise them into a folder per series, under a top-level TekPub folder, something like the following:
<pre>
  + TekPub
  |  + ASP.NET MVC 2
  |  |  Episode 1.zip
  |  |  Episode 2.zip
  |  + Ruby on Rails 3
  |  |  Episode 1.zip
  |  |  Episode 2.zip
  |  |  Episode 3.zip

  ...etc.
</pre>

Expanding and then deleting each zip file is a dull manual process, so like any good developer, I chose to automate it.  Here's a PowerShell one-liner to expand and then delete all the archives found in this sort of shallow directory structure.  It wouldn't be too difficult to convert it to a fully-recursive process, so I leave that as an exercise for the reader :-) 

<script src=""https://gist.github.com/748258.js?file=Expand-AllArchives""></script>"
Revisiting the SOLID Principles: SRP for Methods?,NULL,"Over the weekend, I attended <a href=""http://developerdeveloperdeveloper.com/ddd9/"" title=""DDD9"">DDD9</a>, a really excellent and free event for developers.  The event was held at Microsoft's office in Reading, but was not a Microsoft event: Microsoft employees are forbidden from speaking.  This really is the premier developer event in the UK.  

I may do a round-up of the day here shortly, but in the meantime, I wanted to re-visit a topic I've touched on here before: <a href=""http://www.codebork.com/2009/02/18/solid-principles-ood.html"" title=""SOLID Princples of OOD"">the SOLID principles</a>, and specifically the Single Responsibility Principle (SRP).  One of the sessions I attended at DDD9 covered the SOLID principles in some depth (thanks, <a href=""http://designcoderelease.blogspot.com/"">Nathan</a>!) and this triggered a thought I'd had previously regarding the first of these principles, the Single Responsiblity Princple.
<!--break-->
The thought was relatively simple: could &mdash; <em>should</em> &mdash; the SRP be applied to methods as well as classes?  As it stands, the definition of the SRP, and indeed of the other SOLID principles, is in terms of class design, rather than member design:

<blockquote>
A class should have one, and only one, reason to change.
</blockquote>

Over the last couple of years since I wrote my original post on the SOLID principles, I have frequently been trying to apply the SRP to methods as well as classes.  This is, in part, due to my misremembering the exact definition of the SRP, and believing it to encompass members of a class as well as the class definition itself; I think, however, that applying the SRP sparingly to methods may result in cleaner code than restricting its use to classes only.  

<strong>Update: My friend <a href=""http://blog.analysisuk.com/"" title=""Analysis UK blog"">Steve</a> pointed out that my code example violated the SRP at a class level.  There's more than a hint of irony there :-)  As a result, I've updated this post with a new, improved example, and learned not to write blog posts late at night. Thanks go to Steve also for his feedback on the new examples. Enjoy.</strong>

Consider the following situation: an ASP.NET MVC action must display a particular record retrieved from a database, given the record's identifier.  

<script src=""https://gist.github.com/812575.js?file=MethodMultipleResponsibilities.cs""></script>

I count one, maybe two possible reasons for this method to change, i.e., two responsibilities it must handle:
<ol>
  <li>Building the view model.  What happens if you need to add an extra field to the view, or </li>
  <li>Returning the correct <code language=""csharp"">ActionResult</code>.  What happens if you want to return the record in JSON format for display via an AJAX call?  A new method parameter would be required to indicate that preference, or a new Action would need to be created to handle the situation.  There are pros and cons to both approaches, but if you chose the first, how would you implement that choice?  A simple if statement selecting the right ActionResult?  Delegate to a new method to get the right ActionResult?  Implement a new controller for the JSON actions?</li>
</ol>

Note that I left out retrieving the record from the database. I consider this to be already delegated to the repository object, and so isn't a responsibility of the Action itself; instead, the Action is simply querying the repository for the appropriate record.  

My preferred implementation for this routine would therefore be something like:

<script src=""https://gist.github.com/812575.js?file=MethodsSingleResponsibility.cs""></script>

Of course, the downside of this approach is that you start to end up with a long call stack, and lots of very small methods that don't do much (= more code).  I would argue, however, that for methods above a certain complexity, your code will be vastly more readable if you break your methods into smaller chunks, each with a defined responsibility.  I would even go so far as to suggest that it might be a good pattern to use for class interface (i.e., public) methods: these often provide a fa&ccedil;ade composing a number of individual actions.  In my opinion, each of those actions should be encapsulated in their own method.  

Indeed, one of <a href=""http://www.martinfowler.com/"" title=""Martin Fowler's homepage"">Martin Fowler's</a> <a href=""http://www.refactoring.com/index.html"" title=""Refactoring Patterns"">refactoring patterns</a> is called <a href=""http://www.refactoring.com/catalog/extractMethod.html"" title=""Extract Method Refactoring Pattern"">Extract Method</a>, and suggests that, when you have a code fragment that can be grouped together, you create a new method encapsulating that fragment and give the new method a name explaining the purpose of the method.

I briefly debated this with one of the more senior developers at Citrix a while back: he was of the opinion that the SRP should only be applied to classes, and not methods, I think on the basis that classes were what the SRP was defined for, and it's ok for methods to have multiple responsibilities, because that might be necessary for the routine to complete correctly.  

I think the readability concern is the important one here.  As we all know, <a href=""http://blog.stackoverflow.com/2009/03/it-stack-overflow-update-naming-is-hard/"" title=""IT Stack Overflow Update: Naming is Hard"">naming is hard</a>, and it is important to give methods (and variables, fields, etc.) good names in order to promote readability.  It becomes very difficult to give a method a good descriptive name if the method is doing multiple things: RetrieveRecordAndBuildViewModel() is a good name, but TryRetrieveRecordAndBuildViewModelAndDoSomethingElse() is starting to get silly.  BuildViewModel(record) is a better name, and the design has improved too with the Record dependency passed in as a parameter.  

I'm not advocating the use of the SRP in every method, as I believe this would actually hurt maintainability and readability, but for sufficiently complex methods, and for class interface methods, I think it makes sense to apply it.  Let me know what you think; I'm interested in generating discussion here."
Code Kata: Scales for development,"Anyone who has ever learnt a musical instrument will tell you of the pain of learning scales and arpeggios.  Those that stuck with their instrument will tell you that these are the constructs around which all music is based, defining which notes sound right or good within a given key.  The most common keys in classical music are grouped into major (""happy"") and minor (""sad""); other musical forms have other scales, such as modes and blues.  
","Anyone who has ever learnt a musical instrument will tell you of the pain of learning scales and arpeggios.  Those that stuck with their instrument will tell you that these are the constructs around which all music is based, defining which notes sound right or good within a given key.  The most common keys in classical music are grouped into major (""happy"") and minor (""sad""); other musical forms have other scales, such as modes and blues.  

But this is a software development blog, not a music blog. Why am I talking about musical scales? Well, you may not be surprised to read that the idea of practising the equivalent of scales applies to any skill you might want to learn.  For writing, it's practising writing little pieces of text: blog posts, Stack Exchange questions and answers, etc. For music, it's scales and arpeggios. For software development, code kata.

Code kata are little exercises to be practised over and over. They encapsulate a simple problem (""simple"" as in ""well-defined"", not as in ""easy"") such as scoring a game of ten-pin bowling, or calculating prime factors. The exercises require you to design and implement a solution to the presented problem, and are a great way of learning a new technique (they lend themselves excellently to TDD, for example), or a new language.

I'm making a start with The Pragmatic Programmers' kata over at http://www.codekata.com.  Others are available from http://katas.softwarecraftsmanship.org and http://programmingpraxis.com.  I'll be blogging my progress here; if you're doing kata too, let me know how you're getting on.  "
Code Kata: Supermarket Pricing,NULL,"So this afternoon I've been attempting the first of The Pragmatic Programmers' kata.  This is a modelling-only exercise, designed to get you thinking around a problem. You'll need to be familiar with the kata in order to get the most out of this post, so I'll go get a cup of tea whilst you <a href=""http://codekata.pragprog.com/2007/01/code_kata_one_s.html"">go read up on it</a>.
<!--break-->
Ok, now that you're more familiar with the kata and I'm adeqately refreshed, let's take a look at this thing.  I've come up with two approaches for your critique, and I suspect there's a better one just out of my reach.

<h3>Solution 1: a simplistic solution</h3>

Assuming we have a class called <code>Product</code>, it has one field called <code>Price</code>.  This solution essentially assumes that the actual price of an item changes when its put on offer.  For example, the can of beans at £0.65 becomes £0.33 per can when placed on a 3 for £1 offer; as a result of this, fractional money is permitted in this model to ensure correct rounding (so a single can is actually 33.3333&hellip;p recurring, not £0.33); without fractional money, products must have unequal pricing to make up for rounding errors (such as two cans at £0.33 and one at £0.34).  

This obviously leads us into a number of potential problems.  For example, all cans are now on sale on the offer price, no matter what quantity you purchase them in; furthermore, the shelf of 100 cans is now valued at £33.33, whereas it might be better for the supermarket to continue to value that shelf at the full £65.  

Equally, this approach doesn't work for items priced by quantity, such as loose strawberries priced at £1.99/kilo.  Is the price £1.99?  Do you then sell, for example, a quarter of a product priced accordingly?  

Buy-2-get-1-free can only work here if you calculate the price of a can as (£0.65 * 2)/3, in which case you are again changing the price of all cans on sale, no matter whether the customer buys 1 or follows the ""rules"" of the offer.

I think rounding wouldn't need to happen, as fractional money is permitted by the model.  However, once the prices have all been resolved, they would need to be rounded to the usual whole penny.  

<h3>Solution 2: a better (?) solution</h3>

The first solution could use some improvement.  For starters, the fractional money doesn't model the real world: no one carries around 0.5p any more, let alone 0.3333&hellip;p.  My second attempt sees the <code>Product</code> class expose two price fields: the base price (i.e., the standard retail price), and the offer price.

In the 3-for-a-pound scenario, the base price remains at £0.65 (this is fixed for the product) and the offer price is set to £0.33.  Rounding happens when setting the offer price.  The solves the problem of purchasing 4 cans in the offer, as you can calculate that the correct price is (offer price * 3) + base price.  <em>Or is it?</em>  In actual fact, the enforced rounding requires compensation: it should actually be ((offer price * 3) + £0.01) + base price.  An audit log would be needed to ensure that this was functioning correctly!

In the buy-2-get-1-free scenario, it becomes very difficult to work out what the offer price should be.  The best I could come up with was ((base price * 2)/3), which would be appropriately rounded (and again would need to be compensated for).  

This model is equivalent to the previous one for the strawberries.  The price per kilo is set as the base price, and is resolved at the point of sale.

However, in its favour, the shelf of 100 cans of beans is now deemed to be worth the full £65 again, and fractional money is not permitted by the model.  

So, this improves only slightly on the simplistic model, and all those extra pennies being added in feels a bit like the <a href=""http://en.wikipedia.org/wiki/Deferent_and_epicycle"" title=""Deferent and Epicycle"">epicycles</a> of old.  Additionally, they're obviously a recipe for disaster.

<h3>Solution 3: the ""Enterprise"" solution</h3>

The idea with this solution is to add in an external service to handle the responsibilities of calculating and administering the offers. So, a <code>Product</code> now has a flag to indicate whether or not it is a discrete item (such as a can of beans) or sold by quantity (such as loose strawberries). A <code>PriceManager</code> takes a <code>Product</code> and a quantity and resolves that to a price, e.g., for one can of beans, it would return £0.65; for 300g of strawberries priced at £1.99/kilo, it would return £0.60.  Rounding takes place within the <code>PriceManager</code>, and is calculated to the nearest penny.  The <code>PriceManager</code> calls an <code>OfferManager</code> to resolve any special offers applicable.  The <code>OfferManager</code> returns to the <code>PriceManager</code> an method for calculating the offer for that product.  Until the offer is satisfied, the <code>PriceManager</code> continues to return the standard retail price for the item (e.g., £0.65 for a can of beans), and when the offer rule is completely matched, the final item in the offer makes up the difference in price.  

So, for three cans of beans for £1, the <code>PriceManager</code> obtains an <code>Offer</code> stating that it requires three cans of beans.  The <code>PriceManager</code> returns £0.65 for the first two cans and -£0.95 for the third.  It will continue doing this for as many cans of beans as are purchased by the customer, thereby correctly charging the customer for the beans

In the buy two get one free offer, the <code>PriceManager</code> again obtains an <code>Offer</code> stating that it requires three cans of beans, and again returns £0.65 for the first two cans.  For the third can, it returns £0.00.  As before, it will continue doing this for as many cans of beans as are purchased by the customer, correctly applying the order.  

Finally, it separates the ideas of cost and price, so that the shelf of 100 cans of beans is still valued at £0.65 for stock costing purposes, even though it will be sold at a lower price.  

<h3>Conclusion</h3>

I think solution 3 is the most ""correct"", in terms of addressing the problem domain, but it seems really heavyweight (hence the title).  I welcome your feedback on these models, and would be interested to hear your own ideas on how to approach them."
...aaaand we're back.,,"Well, you might have noticed a change of look around here. Over the weekend, I upgraded this blog to Drupal 7, which was released a couple of months ago.  

<!--break-->

It seems that sometime in the last year Drupal also made the move to Git.  This, I hope, will vastly improve the update and third-party add on story; indeed, I am trying out the following plan:

  1. Check out the latest Drupal source (7.x branch) from http://git.drupal.org/project/drupal.git ([Drupal-specific instructions](http://drupal.org/node/3060/git-instructions/7.x)).  
  2. Check out each module as a Git submodule.  E.g., for the Views module, the appropriate incantation would be (from your Drupal installation directory) `git submodule add http://git.drupal.org/project/views.git sites/all/modules/views -b 7.x-1.x`.

I think this should allow me to keep my Drupal installation up-to-date automatically just by running `git pull` and `git submodule update` every so often.  I won't be able to verify this until a module or Drupal core update is released, though.  

Drupal 7's emphasis on improving usability over new features has yielded good results.  It's much easier to find the things I'm looking for in the admin interface; there's a new shortcut menu that sits at the top of the page, and the admin tasks overlay the site itself.  Some sections have been collapsed into a single tabbed section.  

The theme I was using for my blog previously ([AD The Morning After](http://drupal.org/project/ad_the-morning-after)) hasn't yet been ported to Drupal 7, so for the moment I'm running the new default theme Bartik.  I've been playing around with the [Omega](http://drupal.org/project/omega) theme as well, which provides a base for building HTML5 and CSS3 themes.  There are a couple of themes built on top of Omega (notably [Beta](http://drupal.org/project/beta) and [Gamma](http://drupal.org/project/gamma))

I've also installed the [Markdown](http://daringfireball.net/projects/markdown/) [module](http://drupal.org/project/markdown) because I was fed up with editing HTML manually.  I've set this as the default input format, so you can take advantage of this when leaving comments too.  "
Software Craftsmanship Conference 2011,"At the end of May, I attended the Software Craftsmanship Conference 2011 at Bletchley Park, near Milton Keynes.  As with the 2010 conference held in October this event proved to be well worth attending, although sadly (and counterintuitively) the weather didn't really allow for tours of the Park this time.  Thanks to Jason Gorman, the speakers, and the folks at Bletchley Park for organising and contributing to the event, and to the volunteers at Bletchley Park in particular for braving the elements to offer us tours of the historic site.  

Read on for a summary of my day...","Despite some initial reservations about the sessions on offer at this year's Software Craftsmanship conference, I found the day interesting and useful.  It was good to see some familiar faces from last year's event as well as some of the leading lights in the dev community such as [Gojko Adzic](http://gojko.net/ ""Gojko's blog"") and Sandro Mancuso, convenor of the London Software Craftsmanship Community.  There was an interesting mix of sessions and, importantly, session formats, with my day being neatly balanced between coding and discussion sessions.  Coding sessions were invariably pair-programmed using some form of <abbr title=""Test-Driven Development"">TDD</abbr> methodology.

### _How Object Oriented Are You Feeling Today?_  Krzysztof Jelski ###

[Session overview:](http://vimeo.com/23404983)
<iframe src=""http://player.vimeo.com/video/23404983?title=0&amp;byline=0&amp;portrait=0"" width=""400"" height=""320"" frameborder=""0""></iframe>

This was, I think, my favourite session of the day, and a great way to get into the event as a whole.  The session is based around Jeff Bay's Object Calisthenics which proposes the following nine rules for writing better Object-Oriented Code:

1. Use only one level of indentation per method
2. Don’t use the else keyword
3. Wrap all primitives and strings
4. Use only one dot per line
5. Don’t abbreviate
6. Keep all entities small
7. Don’t use any classes with more than two instance variables
8. Use first-class collections
9. Don’t use any getters/setters/properties

The rules are quite often extreme, and are certainly artificial, _but_ they are brilliant in small self-contained exercises such as the bank account scenario we worked on during the session.  They really do help you get into the mindset of object orientation when procedural code can feel more natural.  

Some of the rules are easier to follow than others: rule 6 falls out of some of the others, such as rule 3; rule 5 is simply a rule for good naming that I follow during daily development anyway.  Rule 9 was a really tough one to follow, and I think our compliance with it was a bit suspect (working in C#, we used a method called `GetCurrentBalance()` in place of a property `CurrentBalance`).  Rule 3 was not difficult to follow per se, but it did increase the amount of code enormously and made you second-guess just how far you followed it.  For example, in the bank account exercise we wrapped the balance, which might normally have been represented as a decimal, into an object `Money`; however, this in turn used decimals in its implementation which made me wonder whether we should have wrapped that as well!  

Having got into the OO mindset during this session, I found some of the ideas I picked up during it were easily applicable in the CyberDojo session in which I participated towards the end of the day.  There is real value in practising these rules in a small exercise, viewing the über-OO code that you get out of it, and following the patterns of implementation you see in the result in your everyday work.  

My one ""complaint"" about this session is that it was too short!

### _Enhancing Legacy Test Suites_ Christian Horsdal ###

[Session overview:](http://www.youtube.com/watch?v=lz_cBR8Mg0E)
<iframe width=""425"" height=""349"" src=""http://www.youtube.com/embed/lz_cBR8Mg0E"" frameborder=""0"" allowfullscreen></iframe>

This session took me a little by surprise: I found it was a discussion-based session when I was expecting a coding session.  That'll teach me to read the session summaries and watch the overview videos beforehand...  Anyway, the premise of the session is that even though some code bases can have extensive suites of unit tests, the quality of the tests may be low (this is something I can sympathise with).  Christian went through a few pointers of what could be considered a poor-quality test (tests named Test1, assertions that test little or nothing, tests that test the implementation rather than the contract, etc.) and then fired up Visual Studio 2010 and the Nerd Dinner application produced by [Scott Hanselman](http://www.hanselman.com) and others as a tutorial for ASP.NET MVC (v1.0, so it's a couple of years old now) and running live at http://www.nerddinner.com/.  We then spent the better part of an hour <span style=""text-decoration: line-through"">picking apart</span> critiquing the tests in the application.  The most interesting thing for me was how obvious it was to people in the room that the tests had been written after the code and not beforehand as an integral part of the development workflow, and the best way to enhance this suite of tests was to re-write tests as you came to them.  

Christian's own write-up of his session is [here](http://horsdal.blogspot.com/2011/05/take-away-points-from-my-software.html).

### _CyberDojo_ Jon Jagger ###

[Session Overview:](http://vimeo.com/15104374)
<iframe src=""http://player.vimeo.com/video/15104374?title=0&amp;byline=0&amp;portrait=0"" width=""400"" height=""330"" frameborder=""0""></iframe>

This was another fun coding session, based around Jon's CyberDojo app which you can try for yourself at http://www.cyber-dojo.com/.  The idea behind the app is to practice coding in a test-driven manner on a simple solution.  The idea is not to finish the exercise on the first (or subsequent) attempt, but instead to improve your coding skills through deliberate, repeated practice.  It was a useful introductory session to the app and to this style of practice, but didn't go much further than this.  When running the second iteration in Java (the first iteration was in Ruby and consequently was... difficult), I was able to apply some of the ideas learned in the morning's Object Calisthenics session to improve the OO-ness of the solution.  

### _Personal Codes Of Conduct_ Matt Williams ###

[Session overview:](http://www.youtube.com/watch?v=szf68J4bH38)
<iframe width=""560"" height=""349"" src=""http://www.youtube.com/embed/szf68J4bH38"" frameborder=""0"" allowfullscreen></iframe>

Matt's session was one of the more interesting discussions on the software industry I've been in for a while.  Matt's idea was to try to take some lessons from medicine and apply them to software, and in the main I think he succeeded.  His idea was to take some of the principles of the Code of Conduct set out by the General Medical Council and translate it into something relevant to the software industry; he has more recently been using this as his own personal Code of Conduct, and expects his colleagues and customers to hold him to it.  The following discussion session revolved around the usual Craftsmanship topics of quality and process, and knowingly side-stepped that of certification.  One area I would have liked to have seen explored in more detail is that of reputation, which was mentioned by one participant but was not really developed for some reason.  Considering the analogous situation of cowboy builders, you quickly come to realise that reputation is paramount to securing a good builder: people tend to re-use tradesmen over a large number of years based on previous positive experiences, good personal recommendations, etc.  If you pick someone new out of the Yellow Pages each time, or respond to a leaflet through your door, you're guaranteed to have at least one bad experience.  Part of the problem appears to be in educating our potential customers in the value of demanding good quality, well-tested, highly-covered code when this quite often falls prey to the requirement of ""and I want it yesterday!""  

### Conclusion ###

I had a great time at SC2011, and am already looking forward to SC2012.  I hope that this summary has convinced you that you should be there too!  "
TDD and Pairing,,"Today I spent most of the day pairing on a task with another developer. This is unusual for our company (the usual <span style=""text-decoration: line-through"">arguments</span> fears are raised) but we both found it a useful experience.  Here's what I learned today.  

As part of the pairing scenario, I suggested using a ping-pong test-first approach: 

  1. one developer writes a failing test and passes control to the other developer. Only one assertion is allowed per test method.
  2. the second developer then implements the code necessary to make the test pass, checks in to source control, writes a second failing test and passes control to the first developer
  3. the first developer writes an implementation, checks into source control, writes a failing test, and passes control to the second developer
  4. goto 2.

We found this a useful approach that allowed us both to build up a picture of the system as we developed it: both developers have to work out the tests and implementation, either by writing the failing test or by making the test pass.  I have since found (as is usually the case with TDD) that we have exceptionally high code coverage: 100% nearly everywhere.  

Progress was possibly a little slow given that I am a relative novice to TDD and my partner is even more so.  This is what I learned from the process, that I'm trying to feed back into my personal TDD style:

  1. **Think about the assertions first.**  This stops you getting bogged down in the implementation details of the test: you can go back and write them later, often with the help of ReSharper.  
  2. **Baby steps really are important.**  In the implementation of our first test, we jumped the gun a little bit implementing a List<T> as a data store for the class.  Strictly speaking, all we needed to pass the test was to return a number.  This seems like a silly implementation step to take: you know you're going to need to change it later and it doesn't do anything useful other than specifically pass that one single test.  However, when we came to write our second, third, and seventh tests, we found that all three passed coincidentally because we'd taken that leap too early.  
  3. **Baby steps are important for the tests too.**  A couple of times we tried to write a test that was too large for the progress we'd made so far.  Essentially, this was a test that said ""fulfil the whole specification"".  It is easier to come up with a sensible, single assertion in a smaller tests than it is in a larger or all-encompassing test.

* * * 

I'm really loving my job at the moment.  We're hiring, so why not come [work for us](http://grantadesign.com/jobs/index.htm)? We have [graduate roles](http://grantadesign.com/jobs/graduate-softwareengineers.htm), [senior roles](http://grantadesign.com/jobs/softwarec_jan11.htm), and [ASP.NET roles](http://grantadesign.com/jobs/asp.htm).  We have the widest range of teas in Cambridge."
Project Roslyn: .NET's Compiler as a Service,,"I spent some of this evening watching the video from Microsoft Research on their Project Roslyn, the codename for the Compiler-as-a-Service functionality they're developing for .NET vFuture.  
<!--break-->
The most interesting parts of the talk for me were:

1. The C# and VB compilers are being completely re-written.  The compilers have been written, for the last 10+ years in C++, and have been black boxes that take in source code and spit out IL.  The Roslyn compilers are being written in *managed code*, in part to enable greater componentisation so that, for example, you can plug directly into the part of the compilation pipeline that most interests you.  
2. Looking at the APIs Roslyn exposes, it seems like products like ReSharper will become much more easily implemented.  Will this introduce more competition into the refactoring tools market?  Maybe.  The Syntax Tree API (slides 19-23) looks very readable.
3. Anders Hejlsberg is lead architect of Roslyn.
4. A consideration is to release an open-source version of Roslyn to enable experimentation with your own new language features.  (There will be no support for extending or modifying the parser through the compiler APIs).
5. The kind of refactorings Roslyn will enable go way beyond the existing Rename Method and Extract Interface. Roslyn can enable refactorings that modify your code to, e.g., increase program security, or increase performance. These are things that would otherwise have to be designed into the program from the beginning.  

There's also some interesting stuff on how people use refactoring tools.  

You can view the full video embedded below, or [with slides on Microsoft Research's site.](http://research.microsoft.com/apps/video/default.aspx?id=152281)  [Silverlight](http://www.microsoft.com/getsilverlight/get-started/install/default.aspx?reason=unsupportedbrowser#) is required to view either.

<object data=""data:application/x-silverlight-2,"" type=""application/x-silverlight-2"" width=""320"" height=""246""><param name=""source"" value=""http://research.microsoft.com/apps/video/ClientBin/EmbeddedPlayer.xap""/><param name=""enableHtmlAccess"" value=""true"" /><param name=""initParams"" value=""id=152281,start=0,end=4551"" /><param name=""background"" value=""white"" /><param name=""minRuntimeVersion"" value=""3.0.40818.0"" /><param name=""autoUpgrade"" value=""true"" /><a href=""http://go.microsoft.com/fwlink/?LinkID=149156&v=3.0.40818.0"" style=""text-decoration:none""><img src=""http://go.microsoft.com/fwlink/?LinkId=108181"" alt=""Get Microsoft Silverlight"" style=""border-style:none""/></a></object>"
Test-Driving a Leap Year Calculator,,"In preparation for [CAMDUG's first ever Dojo](http://codebork.com/2011/08/14/camdugs-first-ever-dojo.html) yesterday, I worked through the Leap Year kata, which I had intended to run in the first session, ""Introduction to TDD"".  I ended up swapping it out for the Roman Numerals kata, because I got through two iterations of the exercise in about 20 minutes.  However, I thought it would be useful to share the problem and my approaches to it with my readers.  
<!--break-->
The Problem
========

Like many kata, the problem is simply stated:

> Write a function that returns true or false depending on whether its input integer is a leap
> year or not.  A leap year is defined as one that is divisible by 4, but is not otherwise divisible by 
> 100 unless it is also divisible by 400.
>
> For example, 2001 is a typical common year and 1996 is a typical leap year, whereas 1900 is an
> atypical common year and 2000 is an atypical leap year.

First iteration
========

For my first iteration, I took an approach that I think is akin to Acceptance Test-Driven Development (ATDD).  That is to say, I took the four example years from the problem statement above and used them as my acceptance cases.  If all four acceptance tests pass, the system could be considered complete.  

[gist:1145099:acceptance_tests.cs]

[gist:1145099:implementation.cs]

I was left a bit uneasy with this approach, however: it seemed too easy, and I was only testing the examples given.  How did I know that, for example, 1984 would be considered a leap year and 1985 wouldn't?  What happens if -1984 is provided to the method?  What about 0, and other test cases I haven't even considered yet?

Second iteration
==========

I then deleted my first attempt; this is an important step when doing many iterations of a kata as it helps you to clear your mind of the details of previous solutions and look at the problem anew.  I tackled the problem with my more usual form of TDD in the second iteration, which is to say I wrote tests to exercise the method contract rather than fit the provided examples.  

My first test exercises the most basic rule of a leap year: if it is divisible by 4, then it is a leap year:

[gist:1145103:FirstTest.cs]

I went with the TDD approach of writing the simplest possible code to make the test pass:

[gist:1145103:FirstImpl.cs]

My second test exercises the inverse condition: if the year is not divisible by 4, then it is not a leap year:

[gist:1145103:SecondTest.cs]

And again, the simplest possible code to get this to pass is:

[gist:1145103:SecondImpl.cs]

I jumped a bit ahead with my third test, and ended up breaking my flow: I wrote a passing test.  That's a bad thing to do, because it means you have to really think hard about whether or not it is passing *for the right reason*.  If they're passing for the wrong reason and you leave it be, you're likely to run into problems later.  

[gist:1145103:ThirdTest.cs]

Because this test passes, there's no implementation step.  

Instead of writing a test for the situation where the year is divisible by 400, I should have written a test for the situation where the year is divisible by 100, which should return false:

[gist:1145103:FourthTest.cs]

And there is an associated implementation to write for this test:

[gist:1145103:FourthImpl.cs]

Aha!  Now test 3 fails.  It is hopefully now made clear why I should have written these last two tests the other way around: tests should only start to fail when you break something, which I have not done here.  Anyway, I'm now in the situation I would have been in if I had written the third and fourth tests the other way around, and I can write the implementation for the third test:

[gist:1145103:ThirdImpl.cs]

An integral part of the TDD workflow is refactoring: the rhythm that should be adopted is ""Red, Green, Refactor"".  My chosen refactoring was to pull out the small amount of logic testing whether a year was divisible by 100 into a new method called `IsCentury()`:

[gist:1145103:Refactor.cs]

At this point I decided to stop, but there are more refactorings to do (e.g., the code checking whether the year is divisible by 400, and perhaps the one for years divisible by 4 as well).  I felt more comfortable with this approach because I was testing the general rules rather than the specific examples.  I felt comfortable with the idea of extending this test suite and implementation to handle some of the edge cases (negative numbers, etc) I mentioned above.  I think the code for the second iteration is a fair bit cleaner than the first, even though there's only a couple of small refactorings between them.  

I'm interested to read feedback on both these approaches: am I missing something with ATDD?  Is my usual TDD approach flawed?  Am I refactoring early enough in my workflow?  "
Seven Languages in Seven Weeks: follow my progress on GitHub,,"At NxtGen's Fest event back at the beginning of July, I bought a copy of Bruce Tate's excellent and highly-regarded book *Seven Languages in Seven Weeks*.  The aim of the book is to teach you seven new languages (in seven weeks!) to change the way you think about your code and the languages you use in your day-to-day job.  Each language covered (Ruby, Io, Prolog, Scala, Erlang, Clojure and Haskell) approach problems in a different way: Ruby is a pure object-oriented language with dynamic typing, for example, whilst Haskell is a pure functional language; Scala bridges this gap somewhat, whilst Prolog resides in a completely different paradigm all of its own (logic programming); Erlang is great for multi-process applications, underpinned by a philosophy of ""let it crash""; Clojure is an implementation of Lisp for the JVM, and I've never even heard of Io. 

You can track my progress through the book via the [GitHub repository](https://github.com/alastairs/7in7) I set up for it."
CAMDUG's first ever Dojo,,"Yesterday, [CAMDUG, the Cambridge Developer's Group](http://www.camdug/), ran its first coding dojo generously hosted by my employer [Granta Design](http://www.grantadesign.com/jobs/) (we're hiring!).  Rooted in the principles of [Software Craftsmanship](http://manifesto.softwarecraftsmanship.org/), dojos bring developers together to sharpen their Test-Driven Development and Design (TDD) saws.  Like musicians practising their scales, kata are [exercises for developers](http://codebork.com/coding/2011/05/15/code-kata-scales-development.html) to instil muscle memory.  

CAMDUG was joined for the first time by members of the [London Software Craftsmanship Community](http://www.meetup.com/london-software-craftsmanship/) including one of its co-founders, Sandro Mancuso who was gracious enough to cross-promote the event to his own members.  Thanks Sandro!  The mix was approximately 50:50 which brought us an interesting diversity of primary languages too: ~50% Java, ~30% C#, ~10% Python and ~10% Haskell.  This was particularly good for CAMDUG as we originally started out as the Cambridge .NET User's Group and expanded the remit about eighteen months ago.  

<!--break-->

First-session: Introduction to TDD
=====================

I composed the day from four sessions, with a good discussion session over lunch (generously provided by [Red Gate](http://j.mp/rickyleeks/)). The first session was an intended to be an introductory session, but given that the vast majority of people in the room were familiar with the principles of TDD I was able to skip much of that content.  Instead, we got straight into the first kata, which was to convert numbers into [Roman Numerals](http://en.wikipedia.org/wiki/Roman_Numerals).  Two iterations on this problem were completed, with people deleting their code and swapping pairs between them. There were some interesting approaches to the problem: some people tackled it with sequential numbers from 1 and tackling each new case as it came along, before introducing some random higher numbers to exercise trickier facets of the problem; others tackled the simple cases (1, 5, 10, 50, 100, 500, 1000) before moving onto trickier numbers like 2, 18, etc.  Additionally, some people struggled with understanding the domain of the problem (although seemed to grok the problem as a whole) and ended up spending one iteration of this session on what felt to me like Big Design Up Front.  

Second Session: Cementing Your Knowledge
===========================

The second session was intended to be more challenging, and so I set the problem of the game of Minesweeper to be attacked in one paired iteration.  The aim of the kata is to generate a hint-field from a description of a Minesweeper board.  The hint-field places the mines on the board, and fills in all the other squares with a number indicating how many mines around found in the squares immediately surrounding that square.  The kata describes some sample input in the following format

    4 3
    *...
    ..*.
    ....

This states that the board is 4 squares wide by 3 squares high; mines are represented as asterisks and empty squares are represented as dots.  That input should then be converted into the following output:

    *211
    12*1
    0111

Even allowing the whole hour for this exercise wasn't long enough for people to make a really substantial amount of progress, but there were a few pairs that did make significant progress on the problem.  Lunch arrived toward the end of this session, so I wasn't able to do the same kind of round-up of approaches as I did the first.

Third Session: TDD in the Real World
======================

After some great discussions over some tasty food, we started back after lunch with a more ""real-world"" problem than we had been approaching in the morning: developing a simple bank account system.  The initial exercise was to implement Deposit, Withdraw and Check Balance behaviours.  If pairs completed those three behaviours before the end of the session, they were free to implement further behaviours of their choice such as Transfer, Print Statement, etc.  I think only one or two pairs did in fact make it onto this stage in the hour, so this was a slightly better-judged session time-wise.  Interestingly, one or two pairs developed their system under test with a full set of green tests that sometimes only tested the happy path.  As I was doing my rounds in this session, I prompted a few people to consider the situation where a negative amount of money was deposited into the account.  Another interesting observation was the number of people using ints to represent quantities of money and this led me to consider an alternative approach to my default (using floats to denominate pounds and pence): using an unsigned int or long to represent the amount in pence.  This removes problems caused by rounding errors, but of course limits you to only a single currency (because the conversion to sterling may result in fractional pence).  

Fourth Session: An Objective Work-Out
========================

This was a more advanced session, and the highlight of the day for some (including me).  For this exercise, we kept the same pairs and same code from the previous session and the first aim was to complete a refactoring exercise.  The exercise was to refactor the system to obey the [nine rules of object calisthenics, as defined by Jeff Bay](http://j.mp/objectcalisthenics):

1. One level of indentation per method
2. Don’t use the ELSE keyword 
3. Wrap all primitives and strings
4. First class collections
5. One dot per line
6. Don’t abbreviate
7. Keep all entities small
8. No classes with more than two instance variables
9. No getters/setters/properties

(Explanations of the rules are provided in the [paper](http://www.cs.helsinki.fi/u/luontola/tdd-2009/ext/ObjectCalisthenics.pdf), which is a short but very interesting read.)  These rules are intentionally very strict and are designed to lead you into writing highly object-oriented code.

Some of these rules were easier to follow than others; as we'll see shortly, rule 9 was the hardest to follow, but other than that different pairs struggled with different rules.  Rule 6 was declared the easiest to follow.

Unanimously, the quantities of money being handled, previously handled as ints or doubles, were wrapped up into a Money class.  One question that was immediately thrown up was how to implement the Check Balance behaviour without using a getter.  This is indeed a tricky question, and there are a number of approaches to solving it: some wrapped the concept of a Balance into a separate class (arguing, rightly, that a Balance can be negative whilst Money cannot), although this threw up a few new problems to solve; others calculated the balance based on a list of transactions.  

Post-Close Session: Pub
===============

We retired to The Earl of Derby afterwards for a drink and a discussion over the day.  One particularly fascinating discussion surround the interplay between the ninth rule of the object calisthenics exercise (no getters and setters) and the [Single Responsibility Principle (SRP)](http://codebork.com/2009/02/18/solid-principles-ood.html#SRP): how do you implement a statement printer obeying the SRP and without using getters?  You either need a class that handles printing of a statement (to obey the SRP) that retrieves information about the statement via getters, or you need to violate the SRP and require that statements, transactions and money all know how to print themselves.  Using data-transfer objects (DTOs) or ViewModel objects doesn't help at all either, because you would need to violate the ninth rule twice in order to use them: once on the Model objects to create the DTOs/ViewModels, and once to print the DTOs/ViewModels in the printer object.  

I had seen on Twitter that Sandro was reading *Game of Thrones* (as am I), so we also spent a bit of time talking Sci-Fi and Fantasy :-)  

Wrap-Up
=====

I wil be running a similar day at work in the near future, so this turned out to be really useful in working out what to do and what not to do next time.  

I think down to my inexperience in running this kind of event (it was my first as organiser/facilitator) and some nerves on the day, I started one or two of the sessions before everyone had completely grokked the task at hand, which meant that they either didn't get as much out of that session as they might have, or that I had to do the rounds and ensure everyone was kicking off ok.  

When I got home, [this post](http://blog.coderetreat.com/on-the-role-of-the-coderetreat-facilitator) popped up in my RSS reader, which would have been a useful read beforehand, so I will digest that before the next.  

Some administrivia bits fell through, as well; I wasn't able to buy the appropriate labels for Meetup's name badges, so we had to do a quick round-room introduction at the start of the day.  This was doubly disappointing for me, as I had intended to put a mark on each name badge indicating what languages the attendee was comfortable using to make pairing a little easier.  That said, we didn't seem to have any problems getting the pairs sorted relatively quickly.  Additionally, we weren't able to provide an internet connection to attendees, but this didn't hinder the day as much as it might have done at other types of event.  

It was *really* difficult keeping to time; we started five minutes late and although we got back on track after lunch we ended up finishing the afternoon ~30 minutes over time.  At future events of this kind, I will need to allow more time in the schedule for the discussions.  

I also need to remember that The Flying Pig doesn't open until at least 6pm on a Saturday.

Slides
====

The slides I used during the day, including some simple descriptions of the kata, are [available for download](http://www.codebork.com/sites/default/files/CAMDUG%20Dojo%20Day.pptx)."
Pretty-printing JSON with Json.NET,,"I was struggling to find a way to pretty-print a JSON string in .NET.  I already have the excellent [Json.NET](http://nuget.org/List/Packages/Newtonsoft.Json) package available and after a bit of hunting, I came up with this:

[gist:1152445]

It wasn't obvious how to do this, for some reason - I spent a while investigating the `JsonReader` and `JsonWriter` classes to no avail.  

Hope this helps you too!"
Public Service Announcement: Io and Prolog on Mac OS X,"I encountered a couple of issues moving on from the Ruby chapter of Bruce Tate's book *Seven Languages in Seven Weeks*:

1. Io does not currently compile,
2. Prolog installs to an odd location.  ","A quick public service announcement for anyone else currently working through *Seven Languages in Seven Weeks* on a Mac.  

Chapter 3's language, [Io](http://www.iolanguage.com/), does not currently compile on Mac OS X if you attempt installation via [Homebrew](http://mxcl.github.com/homebrew/).  [This patch](https://github.com/stevedekorte/io/pull/140) should fix it once it has been pulled into Io master. 

Chapter 4's language, Prolog, can be installed from the [GNU Prolog web site](http://www.gprolog.org/#download).  It installs to an odd location, `/opt/local/bin`, which you will need to add to your path to get it working properly.  

Hope that helps someone else out there!"
Implicit Interface Implementation in .NET,,"I've just been reading Brad Wilson's latest blog post, [Interface Attributes != Class Attributes](http://bradwilson.typepad.com/blog/2011/08/interface-attributes-class-attributes.html), and hit upon this interesting nugget regarding the implicit implementation of interfaces in .NET:

> Implicit implementation (as seen above) is when a class implements the method/property in question as
> a public method or property on the class. It's important to note that this method/property is NOT the
> same thing as the interface method/property; it merely has the same signature, and thus can be used to
> implicitly create the implementation of the interface. In reflection terms, the two are distinct and
> different.

Hum. I did not know that.
<!--break-->"
PowerShell function Get-ScriptDirectory,,"Very often when I'm writing a script in PowerShell, I will need to obtain the directory in which that same script resides.  This is sadly not completely trivial in PowerShell, but the following function achieves the desired result:

[gist:1215274]

Hopefully, having created a gist for this and blogged about it, I won't have to keep resorting to Google in future to remind me how to do this.  This particular version was posted to [the PowerShell Code Repository](http://poshcode.org/2887) by Andy Arismendi.  "
Initialising a collection from another collection,,"Here's a handy trick I discovered whilst fixing a bug.  You can initialise a collection from another collection and add new items to it as follows:

[gist:1219663]

Note that the collection initialiser on `AllFoo` runs *before* the constructor, so the `Foo` created with constructor parameter 4 will appear *first* in `AllFoo`, followed by everything in `BaseCollectionOfFoo` in the expected order.  "
"Why I'm nervous about attending #GiveCampUK, and why I think that's ok",,"So earlier today, I posted on Twitter that [Meanwhile, I'm quietly shitting myself about #givecampuk which is in just two days' time!](http://twitter.com/#!/alastairs/statuses/126623240788520960)

This prompted a slew of replies, not least from [Paul Stack](http://www.paulstack.co.uk/), one of the [GiveCamp UK](http://www.givecamp.org.uk/) organisers, who said

> ""I wish there was no nerves over #givecampuk - we are all in the same boat - noone is better than anyone else :)""

There was some discussion on this topic a few weeks back, which Paul addressed in an excellent [blog post](http://www.paulstack.co.uk/blog/post/why-you-shouldnt-feel-nervous-about-givecampuk.aspx) at the time, essentially stating that no one person knows *everything* about software development, and everyone (including the likes of Scott Guthrie) considers themselves to be learning from other people all the time.  

Here's the thing: I get all that, and I agree with it; the continual learning process is why I love this career that I've chosen as much as I do.  But still I'm nervous.  Why?

Paul said that GiveCamp UK is ""**NOT A JOB INTERVIEW**"" (emphasis his), and I do agree with that statement, but I think it *is* like the first day of a new job.  It has the same unknowns: people, project, product, practices and processes, and in that regard I think those nerves I'm feeling are *perfectly healthy*.  They're the same nerves I felt when I started at Granta a little over a year ago.  They're the same nerves I feel when I play a solo bassoon gig.  They're same the nerves I still feel sometimes when playing an orchestral gig.  They're the same nerves I'm feeling for my new role leading a small Agile team at work.  It's not so much performance anxiety (although there is certainly an element of that) as jitters fuelled by excitement and the unknown; it's the build-up of adrenaline as I get to face another great challenge.

GiveCampUK is going to a fantastic experience and, most likely, a massive rush.  These nerves are just the start of it."
#GiveCampUK 2011 - Oh My,I am going to open this blog post with a bold statement: GiveCamp UK was one of the greatest experiences of my life.  Here's my story. ,"Friday 21 October : ""Sleep is a weapon""
---------------------------------

I took Friday off work, did a spot of shopping in the morning, and hopped on a train down to London.  Thank god the hotel and the venue were so close to Kings Cross; it was an absolute lifesaver not having to battle the Tube etc.  Checked into the hotel, relaxed for a bit before meeting [Adrian](http://www.adrianbanks.co.uk/), then wandered over to [UCL](http://www.ucl.ac.uk/) to register.  

A little while later (we turned up quite early), the event got under way with an introductory talk from [Rachel](twitter.com/rachelhawley) and the charities introducing themselves and their requirements.  Many of the [projects](http://www.givecamp.org.uk/blog/introducing-our-givecamp-uk-2011-charity-projects/) sounded really interesting!  We then had the opportunity to mix with other volunteers and the various charities to decide which project we wanted to work on.  I spent some time milling around, but the two projects I was particularly interested in were the CMS/repository of resources for the [Charity Technology Trust](http://www.ctt.org/) and the [Nathan Timothy Foundation](http://www.nathantimothyfoundation.org/)'s site to aggregate their various items of content.  I was curious too about the wildcard project, which intended to build phase one of an open-source <abbr title=""Customer Relationship Management"">CRM</abbr> geared towards simplicity, and purpose-built for the voluntary sector.

This ""mixer"" session was one of two bits of GiveCamp that didn't really work for me.  It seemed that the choice of projects was quickly reduced before the session really got started:

* One project team had selected their technology before GiveCamp even started, and it was one with which I was unfamiliar (node.js).  I was quite interested in this project and it was a bit of a shame to have to discount it. Mind you, I'm not sure I could have coped working with Gary Short and Rob Ashton ;-)
* Another project team seemed to be fully-assembled by the time I got to them, to the extent that they had started work!  It seemed like I'd have a fair bit of catching up to do if I wanted to join that team.
* One charity had specified they want to work with WordPress.  I've done enough PHP to know I don't want to touch it again if I can help it.  

After doing the rounds and spending some time in the discussion on the wild-card project with Kendall Miller, it became obvious which project I was going to pick.  Kendall has great energy and enthusiasm, and I figured that was something we were going to be in need of at 3am Saturday night when nothing worked. (Thankfully, it didn't come to that!)  

We started going over the full *half-page* of requirements and working out how we wanted to tackle the solution.  Whilst the aim was a free open-source CRM aimed at charities, what we decided to go with was a cloud-based <abbr title=""Software as a Service"">SaaS</abbr> solution that would be super-simple to deploy: you just can't beat the power of the cloud for making deploying an entire system easy.  The CRM needed to be able to import data from Excel to make it easy for charities to get their existing data into the system.  The CRM had to be able to manage campaigns, donations and members, and much more besides.

(**Aside:** This was the second bit of GiveCamp that didn't quite work for me. All teams were discussing their projects in the very echoey hallway/corridor that was our base for the weekend, and it made it *really* difficult to hear half the discussion. It would have been useful if we'd had separate rooms for the project teams at this point. Luckily Kendall had a loud voice!)

And with that, **[GiveCRM](http://www.givecrm.org.uk/) was born!**  We started dividing up the work: one team would work on the back end database, one team would work on an API for importing and exporting data from Excel, one team would work on the setup and provisioning site, and one team (the largest) would work on the core CRM web app.  With our billets posted, we broke up to go start planning our teams' work.  

Here's one of the things I loved about GiveCamp: when discussing what technologies we were going to use, someone threw an idea out there (e.g., "".NET 4.0"", ""ASP.NET MVC 3"", ""Git"", etc.), and unless someone had a particular problem with it (which no one did), we just went for it. No lengthy evaluations, no dithering, just good swift decision making. Kendall was exceptionally good at keeping up the momentum on the decision making all weekend, which meant that we were able to execute massively faster than I had thought possible.  

I was working with [Robin Minto](http://www.twitter.com/robinem) on the setup/provisioning side of things.  We spent our time in the excellent [Balsamiq mock-ups](http://www.balsamiq.com/), figuring out what the minimum required information was that we needed to set up a site, and constructing a workflow for the provisioning process.  We were later joined by [Ben Scott](http://www.twitter.com/bpscott), a web developer, who provided invaluable expertise in constructing these mock-ups.  

<a href=""http://codebork.com/sites/default/files/Setup%20UI.png"" title=""Large version of Setup UI mockup""><img src=""http://codebork.com/sites/default/files/Setup%20UI.png"" alt=""Setup UI mockup, a two-stage process"" title=""The two-stage GiveCRM setup process"" width=""700"" /></a>

<a href=""http://codebork.com/sites/default/files/Setup%20UI%202.png"" title=""Large version of Setup UI mockup with validation error messages.""><img src=""http://codebork.com/sites/default/files/Setup%20UI%202.png"" alt=""Setup UI mockup with validation error messages"" title=""The second step of the set-up process"" width=""700"" /></a>

<a href=""http://codebork.com/sites/default/files/Data%20Import%20UI.png"" title=""Large version of Data Import UI mockup""><img src=""http://codebork.com/sites/default/files/Data%20Import%20UI.png"" alt=""Data Import UI mockup"" title=""The data import UI."" width=""700"" /></a>

Whilst the two UI teams were discussing UX design, simplicity, etc., the database and Excel teams were beavering away working to create something we front-enders could build on the following day.  A big thanks to [Chris Diver](http://www.twitter.com/chrisjdiver), [Anthony Steele](http://www.twitter.com/AnthonySteele) and [Mark Rendle](http://blog.markrendle.net/) for getting us set up with a SQL database, dummy data to go in it, and a Simple.Data data access layer over the top, respectively; and to [Nathan Gloyn](http://designcoderelease.blogspot.com/) and [Johan Barnard](http://www.twitter.com/johanbarnard) for building a really nice, fully-tested API for working with Excel spreadsheets (and taking my improvement requests the following day).  Great work guys!

Around midnight, we retired to catch some precious sleep before starting in earnest shortly after 8am the next day.  

Saturday 22 October : ""We've resorted to using a physical token to 'lock' items in Git""
-----------------------------------------------------------------------

I woke up shortly before my alarm went off at 7am, dreaming about the solution we were going to implement and some of the problems we had to solve. No, really I did.  We arrived back at GiveCamp shortly after 8am, I swear the earliest time I have been at University in my life (even earlier than [DDDs](http://www.developerdeveloperdeveloper.com/)!).  What greeted us there, however, was a feast: bacon and sausage baps/batches/rolls, muffins, pastries, croissants, pains au chocolat, tea, coffee...  We sure weren't going to go hungry or thirsty this weekend!

By 9.30 or so, we'd divided up the development tasks amongst ourselves.  This was where things started to get interesting.  It soon became clear that my laptop was not happy and that my development VM was borked: no network connectivity in or out was going to make it very difficult indeed to work on anything at all (thanks, Parallels).  That would have been game over for me, had Kendall not stepped in and lent me his laptop for the weekend.  The guy is a legend.  

A larger problem for the team was our use of Git.  It was clear (unfortunately, only in hindsight) that we were using the wrong workflow for a distributed version control system, and everyone committing to master and pushing and pulling from the same remote repository resulted in us having to repeat a fair bit of work.  We probably lost a good couple of hours' work to our Git problems.  Robin had to repeatedly blow away his local repository and re-clone from the remote.  The main issue for us was conflicting changes in the csproj and sln files.  We ended up solving this in the time-honoured tradition of centralised version control systems: a MUTEX lock.  (This is how, in hindsight, it's obvious we were following the wrong workflow.)  Interestingly, at least two other teams had the same issue.  Git sadly got very little love that weekend.  

Another problem for us was conflicting changes to the database project (and the database itself).  This we solved more practically: Chris set up the database on his laptop, and Kendall ordered us an 8-port switch so we could all connect to Chris' database.  Less than an hour later, it arrived, and the problem was instantly solved.  

Lunch was a super-tasty burrito provided by [Luardos burrito van](http://luardos.co.uk/).  Check them out, their food was *really* good!

By about 3.00pm, after a singularly frustrating morning with little perceived progress, stuff suddenly started coming together.  The pace ramped up throughout the rest of the day: feature after feature slotted in to place.  Throughout the day, I was working on the import-from-Excel functionality that is a key part of the setup story.  I was quite happy with how this work progressed, as I was able to implement the feature with relatively good test coverage (not great, mind; something I'm now rectifying) and following best practices like dependency injection.  

That evening, Kendall found a wandering designer from [VerseOne](http://www.verseone.com/) to create us a logo:

<img src=""https://github.com/downloads/GiveCampUK/GiveCRM/GiveCRM-icon.png"" alt=""GiveCRM Logo"" title=""GiveCRM logo"" width=""700"" />

I later discovered that I had implemented the Excel importer in the wrong place: I had been thinking of it in terms of the setup process, so implemented it in GiveCRM's Admin site (where charities can sign up to GiveCRM, have their instance created, etc.).  In fact it's part of GiveCRM itself, where charities can add and manipulate their own data.  That was going to have to be a task for Sunday morning, if it got done at all.

Sunday 23 October : ""Down to the wire - I know because we're now saying 'nope, don't even try that.'""
---------------------------------------------------------------------------------

I woke up shortly before my alarm went off at 7am, dreaming about the solution we were going to implement and some of the problems we had to solve. This time, I also felt hungover (although no alcohol had been consumed since Thursday night): the caffeine and sleep deprivation was starting to take its toll.  

I rolled into UCL a little later than the previous day, at about 8.30am.  We were again greeted by the same breakfast feast as Saturday, and we soon agreed the concept of ""breakfast pudding"" was a Good Thing™.  

All development had to stop at midday Sunday, so we were now coding against the clock. The sugar and caffeine injection, combined with the adrenaline of working to this deadline, left me a little bit buzzy:

[I'm so tired, I woke up feeling hungover. I need the largest, strongest cup of tea ever right now. ](http://twitter.com/#!/alastairs/status/128006276365033472)
[Called in the big guns: two cups of tea at once.](http://twitter.com/#!/alastairs/status/128013278289985537)
[Chain drinking tea. It's all good.](http://twitter.com/#!/alastairs/status/128017338283991041)
[Phrase of the weekend, as coined by @nbarnwell: Breakfast Pudding.](http://twitter.com/#!/alastairs/status/128020255183355904)
[Feeling a bit *too* enamoured with this carrot cake muffin. The lack-of-sleep madness is starting to set in.](http://twitter.com/#!/alastairs/status/128022586339758082)
[I saw my two cups of tea and raised it a bottle of water.](http://twitter.com/#!/alastairs/status/128038996319285248)
[Busy busy busy. And buzzy buzzy buzzy.](http://twitter.com/#!/alastairs/status/128055270327853056)

I had a few bits to finish off with the import work, such as integrating it with the main site (tricky, given that it was in the wrong bit of the site) to produce a smooth demo, and add the final polish.  With regular progress checks throughout the morning, but ultimately with some time to spare, we were done: 

[...and that's it. End-to-end demo WORKS. I am so proud of the work we've completed this weekend.](http://twitter.com/#!/alastairs/status/128069253420167168)

We spent a bit of time practising our demo and taking team photos before lunch, which turned out to be a truly excellent hog roast.  Later, we moved across the road to UCL's Cruciform Building, a large red-brick building, for the demos.  

Everyone had made excellent progress with their projects over the GiveCamp weekend, and this came across in the demos presented.  Massive respect to the team working with the YouCan Hub for *going live* that weekend, and to Dylan Beattie's team working with Scene and Heard UK who completed the brief on **Friday night** and worked with the charity to build out a whole bunch of other stuff for them.

<a href=""http://www.flickr.com/photos/bertcraven/6273846394/in/photostream"" title=""The GiveCRM demo, on Bert Craven's Flickr Stream""><img src=""http://farm7.static.flickr.com/6096/6273846394_bfc3260a9e_z.jpg"" alt=""The GiveCRM demo"" title=""The GiveCRM demo"" /></a><br />
<span style=""font-size: small"">Photo copyright &copy; <a href=""http://www.flickr.com/photos/bertcraven/"">Bert Craven</a> 2011. Used with permission.</span>

Thank yous
----------

I've marked links in bold where I or my team used a product or service from that sponsor over the course of the weekend. An extra-special thanks to them!

### Sponsors ###
 * [EduServe](http://www.eduserv.org.uk ""Technology solutions that save money for the education, health and public sectors as well as commercial organisations."")
 * [SQLBits](http://sqlbits.com ""Over 200 hours of SQL Server video content from our past conferences, available free at SQLBits.com, featuring expert speakers from around the world."")
 * **[Gibraltar Software](http://www.gibraltarsoftware.com ""Gibraltar is an affordable, effective, end-to-end solution for logging, error management, and application monitoring"")**
 * [InFusion](http://www.infusion.com ""Infusion solves business problems by combining expert software engineering with appealing user experiences and design"")
 * **[Red Gate](http://redg.at/mnm95h ""Red Gate Software makers of simple tools for Microsoft developers, DBAs."")**
 * [FusionIO](http://www.fusionio.com/ ""The Fusion-io storage memory platform significantly reduces latency and improves the processing capabilities within a datacenter by relocating active data from centralized storage to the server where it is processed."")
 * [Black Marble](http://www.blackmarble.com/ ""Black Marble Software Consultancy and Development West Yorkshire. Covering Leeds, Bradford, Halifax, Huddersfield, West Yorkshire and the North of England."")
 * [JustGiving](http://www.justgiving.com ""JustGiving (www.justgiving.com) is the UK’s largest online fundraising platform and has helped 12 million people raise &#163;700 million for more than 9,000 charities since 2001. In 2009, JustGiving’s CEO Zarine Kharas was awarded the RSA’s Albert Medal for “democratising fundraising and technology for charities” and JustGiving was awarded the &quot;best use of technology award&quot; at the Sunday Times Tech Track 100 Awards."")
 * [DevExpress](http://www.devexpress.com/ ""Save time and money with high quality pre-built components for ASP.NET, WinForms, WPF, Silverlight and VCL as well as IDE Productivity Tools and Business Application Frameworks, all backed by world-class service and support. Our technologies help you build your best, see complex software with greater clarity, increase your productivity and create stunning applications for Windows and the Web in the shortest possible time. Learn more at www.devexpress.com."")
 * [CounterSoft](http://www.countersoft.com ""Since 2003 Countersoft has been about enabling the collective capability that exists within all teams and projects"")
 * **[JetBrains](http://www.jetbrains.com ""JetBrains :: World&#39;s Leading Vendor of Professional Development Tools"")**
 * [Telerik](http://www.telerik.com ""Telerik is a market-leading provider of UI controls and components for ASP.NET AJAX, ASP.NET MVC, Silverlight, Windows Phone (WP7), WPF and Windows Forms, as well data access layers, reporting, code analysis and refactoring tools across all major Microsoft development platforms. Combined with its automated software testing and agile project management tools as well as its content management system, Telerik offers an end-to-end solution for building applications with unparalleled richness, responsiveness and interactivity. Telerik products help thousands of companies be more productive and deliver reliable products under budget and on time."")
 * [Quest](http://www.quest.com/ ""Quest Software (Nasdaq: QSFT) simplifies and reduces the cost of managing IT for more than 100,000 customers worldwide. Our innovative solutions make solving the toughest IT management problems easier, enabling customers to save time and money across physical, virtual and cloud environments.  For more information about Quest solutions for application management, database management, Windows management, virtualization management, and IT management, go to www.quest.com. "")
 * [ThoughtWorks](http://www.thoughtworks.com ""ThoughtWorks, Inc. is a global IT consultancy providing Agile-based systems development, consulting, and transformation services to Global 1000 companies. It has pioneered many of the most advanced and successful Agile methods of software development and best practices used in the industry today. At its core, ThoughtWorks helps its clients maximize investment and performance across a portfolio of complex, business-critical applications, while reducing time and risk. Its products division, ThoughtWorks Studios, offers tools to manage the entire Agile development lifecycle through its Adaptive ALM solution™, comprised of Mingle&#174;, Go&#174; and Twist&#174;. ThoughtWorks employs 1,700 professionals to serve clients from offices in Australia, Brazil, Canada, China, Germany, India, the United Kingdom and the United States. For more information, please visit www.thoughtworks.com."")
 * [Kentico](http://www.kentico.com ""Kentico CMS is a versatile, extensible platform that enables you to deliver the latest Web technology to customers of all sizes. Content-oriented sites, community sites, online stores and intranets—Kentico can do it all. Use Visual Studio and your .NET Framework skills to accelerate the development process. Add out-of-the-box modules that will wow your customers with advanced functionality. Customize and integrate the site with other systems using fully documented open APIs. And enjoy a streamlined deployment experience that is second-to-none. Kentico is designed to maximize your productivity, so you can work smarter not harder. If you are looking for a Web development platform that delivers immediate results while supporting long-term business growth, try Kentico CMS."")

### Contributors ###

I've marked links in bold where I or my team used a product or service from that contributor over the course of the weekend. An extra-special thanks to them!

 * [PostSharp](http://www.sharpcrafters.com/)
 * **[LeanKitKanban](http://www.leankitkanban.com/)**
 * [InformIT](http://www.informit.com/index.aspx)
 * [AppHarbor](http://www.appharbor.com)
 * [Pusher](http://www.pusher.com/)
 * [PluralSight](http://www.pluralsight-training.net/microsoft/)
 * [LogicNP](http://www.ssware.com/)
 * **[HasBean](http://www.hasbean.co.uk)**
 * **[TeaPigs](http://www.teapigs.co.uk)**
 * [SqlSentry](http://www.sqlsentry.com/)
 * [TX Text Control](http://www.textcontrol.com/)
 * [O'Reilly](http://www.oreilly.com/)
 * **[Dominos Pizza](http://www.dominos.co.uk/)**
 * [TypeMock](http://www.typemock.com)
 * [DiscountASP](http://discountasp.net/)
 * [SaaS Made Easy](http://www.saasmadeeasy.com)
 * [BeanStalk](http://beanstalkapp.com/)
 * [Heroku](http://www.heroku.com)
 * [SyncFusion](http://www.syncfusion.com/)

### People ###

A **MASSIVE** thank you to <a href=""http://www.paulstack.co.uk/"">Paul Stack</a> and <a href=""http://www.twitter.com/rachelhawley"">Rachel Hawley</a> for being mad enough to run this event, and all their efforts organising GiveCampUK; it is a truly phenomenal achievement.  

Huge thanks also to the GiveCampUK assistants, <a href=""http://www.twitter.com/plip"">Phil Winstanley</a>, <a href=""http://www.twitter.com/apwestgarth"">Andy Westgarth</a>, <a href=""http://www.twitter.com/davesussman"">Dave Sussman</a> and <a href=""http://www.twitter.com/lethrir"">Kim Richmond</a> for their efforts in ensuring everything ran smoothly, resolving problems.

UCL's loan of their building for the weekend was hugely appreciated, in spite of the various wireless connection issues.  As Paul mentioned, no other venues were willing to offer us a space for free, which UCL very kindly did.  Fabulous generosity from them.  

Most of all, a big thank you to the GiveCRM team for being such a great bunch of people to work with!

<a href=""http://www.flickr.com/photos/bertcraven/6273304619/in/photostream"" title=""The GiveCRM team, on Bert Craven's Flickr Stream""><img src=""http://farm7.static.flickr.com/6105/6273828732_f67b4b3ce4_z.jpg"" alt=""The GiveCRM team"" title=""The GiveCRM team"" /></a><br />
<span style=""font-size: small"">L-R: <a href=""http://www.twitter.com/anthonysteele"">Anthony Steele</a>, <a href=""http://www.twitter.com/lethrir"">Kim Richmond</a>, <a href=""http://www.twitter.com/chrisjdiver"">Chris Diver</a>, <a href=""http://www.twitter.com/saqibs"">Saqib Shaikh</a>, <a href=""http://www.twitter.com/robinem"">Robin Minto</a>, <a href=""http://www.twitter.com/kendallmiller"">Kendall Miller</a>, <a href=""http://www.twitter.com/adrianbanks"">Adrian Banks</a>, <a href=""http://www.twitter.com/alastairs"">me (Alastair Smith)</a>, <a href=""http://www.twitter.com/johanbarnard"">Johan Barnard</a>, <a href=""http://www.twitter.com/markrendle"">Mark Rendle</a>, <a href=""http://www.twitter.com/nbarnwell"">Neil Barnwell</a>, <a href=""http://www.twitter.com/nathangloyn"">Nathan Gloyn</a>.  Photo copyright &copy; <a href=""http://www.flickr.com/photos/bertcraven/"">Bert Craven</a> 2011. Used with permission.</span>

Conclusion
----------

This was the first ever GiveCampUK, and a number of people I have spoken to about it in the last few months since it was announced indicated an interest but a reluctance to take part, borne I think out of not knowing what the experience would be like.  I started this blog post by saying that GiveCamp UK was one of the greatest experiences of my life, and **I stand by that statement wholeheartedly.**  I hope that, with the inaugural event behind us, those people interested in it this time around but that didn't sign up will have a better idea of what to expect and might be more willing to sign up next time.  From the excellent organisation of the event, to the chance to work with some top developers and interesting technology, to the massive sense of achievement and the pride in the work completed, this was utterly unique and electrifying and inspirational.  

**See you at GiveCampUK 2012.**

Bootnote
-------

[GiveCRM](http://www.givecrm.org.uk/) is under active continued development. You can [fork us on GitHub](http://www.github.com/GiveCampUK/GiveCRM), follow us on Twitter ([@GiveCRM](http://www.twitter.com/givecrm), and [""Like"" us on Facebook](http://www.facebook.com/givecrm).  We're in the process of putting together a coherent plan to take the project forward in a more structured way; keep an eye on the Twitter account for details!"
Git: Rescuing changes that have been merged over,,"So we ran into a little bit of bother with some pull requests to [GiveCRM](http://www.givecrm.org.uk/): I had submitted a feature that had been merged in, but it was subsequently lost (in part) when a later pull request was merged in.  It seems the submitter of the later pull request hadn't updated from the central repository before submitting his pull request, so when his pull request was merged my feature was overwritten.  

My experience with Git is almost entirely as an individual developer, so I'm learning how to use it in a distributed team as we go with GiveCRM.  I turned to my friend, colleague and resident Git expert [Adam Wood](http://www.twitter.com/adamwood80) for some advice.

<!--break-->

This is what the graph of commits looked like

                 o----------------o----------
                /                            \
    -----+--+--+-------+------------------+-+-+----
                        \                  /
                         x-------x--x-x-x--

The `+`s are master; the `x`s are my feature's branch, and the `o`s are the other branch.  

I wondered initially if it might be resolvable by taking a branch at the point at which I finished my feature and rebasing that on master, but this didn't work, as might be expected.  We tried a few other things, like revert and reset --soft, but eventually we worked out we could do the following, which was rather simpler.  The following commands are what I did; there's probably a quicker/easier way to do some of them.

    git checkout 12345abc

This puts you in ""detached HEAD"" mode at the given commit. This basically means, as far as I can tell, that the repository contents are at the specified commit, but you can't make any changes to the repository and expect to keep them: git's internal reference tracking is sort of ""turned off"".  (Hopefully someone can explain that better than me!)

    git checkout -b FeatureRescue

This creates a branch from the current detached HEAD state of the repository.  There's probably a `git branch FeatureRescue 12345abc` that I could have done instead, but whatever.

    git checkout -b integration

Create an integration branch to mess around in - you don't want to lose the branch you've created where the feature is still intact!

    git merge abcdef123

Merge the commit(s) that overwrote the feature.  I was lucky that the pull request only contained one commit.

    git mergetool

Fix up the merge conflicts that should have arisen when the pull request was merged in...

    git commit -m ""Re-apply [x] change""

Commit the merge to the integration branch.  

    git checkout master
    git merge integration

Switch back to master, and merge in the integration branch.  Sorted!

Not *quite*.  I ran into the unfortunate situation where there was another file that was correctly updated in the integration branch, but didn't get merged into master at the end.  This, I think, was because the file was not changed on master after my initial pull request.  I had to apply this change manually, but that was thankfully trivial.  

If you know a simpler way to accomplish this, please leave me a comment!"
Introducing GiveCRM,,"Compulsory opening statement: It's been a little while since I last blogged, and there's a level of irony in that fact because now I have rather more to blog *about* than I did before.  In the last few months I've been working with a small handful of guys from my [GiveCampUK](http://www.givecamp.org.uk/) team to launch [GiveCRM](http://www.givecrm.org.uk/).  Whilst we were all quite fired up to be working on it over the course of the GiveCampUK weekend, not everyone wanted to carry on with it beyond that event, and not everyone has had time to dedicate to it.  Such is the nature of open source, I guess.  
<!--break-->
## What is GiveCRM?
 
GiveCRM is a Customer Relationship Management (CRM) system for charities and other non-profit organisations.  It has a focus on exceptional ease of use, and is designed explicitly for the needs of the non-profit sector.  Using GiveCRM, you can manage details of donors and other members of your organisation, record donations made, track campaigns to drive donations and see what donations resulted from which campaign.  **Note:** we do not enable accepting donations online, only the recording of the donated amount against a particular person.

We're building on ASP.NET MVC 3, with plans to move to MVC 4 in the near future.  The code is all C#, and we've used Twitter Bootstrap to develop v1 of our user interface, and Simple.Data to handle our data access.  We're developing a Software-as-a-Service (SaaS) product with a monthly fee to cover our hosting costs, but we also provide a standalone version of the application for charities who need or wish to deploy a private instance.  

You can find and fork the source code on [GitHub](http://www.github.com/givecampuk/givecrm/).  Send us a Pull Request for one of our [open bugs or features](http://www.github.com/givecampuk/givecrm/issues).

## Why are you blogging about this now?

Development has picked up again recently on the project, and we've hit a couple of interesting design points along the way.  Chief among these is our multi-tenancy strategy for the SaaS product, where we host multiple customers on one box.  I'll be talking about these more in future posts.  "
DDD South West 4,,"DDD South West 4 ran a couple of weeks back, and I was lucky enough to be amongst the attendees.  Here's a round-up of my experience.

<!--break-->

I was travelling to DDD South West 4 with my friend and colleague [Adrian](http://www.twitter.com/adrianbanks); this was our second DDDSW, and after last year decided that it would be better to catch the train to Bristol rather than driving as we did last year; in another change from last year, we also decided to book into the speakers' hotel which turned out to be a good choice.  Like last year, the event was held at the University of West of England, a little way outside Bristol.  UWE proved to be a good venue, if a bit warm due to the air conditioning being turned off (on the warmest day of the year so far).  

The journey to Bristol from Paddington was pretty horrible: we'd decided not to book seats on the train, which was packed.  Even the corridors were rammed with people standing all the way there.  I will, in future, be reserving seats on longer train journeys!  We didn't make it in time for the pre-event dinner on the Friday night, so we checked into the hotel and retired to the pub over the road, the rather good Fox Den, where we met a few other attendees.  After the Fox Den closed later that evening, we moved to the hotel bar, found a larger group of attendees, and stayed up for a while chatting. 

The event itself began with an early start, a disappointing breakfast at the hotel, and a wander over to UWE.  We arrived in plenty of time to pick up our swag bag and register for the post-event meal, and went in search of the coffee and pastries.  

After a brief welcome and introduction from über-organiser [Guy](http://www.guysmithferrier.com), I went to my first session, [Chris Hay](http://www.twitter.com/chrishayuk)'s *Redis the New Black* on the repeat track. Chris gave us a clear and thorough overview of Redis, and was as engaging and interesting a speaker as always; highlights included the section he presented from the floor, and the story about the time he nearly got killed by the mafia.  Redis seems like a cool technology, and I'll keep it in mind for the future if I find I need a separate caching store.  

Next up, I went to [Tim Gaunt](http://www.twitter.com/timgaunt)'s session, entitled *Taking it from Side Line to Full Time*, figuring it would be useful for [GiveCRM](http://codebork.com/2012/06/04/introducing-givecrm.html).  Tim presented very well on the nuts and bolts of kicking off a start-up, and I'm glad to say that it seems we're doing many of the right things for GiveCRM based on his experiences.  

After the coffee break, I went along to [Ashic Mahtab](http://www.twitter.com/ashic)'s *Clean up your JavaScript Act*.  Ashic spoke clearly on using [QUnit](http://docs.jquery.com/QUnit), a JavaScript unit testing framework, in [TeamCity](http://www.jetbrains.com/teamcity/) builds, and how [Knockout.js](http://knockoutjs.com/) can reduce the amount of code that you need to write, whilst also making your front-end JavaScript more testable through the application of the <abbr title=""Model-View-ViewModel"">MVVM</abbr> pattern.  

After collecting our mahoosive lunches featuring sandwiches, cornish pasties, muffins, and more, Adrian and I headed back down to the main room for the lunchtime lightning presentations and grok talks.  There was an interesting and highly varied array of presentations, on topics from [AppHarbor](http://www.appharbor.com/), through to a masters degree programme (...), to some guy's holiday snaps!  To be fair, being a geek, he had also used his round-the-world tour to document the WiFi provision in every corner of the globe.  

After lunch was [Seb Lambla](http://www.twitter.com/serialseb)'s talk on Web Caching. This was an interesting session, as it was presented entirely from the perspective of HTTP, so dug into the nuts and bolts of the various locations and types of cache that might be between you and the requested website, what the various HTTP headers mean (hint: generally the opposite of what they say), and which headers you need to send to support different scenarios.  

Next it was the famous cream tea break, although I had to skip the food as I was still stuffed from lunch!  
The final session I attended was *NoSQL Document Databases: Why Would You Use One?*, delivered by [Ian Russell](http://www.twitter.com/ijrussell/), a first-time speaker at a DDD event. Ian spoke well, and knew the subject matter, but his presenting style didn't seem to sit well with his audience; speaking to him later that week, at the [Progressive .NET Tutorials](http://codebork.com/2012/06/05/progressive-net-tutorials-2012.html), it turns out he was expecting some feedback and dialogue with the audience, which wasn't really forthcoming from that particular group of people, and what discussion there was got bogged down in something of a SQL Server-vs-NoSQL debate.  

Thank you to all the sponsors who made DDD South West 4 possible, in particular Gibraltar Software for sponsoring the morning coffee and danish, and eDevelopment for putting on the afternoon Cream Tea!  Also particular thanks must go to JetBrains as well, for supplying the dotTrace licence I won at the swag giveaway at the end!"
Cambridge Software Craftsmanship Community,,"Last night, a new community was formed in Cambridge. Nearly 20 software developers congregated for the inaugural meeting of the [Cambridge Software Craftsmanship community](http://www.meetup.com/Cambridge-Software-Craftsmanship/), and the feedback on the event has been excellent.  

I've been attending the [London Software Craftsmanship Community](http://www.meetup.com/London-Software-Craftsmanship/) (LSCC) for the better part of a year now, and have thoroughly enjoyed the events that [Sandro](http://www.twitter.com/sandromancuso) and [Samir](http://www.twitter.com/samirtalwar) put together there.  The group has gone from strength to strength, and is now the largest Software Craftsmanship community outside of the US.  

About six months ago, I toyed with the idea of starting a similar group in Cambridge.  I put out a call on Twitter, but had little response so put the idea on the back-burner for a little while.  Then, whilst attending a couple of LSCC events in April, a few of the practitioners there suggested I start up a more local group.  I put out another, firmer, call on Twitter, and this time the response was much more favourable: the seeds had been planted.  With people like Sandro, [Jason Gorman](http://www.twitter.com/jasongorman) and [Dave Green](http://www.twitter.com/activelylazy) (a former LSCC organiser) offering support and encouragement, I knew this idea had legs.  

And then, for a while, it stopped. I spent most of May in the US (those blog posts are still coming...) and everything ground to a halt.  The momentum was not lost, though: even whilst I was away, the group was at the back of my mind, gestating.  I knew that I wanted to model the group on the LSCC, to try and replicate its success, but how much did I want it to be a cookie-cutter group?  I decided to adopt two of the meeting formats that LSCC run, the round-table discussion and the hands on sessions as the initial programme.  On my return to Cambridge, I signed up to [Meetup](http://www.meetup.com/) as a group organiser and put a little bit of flesh on the bones of the group: we now had a place to congregate online.  

A month ago, I officially launched the group, advertising it on Twitter, and taking advantage of the free promotion through Meetup to similar groups.  I was blown away by the response: within two days, 15 out of the 20 places to the first meeting, a round-table discussion session, and by the end of that week we had hit 30 members!  Clearly the group had struck a chord.  

Next I had to arrange some sponsorship: a key ingredient of the round-table discussions is free pizza, and feeding 20 hungry developers is not cheap!  I've been injecting the craftsmanship ethos into my team at work for a while now, so I wanted to give my employer, [Granta Design](http://www.grantadesign.com/), first refusal on the opportunity.  I'm very happy to say that we are sponsoring the group both through providing a space for the group to meet at our offices near the station, and in providing the pizza and drinks for the evening.  

The day of the meeting soon came around, and there was a lot to do: buy the necessary supplies, book the pizza, set up the space, etc. Thank you to Ceri, Adrian and Sean for their help beforehand and to them and everyone else who helped clear up afterwards too!  We had a few introductions whilst waiting for the pizza to arrive, and some small talk over pizza.  The round-table discussion itself kicked off a little while later, and this took the format of an open space-style discussion: topics were provided by the participants, and then the group voted on the ones they wanted to discuss.  We then broke into two groups to discuss Functional Programming, and Legacy Code.  

Both discussions seemed very fruitful: I participated in the Legacy Code discussion, and took away some new information on tools that can be used with C++ for static analysis, unit testing, and code coverage.  The Functional Programming group came back very enthused, with one member even saying they wanted another one!

It was also interesting listening to the particular problems that developers working with C and C++ run into, especially those in embedded software.  There does seem to be less available in the way of tooling for C and C++ developers than there are for managed languages like C# and Java; I guess this is just the nature of the beast when you're writing code that runs on the metal itself rather than inside a runtime.  

After the round-table discussions, everyone re-grouped in the main area and we had a little bit of a discussion on the expected programme for the group.  This is a summary of my short-term plan:

  1. Continue to model the group on the LSCC.  Run round-tables on a monthly basis, on the first Tuesday of each month.  
  2. In a month or two's time, introduce a monthly hands-on session.  These will be a range of exercises, mostly formed around the Code Retreat model of pair programming with Test-Driven Development.  
  3. Later, start to introduce more social meetups.  In the meantime, I will try to include more of this in the regular meetups, such as retiring to a local pub afterwards.  
  4. Also later, introduce day-long events, perhaps joining the Global Day of Code Retreat, or running a Legacy Code Retreat. 

However, these are just my ideas, based on where we are now; as the group grows and evolves, we'll adapt the programme to suit the group.  The monthly hands-on sessions in particular I am keen to open to both members of the group to lead, and to bring in luminaries from the Craftsmanship movement to lead as well.  

In the meantime, our next meetup will be another round-table discussion, to be held at Granta Design on Tuesday 7 August.  Keep an eye on the Cambridge Software Craftsmanship website, and the [#CamSWCraft](https://twitter.com/#!/search/%23camswcraft) hashtag on Twitter for more information!

I'm really looking forward to seeing where the group goes from here.  We've had an auspicious start, and the feedback from last night's event suggests we have a group of developers passionate about their craft on our hands.  Exciting times!"
Azure Websites: To the Cloud!,,"When [Scott Guthrie](http://www.twitter.com/scottgu) and team unveiled the new Windows Azure updates back at the beginning of June, one of the most trumpeted features was the new Azure Websites feature, which brings to the Azure platform a service provided by [AppHarbor](http://www.appharbor.com/) and [Heroku](http://www.heroku.com/) (for the Rubyists): simple hosting of websites on a cloud platform.  One of the killer features of these platforms, and now Azure Websites, is simple deployment to the cloud via git push.  AppHarbor in particular does a good job of this in the .NET space, building your project and running the tests before deciding whether or not to deploy that commit.  They have some serious competition from Azure now, though.  

<!--break-->

I'm using Azure Websites to develop (and, ultimately, host) the new [Cambridge Graduate Orchestra](http://www.cambridgegraduateorchestra.com/) website, and have so far found the experience impressive.  Whilst I'm familiar with Git-based deployment from previous usages of both AppHarbor and Heroku, I'm pleased to see Microsoft actively supporting more open-source tools and technologies in its products and services.  Whilst the Azure Websites service is not as mature as AppHarbor's offering, the combination with other Azure features, such as the Azure Service Bus and the new Infrastructure-as-a-Service bits, makes it a compelling option.  

The first thing you'll need to do is log in to (or sign up for) a free trial of Azure.  Once you've logged in, go to the Azure Portal; if you get the old grey portal, look for and click the link to the new preview portal at the very bottom of the screen.  

Next create a new Azure Website. Click new:

<img src=""http://codebork.com/sites/default/files/Create%20Website.png"" alt=""The new Windows Azure Portal, illustrating how to create a new Azure Website"" title=""Create a new Azure Website"" height=""540"" />

After you hit ""Create Web Site"", you'll see the new website appear in the list as configuring; it will say ""Running"" when it is ready for use, and you'll see a green tick next to it (like the CGO site in the above screenshot).  

Next, click the site to be taken to its dashboard.  This is where you can view all sorts of metrics about your site, including requests, storage, CPU time and more.  Click the link that says ""Set up Git publishing"":

<img src=""http://codebork.com/sites/default/files/Dashboard.png"" alt=""The new Windows Azure Portal, illustrating where to click to set up Git publishing."" title=""Set up Git publishing"" height=""540"" />

This will prompt you for a username and password to use for connecting to Azure via Git.  This is created on a <em>per-account basis, not on a per-site basis</em>, so you only need to do this once.  You'll then get a Git URL with some instructions on how to use it:

<img src=""http://codebork.com/sites/default/files/Git%20instructions.png"" alt=""The new Windows Azure Portal, illustrating the instructions provided to help you set up your local Git repository for publishing to Azure."" title=""Git instructions for publishing"" height=""540"" />

Add a new remote to your existing Git repository, and run `git push azure master`. Simplez.  At the moment, only the master branch is supported for deployment; if you are using a different branching convention, you will receive an informational message stating this.  

The <strong>really nice thing</strong> about this is that after Azure builds your pushed repository, it will generate an MSDeploy package from it, and run a deployment using that package.  That means you can take advantage of all the nice things, like configuration file transforms, that you get with MSDeploy.  

I'm really impressed with the job Microsoft has done here, and look forward to delving more into this in the coming weeks."
Project Kudu: Git deployment for all!,,"What is Project Kudu? Quite simply, it's the new [Continuous Delivery and Deployment](http://continuousdelivery.com/) hotness in the .NET world. I'm frankly amazed more people haven't yet jumped on this, because this has the potential to revolutionise how people do deployments internally and externally. [Project Kudu](https://github.com/projectkudu/kudu) is the framework underlying the Git deployment feature of the new Azure Websites, but you can use it separately from Azure, and, best of all, it's open-sourced under the Apache 2.0 licence!

<!--break-->

My [previous post](http://codebork.com/2012/07/06/azure-websites-cloud.html) focussed on deploying to Azure with Git. What I didn't mention there was that this was making direct use of Project Kudu under the covers, and that you can get the same features in your own environments! This post will describe how to get Kudu running locally and some ideas on integrating it into your CI process for continous deployment.  

Project Kudu is primarily developed by two Microsofties, [David Ebbo](http://www.twitter.com/davidebbo) and [David Fowler](http://www.twitter.com/davidfowl), with solid open-source credentials in the .NET world: David Ebbo is also part of the core NuGet team, and David Fowler put together another awesome bit of tech, [SignalR](http://www.github.com/davidfowl/signalr).  The platform they've built for Kudu is pretty solid, smartly leveraging IIS management functionality provided through the [Microsoft.Web.Administration package](http://nuget.org/packages/Microsoft.Web.Administration).  

Unfortunately, it's a little rough around the edges. Clearly it has been built with Azure in mind, and there are a *lot* of customisations that have been built on top of Kudu to build Azure, not least much of the Azure UI: Kudu's own UI is best described as ""minimal"".  Interesting to see it's been built with [Twitter Bootstrap](http://twitter.github.com/bootstrap/), though.  However, the biggest issue with Kudu currently is the lack of documentation on getting it up and running: the wiki is almost entirely Azure-oriented, and the [Getting Started](https://github.com/projectkudu/kudu/wiki/Getting-started) page can be summarised as ""get the source code, open it in Visual Studio running as an Administrator, and hit F5"".  I will contribute the relevant bits of this blog post back to the project's wiki, once I've worked out how to do so!  

## Setting up a Kudu environment

So, the initial stages are as simple as the wiki describes.  You do have to be careful to run Visual Studio as an Administrator, however, as all the clever stuff it does in talking to IIS requires administrative privileges.  Running Kudu through Visual Studio will give you something to play with, but it won't work for production deployments, obviously.  This is where things start getting a little bit trickier.  

There is a Kudu.Setup.sln in the Git repository, which purports to contain a WiX project for building an MSI for Kudu.  Unfortunately, it seems unfinished: I built the MSI, ran it and nothing happened at all as far as I could see. No UI popped up or anything.  I quickly decided to go for a manual deployment instead.  

Looking at the output generated by the command-line build (run the build.cmd file included in the repository), I noticed that there were two components generated.  The first was the Kudu front-end, an ASP.NET MVC web app; the second was Kudu's back-end service, which implements Git's [Smart (HTTP) Protocol](http://git-scm.com/2010/03/04/smart-http.html) and handles the back-end git operations and site deployment. Remember in my last post how I said that *Azure builds an MSDeploy package and deploys it to your site*, so you can take advantage of the configuration file transforms and other MSDeploy goodness? **That's all handled by Kudu!**

I set up a virtual machine to deploy Kudu onto; thankfully, it doesn't require much: IIS 7.0 or greater, ASP.NET 4.0, and Git installed in `C:\Program Files (x86)\Git\bin`, so it was really easy to get the VM configured. The first two of these can be installed and configured by the [Web Platform Installer](http://www.microsoft.com/web/downloads/platform.aspx), which makes it simpler still!

First, I created an AppPool for Kudu's website to run in.  I did this because Kudu has to run as an administrator, and I didn't want to go changing the default AppPools.  

<img alt=""Right-click the Application Pools node in the tree and choose Add Application Pool"" src=""http://codebork.com/sites/default/files/IIS.Add_.Application.Pool_.png"" title=""IIS Add Application Pool"" /> 
<img alt=""Give the new Application Pool a name, and set it to run under ASP.NET 4.0 in Integrated mode"" src=""http://codebork.com/sites/default/files/IIS.Create.Application.Pool_.png"" title=""IIS Create Application Pool"" />

Now set the AppPool user identity to an administrative user.  I just set the AppPool user identity to be that of the local Administrator user on the machine, but this is a BAD THING™ so **please create a user in the administrators group specifically for the Kudu AppPool!**

<a href=""http://codebork.com/content/iis-apppool-advanced-settings"" title=""Click to enlarge""><img alt=""Select the new Application Pool in the list, and click Advanced Settings"" src=""http://codebork.com/sites/default/files/IIS.AppPool.Advanced.Settings.png"" title=""IIS AppPool Advanced Settings"" width=""700"" /></a> <img alt=""Change the AppPool Identity to an administrative user"" src=""http://codebork.com/sites/default/files/IIS.AppPool.Identity.png"" title=""IIS AppPool Identity"" />

Next, I copied the two build artifacts to directories under inetpub and created a Web Application in the Default Web Site pointing to the location I had copied the front-end to:

<img alt=""Right-click the Default Web Site node in the tree and choose Add Application"" src=""http://codebork.com/sites/default/files/IIS.Add_.Application.png"" title=""IIS Add Application"" /> <img alt=""Fill out the application alias (name), select the AppPool that we created earlier, and set the physical path to the appropriate location for your Kudu binaries."" src=""http://codebork.com/sites/default/files/IIS.Create.Application.png"" title=""IIS Create Application"" />

I now had a website I could hit.  Unfortunately, there's a mismatch between the Kudu config file, and the artifacts churned out by the build: Kudu's web.config file contains a directive for locating the Kudu back-end service, which defaults to `..\Kudu.Services.Web`, whilst the artifact is output to `..\KuduService`.  You can update the config file or rename the directory as suits your own tastes. Whilst there, I also noticed that the web.config defines a path to store all the sites deployed; I decided to create this directory as well, just in case.

<img alt=""Screenshot showing the AppSettings for setting the Service site location and the location for deploying sites locally."" src=""http://codebork.com/sites/default/files/Kudu.Web_.Config.png"" title=""Kudu AppSettings"" width=""700"" />

**UPDATE:** David Fowler pinged me on Twitter to say that Windows Authentication shouldn't be required.  After a bit of investigation, I found that he is of course correct. I think the error was caused by an unrelated configuration setting: it took me a while to get IIS set up properly on my local machine whilst writing this post.  As such, you can skip these steps describing how to enable Windows Authentication.

<p style=""text-decoration:line-through"">When I went to the URL, the first thing I got was an error.  By default, Kudu expects to use Windows Authentication, which I hadn't configured.  I decided to enable it at the server level so that other websites could take advantage of it; this turned out to be a prudent choice when I later noticed that Kudu creates a new site in IIS for each application it manages.  I also had to disable Anonymous authentication for the Kudu application.</p>

<img alt=""Click the node in the tree with the same name as your computer, then double-click Authentication"" src=""http://codebork.com/sites/default/files/IIS.Server.Authentication.png"" title=""IIS Authentication settings"" /> <img alt=""Right-click Windows Authentication and choose Enable"" src=""http://codebork.com/sites/default/files/IIS.Enable.Windows.Authentication.png"" title=""Enable Windows Authentication"" />

We now have a working Kudu environment within which we can create applications for deployment.  By clicking the Admin link in the top right corner, we can verify that it has picked up the back-end service site correctly:

<img alt=""Kudu's &quot;Settings&quot; page lists the physical locations of the Service site and the applications directory"" src=""http://codebork.com/sites/default/files/Kudu.Admin_.Settings_0.png"" title=""Kudu Admin Settings"" width=""700"" />

Provided you see that nice green ""ok"" label, you're good to go and create your first application!  Go ahead and click ""Create Application"". This is where the fun starts!

<img alt=""Simply fill out the name of the application and click &quot;Create application&quot;!"" src=""http://codebork.com/sites/default/files/Kudu.New_.Application_0.png"" title=""Kudu New Application"" width=""700"" />

After you create the application, you'll be given a Git URL to which you can push your repository.  You'll notice that the generated URLs all point to localhost.  

<img alt=""Kudu Application Settings: your generated URLs for Git deployment and viewing the deployed application, and a big red button to delete the application"" src=""http://codebork.com/sites/default/files/Kudu.Application.Settings.png"" title=""Kudu Application Settings"" width=""700"" />

You can fix this on a per-application basis by going into the IIS Manager and altering the bindings for the created sites, but there doesn't seem to be a way at the moment to correct this for all sites; even setting the binding on Kudu site itself doesn't have the hoped-for effect.  

<a href=""http://codebork.com/content/iis-set-site-bindings"" title=""Click to enlarge""><img alt=""Select the site in the tree and click Bindings in the Action pane"" src=""http://codebork.com/sites/default/files/IIS.Set_.Site_.Bindings.png"" title=""IIS Set Site Bindings"" width=""700"" /></a> 
<img alt=""Select the default bindings in the list and click Edit"" src=""http://codebork.com/sites/default/files/IIS.List_.Site_.Bindings.png"" title=""IIS List Site Bindings"" /> <img alt=""Set the Host name to the fully-qualified domain name of the machine"" src=""http://codebork.com/sites/default/files/IIS.Edit_.Site_.Binding.png"" title=""IIS Edit Site Bindings"" />

As yet we have no deployments to the application, so the list of deployments is empty.  However, with a `git remote add staging [generated Git URL]` followed by a `git push staging master`, we can now easily deploy our application to our staging site. (You can of course call your git remote something different from ""staging"" to suit your situation.)  With the first push done, we now have a deployment listed:

<img alt=""Our first deployment: a green &quot;Success&quot; stamp, and a tick"" src=""http://codebork.com/sites/default/files/Kudu.Application.Deployments_0.png"" title=""Kudu Application Deployments"" width=""700"" />

I was a little disappointed to find that the list of deployments doesn't automatically refresh the way it does in the Azure portal.  It's another one of those customisations built on top of Kudu that I mentioned earlier.  I suspect with a bit of SignalR or something it wouldn't be too hard to put together a pull request adding this feature...

## Kudu and TeamCity

So, we've brought the fun; now let's bring the awesome.  You can easily wire this into TeamCity.  

Before I get onto how I set up Continuous Delivery using Kudu for a project at work, I should note that we're using the [Git-Flow branching model](http://nvie.com/posts/a-successful-git-branching-model/) on this repository; this is where the `develop` and `master` branches mentioned below come from.  Git-Flow is outside the scope of this blog post, so hit the link to read more.  

Here's the TeamCity configuration I used to create our build pipeline from source code to deployment:

  1. Create a build configuration to build your application and run the unit and integration tests. Configure this as suits your application.
  
  2. Create a build configuration to deploy your application to your Kudu environment and run your acceptance tests:
    1. Create a build step for the deployment. I used a PowerShell build step, but this could also be a command-line build step or similar. The PowerShell script I used was `git push [generated Git URL] develop:master`. Tough, innit?  We push the local `develop` branch to `master` on Kudu here, because `develop` has the most up-to-date changes in this branching model, but Kudu only supports deploying the master branch.
    2. Create a build step for running your acceptance tests. Configure this as suits your application and tests.
    3. Trigger this build using a Finish Build trigger on your build + test configuration. Be sure to only run this build on successful builds from the build + test configuration.  
  3. Create a build configuration to update your master branch.  Again, I used the PowerShell runner.  This script is a bit more involved, but still quite trivial:
  
        git fetch origin
            
        # check out the master branch, creating it off origin/master if it doesn't 
        # already exist
        git checkout -B master origin/master 
            
        # fast-forward merge develop into master. Under git-flow, this should always 
        # be a fast-forward
        git merge --ff develop 
            
        git push [writeable URL for repository] master

    Again, be sure to trigger this build using a Finish Build trigger on the deploy and acceptance test build, selecting successful builds only.

<p style=""text-decoration:line-through"">The `try..finally { exit }` wrapping the script is required to ensure that the build completes; without it, PowerShell doesn't properly report an exit code, TeamCity has no way of knowing its finished, and so the build hangs. Useful&hellip;</p> This turned out to be due to the fact that msysgit installs both a .cmd and .exe on Windows, and the .cmd version is called by default to delegate to git.exe.  It was the @ sign in my git URL that was causing the build to hang, as cmd interprets this as a special character.  **Don't** use `try..finally { exit }`, as *this will result in green builds even when the build fails!*

I was annoyed that I had to split out the last step to update the master branch into a separate build configuration, but there is no way in TeamCity 7.0 and earlier to skip remaining build steps within a configuration after that configuration fails.  Thankfully, JetBrains are resolving this in v7.1, due for release in a few weeks.  

It's worth noting that you could easily extend this to support Continuous Deployment (to production) if needed.  

I've really enjoyed playing around with Kudu, and it is going to quickly become one of my go-to tools for continuous integration and delivery.  What do you make of it?"
Starting out with RavenDB,,"A while back, I volunteered to re-do the [Cambridge Graduate Orchestra](http://www.cambridgegraduateorchestra.com/)'s website, to bring it up to date with the latest web techniques.  To provide persistence, I opted for [RavenDB](http://www.ravendb.net/), a ""second-generation document database"", as I had heard how good it was for rapid development and what a nice API it has.  These are my initial thoughts having wired it into my existing solution.  

<!--break-->

First off, RavenDB does have a very nice .NET API indeed.  I'm using it in embedded mode, which means nothing more than pulling down a different package off NuGet, and swapping out the instantiation of the standard `DocumentStore` class for an `EmbeddableDocumentStore` instance.  I also opted to customise the data storage directory.  The simplicity with which a connection can be made to Raven is best expressed through a sample:

[gist:3198273:Instantiate and initialise an EmbeddableDocumentStore.cs]

Once a document store has been initialised, you can then create sessions to talk to Raven.  Raven's session implementation follows the [Unit of Work pattern](http://www.martinfowler.com/eaaCatalog/unitOfWork.html) defined by Martin Fowler in his book Patterns of Enterprise Application Architecture.  This means that, within a session, the database is guaranteed to be consistent, and it is not until you call `SaveAllChanges()` at the end of your unit of work (if required) that the database is updated.  Again, working with Raven's session is trivial:

[gist:3198273:Complete a unit of work.cs]

So, the API is a joy to work with, and Raven's document model means you just don't have to worry about the database at all: you just throw your objects at Raven's API and they're persisted as-is.  There's none of that [Object-Relational impedance mismatch](https://en.wikipedia.org/wiki/Object-relational_impedance_mismatch) to get in the way.  

I do have one criticism of Raven so far, though.  Looking at the advice provided on [using Raven in an ASP.NET MVC app](http://ravendb.net/kb/3/using-ravendb-in-an-asp-net-mvc-website), we see a tight coupling of Raven to the controller: sessions are instantiated when an Action is executed by the MVC framework, and closed when the Action has finished being exectued.  Also take into account [Ayende's scorn](http://ayende.com/blog/3955/repository-is-the-new-singleton) for the [Repository pattern](http://www.martinfowler.com/eaaCatalog/repository.html), and the fact that trying to apply such a pattern with RavenDB [tends to cause more pain than that which it tries to solve](http://novuscraft.com/blog/ravendb-and-the-repository-pattern). This leaves us in a very difficult place from a unit testing perspective.  

[Phil Jones](http://orangelightning.co.uk/) [suggests](http://ravendb.net/kb/31/my-10-tips-and-tricks-with-ravendb) *not* abstracting RavenDB for unit testing purposes, and that

> Purists will argue these ""unit tests"" become ""integration tests"". I've not been bothered by ""slowness"".

I guess I am a ""purist"", then :-)  I agree that using an embedded in-memory database makes for some very fast integration tests, but *they are still integration tests*.  The point of unit testing is to be able to test a single unit of functionality **in isolation of the rest of the system**; Raven does not (easily) allow you to accomplish this.  The official advice, described above, ties the lifetime of Raven's session to that of the MVC framework, which means that the session object is not available from within a unit test without providing specific hooks in for the tests (a major no-no).  

A nicer alternative is to inject the `IDocumentStore`, or, better still, the `IDocumentSession` into the controller via its constructor, but this still couples the controller to Raven.  Both the `IDocumentStore` and the `IDocumentSession` can be cleanly configured using an IoC container provided it supports singleton and per-web request lifetime scopes.  Unfortunately, using this approach, and the unit testing best-practice of only mocking types that you own, you are left with an integration test: *you have to inject a real Raven object into the controllers*.  

The way to get around this - usually - is to wrap the third-party library in an abstraction.  In this particular situation, however, I will lose, or have to duplicate, much of the implementation of the Raven API to achieve this.  It seems a shame to have to sacrifice some of my hard-earned principles of good software development at this point to make use of such a great library and technology.  "
Review: Gibraltar,,"A couple of weeks ago, I was invited by [Rachel Hawley](http://www.twitter.com/RachelHawley) to take a look at the latest version of [Gibraltar](http://www.gibraltarsoftware.com/), a real-time logging and error analysis solution. Gibraltar was recently updated to v3.0, and I'd been meaning to look at it in more detail for [GiveCRM](http://www.givecrm.org.uk/) for a while, so figured this was an opportunity I wanted to take up.

<!--break-->

Before I begin, in the interests of full disclosure, I should say that I was asked to write this piece, and that I have worked on GiveCRM closely with [Kendall Miller](http://www.twitter.com/kendallmiller), Principal at eSymmetrix who sell Gibraltar.  I have been offered a personal licence for Gibraltar in return for writing this post.

With that out of the way, let's begin.  I had no experience of Gibraltar before this week, and given the nature of the product I was concerned that it might take a fair bit of configuration to set up.  I'm very glad to say that this was not the case at all: installation of Analyst is a breeze, and setting up a Hub is as simple as filling out a few fields on a web page.  

Wait, Hub what? Analyst who?  You're right, I need to back up a bit and lay down some detail.  

## These ain't yo' Mama's diagnostics

Gibraltar is a product that provides real-time feedback on errors that occur in your application.  The most basic high-level description of it that I can come up with is that it is logging on steroids: rather than just dumping log messages (or, worse, exceptions) to a log file or database table, Gibraltar packages up events and errors in your application in such a way that you can get real information out of them. No more log file parsers, and hours of combing through the detritus of your application: just cold, hard facts pointing to the specific failures. Failed configuration? Assembly loading problems? Gibraltar has your back.

Gibraltar is formed of three parts:

  * The *Agent* collects data from your application and sends it to the Hub.  This must be added to your application prior to deployment.
  * The *Hub* stores the data and forwards it onto one or more Analysts.  This is a cloud-based service offered by Gibraltar, but you can also run your own Hub if you prefer (and at an increased price).
  * The *Analyst* provides insight into the data collected.  This is a desktop application you must install on your development box (or similar).  

This short introductory video covers the detail quite nicely.  

<iframe width=""560"" height=""315"" src=""https://www.youtube-nocookie.com/embed/oK1g-jkBiws"" frameborder=""0"" allowfullscreen></iframe>

##  Configure all the things!
 
As I said above, configuring and setting up Gibraltar is actually really simple.  The Hub is optional, but I recommend giving it a go (it sure beats having all your packages emailed to you); furthermore, the cloud Hub is so ridiculously easy to set up, it's harder *not* to use it.  Analyst is a simple MSI installer and runs great on my Windows 8 machine.  

Configuring Agent is a little more involved, but still crazy simple. Fire up Analyst and click the ginormous green button inviting you to add Gibraltar to your application:

<img alt=""Add Gibraltar to your app"" src=""http://codebork.com/sites/default/files/GibraltarHomeScreen_0.png"" width=""600"" />

This launches the Agent configuration wizard, which walks you through six screens to add Gibraltar to your app.  One feature I particularly like about this wizard is that you can point it at an existing binary, or a Visual Studio project file:

<img alt=""Add Gibraltar to your app"" src=""http://codebork.com/sites/default/files/GibraltarWizard1.png"" />

As you might expect, one of the later steps offers you the opportunity to hook Gibraltar up to a Hub, but the inclusion of that ""Test"" button is a real nice touch. How many times have we provided all the details for a remote server, only to find after configuration that we made a typo somewhere?  No?  Just me, then.

<img alt=""Add Gibraltar to your app"" src=""http://codebork.com/sites/default/files/GibraltarWizard4.png"" />

Once the wizard is complete, you'll find that Gibraltar has added a reference to the Agent assemblies to your project file (and saved a back-up of the project file, another nice touch), and modified your [app|web].config file.  Here are some of the changes made to GiveCRM's web.config after I enabled Gibraltar on there:

<img alt=""Add Gibraltar to your app"" src=""http://codebork.com/sites/default/files/GibraltarWebConfig.png"" width=""600"" />

There's also a bunch of stuff done in the `<healthMonitoring>` section to hook into ASP.NET's infrastructure.  

## A gathering storm

With the Agent configured, you can fire up your application, and Analyst, and take a look at the data collected.  If you're using Gibraltar in a desktop application, you can also hit Ctrl+Alt+F5 to bring up a live diagnostics window on the application, [which seems *really* cool](http://www.gibraltarsoftware.com/See/Record.aspx).  It would be awesome if a JavaScript-based version of this could be brought up on a Gibraltar-enabled website too.  

A nice feature of Gibraltar is that it includes details of the session that caused the error: which user account was running the application, how long the session lasted, what OS it was running, how many cores and how much memory the machine was sporting, and more.  

<img alt=""Add Gibraltar to your app"" src=""http://codebork.com/sites/default/files/GibraltarSessionDetails.png"" width=""600"" />

This would be great for auto-populating bug reports, and what do you know, Gibraltar integrates with FogBugz and Gemini.  However, I'd like to see more integrations with other products, such as Atlassian's JIRA, and GitHub's Issues, or perhaps even services such as UserVoice.  

## Analyse This

Beyond poking into your application's errors, Gibraltar will provide some insight into key metrics on your application.  Note that these need to be application event metrics, such as page hit metrics like request duration.  

<img alt=""Add Gibraltar to your app"" src=""http://codebork.com/sites/default/files/GibraltarCharting.png"" width=""600"" />

For some reason, the standard Windows Performance Counters (such as total processor time and available memory) are included here too, but you can't create a chart from them.  On further investigation, I found that if you chose to create a *graph* rather than a *chart*, then you can plot those too.  I would prefer in each of those views to only have those metrics that can be plotted by the view selected, rather than all of them available, or at least some indication why I can't create a chart of processor time.  

I suspect that this mechanism could be extended to allow you gain some (anonymised) insight into your users' behaviour in your application: which features are used most, which are used least, which need more optimisation, etc. It's not immediately clear that this is the case, but I think it would'nt be too hard to put in a new Windows Performance Counter that fired when a feature was activated, with the name of the feature.  This is something we might like to do with GiveCRM, so that we can get a feel for how to develop the product; there is also a strong use case for doing something similar in the SaaS platform part of the product to identify pain points in the administration functions, and monitor performance of the platform.  

## Conclusion

In my limited playing with Gibraltar so far, I have seen potential in it, but no real use yet.  This is, in part, because GiveCRM is not live, and so I was only able to collect data from local sessions whilst debugging the application. As a result, the exceptions thrown (and logged by Gibraltar) tended to cause Yellow Screens of Death that I was able to troubleshoot directly.  However, I see a lot of value in this approach to error logging and analysis, and so look forward to an opportunity to use it in a live application and appreciate its real value in building quality software.  "
"My American Tour, Part 1: San Francisco",,"## Introduction

As some of you know, my brother Toby got married in San Francisco at the beginning of May.  Congratulations to him and [Sera](http://iammagpie.co.uk/)!  As I've not had many opportunities to visit the US in the past (I've only been once before), I thought I would take this opportunity to make a proper trip of it, and so started mapping out what turned out to be not one but *two* holidays of a lifetime.  

In something of a departure from my usual topic, I hope this short series of blog posts will serve as a reminder to me of the good times I had in the US, and jog my memory of all the awesome places I visited.  I hope you enjoy reading these posts too!

My tour went as follows:

  * 1-5 May: San Francisco
  * 6-8 May: [Yosemite National Park](http://codebork.com/2012/06/04/my-american-tour-part-2-yosemite-national-park.html)
  * 8-10 May: [Highway 1](http://codebork.com/2012/06/04/my-american-tour-part-3-highway-1.html)
  * 11-13 May: [Boston](http://codebork.com/2012/06/04/my-american-tour-part-4-boston.html)
  * 14-17 May: [New York](http://codebork.com/2012/06/04/my-american-tour-part-5-new-york.html)
  * 17-19 May: [Washington, DC](http://codebork.com/2012/06/04/my-american-tour-part-6-washington-dc.html)

## 1 May: Departure and Arrival

I believe the best holidays begin with an early start. Anywhere that requires you to get up before you would normally get up for work must be worth going to, and anywhere that requires you to get up before the sun has been bothered to get up doubly so.  As it turned out, I was up at 4.00 to catch a train down to London to catch my flight. In a last-minute change of plan, I had decided to drive myself to work and leave my car there; this turned into a last-minute panic, when I realised that I'd only left 30 minutes to drive to work and walk to the station.  And so, on that cold and rainy morning I found myself hurrying over the Cambridge Railway footbridge with three weeks of luggage in tow, hoping against hope that I wouldn't miss my train.  Luckily, all was well: I made the train with a few minutes to spare, and the Piccadilly line connection from Kings Cross to Heathrow Terminal 5 could barely have been smoother.  

I met up with Toby and Sera, my family (Mum, Dad and Aunt), Sera's Mum and a couple of Sera's friends after checking my bags, and we headed through security. After a stop at Wagamama for some breakfast (they do *really* tasty omelettes!) we eventually managed to board the plane, a comfortable BA Boeing 747, which would be our home for the next 11 hours or so.  

I'd heard a few bad things about BA, so I'll admit my expectations were quite low. However, they were very good at looking after us throughout the flight, and provided and excellent selection of in-flight films.  I managed to catch up on some films I'd recently missed at the cinema, including *J Edgar*, *The Artist*, and *Hugo*.

When we landed in San Francisco, it was 2.30pm local time and 10.30pm body-time.  We got through a long queue for passport control, found our bags and flagged down a cab.  Unfortunately, Toby and Sera's pre-booked cab didn't show up, but they thankfully managed to sort it fairly easily.  We went our separate ways: Toby and Sera and their friends had booked an apartment near Alamo Square, whilst I'd booked into the excellent [Hotel Rex](http://www.jdvhotels.com/hotels/sanfrancisco/rex) just off Union Square.  This quaint little hotel is done out in Art Deco styling, with a small bar that doubles up as the breakfast room.  After doing a little exploration of the room and a bit of unpacking, I went in search of an early dinner with Mum and Dad.  The concierge recommended [Fino](http://finoristorante.com/), a lovely authentic Italian restaurant [on Post St](https://maps.google.com/maps?q=fino+san+francisco&hl=en&sll=52.19317,0.140706&sspn=0.008326,0.022724&hq=fino+san+francisco&t=m&z=15.), just a couple of blocks from the hotel.  After a couple of very tasty courses, we realised we were completely shattered from the jet lag, and stumbled back to the hotel for a very early night.  The adventures would start for real tomorrow!

## 2 May: City Tour, Golden Gate Bridge, Dinner at The Cliff House

After waking up a couple of times in the night (an 8-hour time difference is a bitch, however knackered you are when you go to sleep), I headed down to the Library Bar for some breakfast.  The Californian-style breakfast proved to be something of a feast: fresh fruit (strawberries, two types of melon, pineapple), yoghurt, tea, coffee, toast, bacon, eggs, sausages, pancakes/French Toast, Californian-style potatoes... I ate like a king every breakfast in California!  Also, I discovered by accident that scrambled eggs and maple syrup go together surprisingly well, in moderation.  The Californian-style potatoes were also very tasty: roasted up with peppers and spices.  

![A view of Union Square, San Francisco](http://farm8.staticflickr.com/7093/7336658006_d0e7dddf3e.jpg ""Union Square"")

**Union Square**

I left with my parents and my aunt to catch a City Tour bus (same chain as the open-top red buses in London, as it turned out) later that morning, which took us on a good route around the city from Union Square out to the Golden Gate Bridge and back again later.  The route took in some of the famous parts of the city, such as Alamo Square, Haight-Ashbury, Golden Gate Park and Bridge, City Hall, etc.  It was a windy day (it's always windy in San Francisco, from what I can tell), and we were on the top deck of the open-top bus without much in the way of warm clothing, etc., so we decided to get off at the Golden Gate Bridge and take some photos:

![View of the full span of the Golden Gate Bridge, taken from the San Francisco side](http://farm8.staticflickr.com/7214/7336490186_882bb39acf.jpg ""View of the full span of the Golden Gate Bridge, taken from the San Francisco side"")

**View of the full span of the Golden Gate Bridge, taken from the San Francisco side**

![View of San Francisco from the far side of the Golden Gate Bridge (pictured)](http://farm8.staticflickr.com/7232/7336504184_ddf2ba638c.jpg ""View of San Francisco from the far side of the Golden Gate Bridge (pictured)"")

**View of San Francisco from the far side of the Golden Gate Bridge (pictured)**

![View of the Golden Gate Bridge, from the San Francisco side, featuring the two struts](http://farm8.staticflickr.com/7087/7336488810_db5f3fb614.jpg ""View of the Golden Gate Bridge, from the San Francisco side, featuring the two struts"")

We returned on the tour bus, and met up with Toby and Sera and their friends after a spot of lunch to see how they were doing. We then returned to the hotel to freshen up and change for dinner; e had booked a table at Sutro's at [The Cliff House](http://www.cliffhouse.com/), which [overlooks the Pacific](https://maps.google.com/maps?q=the+cliff+house+san+francisco&hl=en&sll=37.787882,-122.412262&sspn=0.021468,0.045447&hq=the+cliff+house+san+francisco&t=m&z=15).  The building used to be a public baths, the pools seemingly filled by the Pacific itself.  The table was booked for dinner through sunset, and both the food and the views were just stunning.  

## 3 May: Exploratorium, Fisherman's Wharf, The Aquarium of the Bay, Alcatraz

The following day, we headed out to the [Exploratorium](http://www.exploratorium.edu/), a fantastic hands-on science museum/exhibition [on the edge of the Presidio](https://maps.google.com/maps?q=exploratorium&hl=en&sll=37.792083,-122.456017&sspn=0.042934,0.090895&hq=exploratorium&t=m&z=15).  We easily killed three or four hours there playing with the different exhibits, and some of the most interesting ones were to do with the various behaviours of light in different circumstances (e.g., refracted light, polarised light, etc.).  One of the cool things the Exploratorium does is to mix art and science, perhaps because of its proximity to the Palace of Fine Arts,  which is right next door.

![The Palace of Fine Arts](http://farm8.staticflickr.com/7078/7338506236_aaef8ed277.jpg ""Not actually the Exploratorium: the Palace of Fine Arts, next door"")

**The Palace of Fine Arts, next door to the Exploratorium**

At lunchtime, we wandered over to Fisherman's Wharf, and stopped at the In 'n' Out burger there, on the recommendation of [Paul Stack](http://www.paulstack.co.uk/blog/).  It wasn't quite what I was expecting, given that it's a fast-food chain, but it was about as far away from McDonald's as you can imagine, and the burgers were *damn good*.  

In the afternoon, we had a little wander around Fisherman's wharf, where we encountered some sea lions, and paid a visit to the Aquarium of the Bay, featuring a number of the species that can be found in San Francisco Bay.  The moon jellyfish and nettle fish were particularly impressive, and I found Nemo!

![Sea lions](http://farm8.staticflickr.com/7236/7338516750_3e70ea603d.jpg ""Sea lions"")

**Sea lions**

![Moon jellyfish](http://farm8.staticflickr.com/7240/7338536338_95ec5e80d1.jpg ""Moon jellyfish"")

**Moon jellyfish**

![Nettle fish](http://farm8.staticflickr.com/7071/7338532932_7cba4ffc7a.jpg ""Nettle fish"")

**Nettle fish**

![A clownfish](http://farm8.staticflickr.com/7074/7338529226_8c8c92be19.jpg ""A clownfish"")

**A clownfish. It may or may not be called Nemo**

![A blue tang](http://farm9.staticflickr.com/8158/7338525038_993993464d.jpg ""A blue tang"")

**A blue tang**

![Anchovies](http://farm9.staticflickr.com/8027/7338521568_d73d5ebdb8.jpg ""Anchovies"")

**Anchovies**

After the Aquarium, my parents and aunt returned to the hotel, and later that evening, I embarked on a night tour of Alcatraz.  I have to say, I was very impressed with this: the audio commentary, featuring ex-staff and -inmates of the prison was thorough and always interesting, if initially a little confusing as I struggled to get my bearings at the start.  The tour was immensely atmospheric, as the sun set over San Francisco whilst the tour was underway, and twilight fell on the cellblock.

![The Alcatraz cell block at night](http://farm8.staticflickr.com/7092/7338512396_718efae962.jpg ""The Alcatraz cell block at night"")

**The Alcatraz cell block at night**

The ""shows"" put on by the park rangers (Alcatraz falls under the National Parks Service's repsonsibility) were very informative. I attended two of the available shows: ""The Birdman of Alcatraz"", a talk about the life of Robert Stroud and his time at Alcatraz, and ""The Sound of the Slammer"", a demonstration of the cell doors. The doors were hooked up to a really interesting manual mechanism, utilising a gearbox and double clutch. From the one console, the operator could opt to open all the cell doors, all the odd/even ones, open only selected cells, or open all but selected cells.  The doors were opened and shut using a lever: pull down and push up to open or shut the doors.  

![Broadway, Alcatraz](http://farm8.staticflickr.com/7079/7338548766_24929ebabb.jpg ""Broadway, Alcatraz"")

**Broadway. (That's really what it's called.)**

![A utility corridor between the rows of cells at Alcatraz](http://farm8.staticflickr.com/7071/7338543920_c0cd770e95.jpg ""A utility corridor between the rows of cells"")

**A utility corridor between the rows of cells**

By the time the boat docked back at Pier 33 in the [Embarcadero](https://maps.google.com/maps?q=embarcadero&hl=en&ll=37.80107,-122.39645&spn=0.021465,0.045447&sll=52.19317,0.140706&sspn=0.008326,0.022724&hnear=Embarcadero,+San+Francisco,+California&t=m&z=15), I was pretty knackered, so I caught a streetcar back to Union Square and walked back to the hotel.  

## 4 May: The Wedding!

It was starting to look like I was finally over the jet lag, which was a relief: today was the big day, and the real reason I was in the US at all!

In the morning before the wedding I took a trip on one of the cable cars to Telegraph Hill for a trip up [Coit Tower](https://en.wikipedia.org/wiki/Coit_Tower) with Mum and Dad. The tower was built with money from Lillie Hitchcock Coit's estate, which had been bequeathed to the city for ""beautification"". Lillie Hitchcock Coit was a wealthy socialite, who enjoyed chasing fires with the city's fire brigade. Built in the art deco style, the tower stands 210 feet tall on one of the highest points in the city.  The ground floor of the tower is covered in murals, completed in a semi-cartoon style and depicting aspects of San Francisco life.

![A San Francisco Cable Car](http://farm9.staticflickr.com/8010/7354040614_4c42c7e15b.jpg ""A San Francisco Cable Car"")

**A San Francisco Cable Car**

![An example of a Telegraph Hill residence, near the Coit Tower](http://farm9.staticflickr.com/8003/7354040890_4c685cb01b.jpg ""An example of a Telegraph Hill residence, near the Coit Tower"")

**An example of a Telegraph Hill residence, near the Coit Tower**

![Part of the mural running around the base of the Coit Tower](http://farm8.staticflickr.com/7075/7354041112_e3aa9354f2.jpg ""Part of the mural running around the base of the Coit Tower"")

**Part of the mural running around the base of the Coit Tower**

![A view of San Francisco from the top of the Coit Tower](http://farm8.staticflickr.com/7234/7168829125_81bcfa0900.jpg ""A view of San Francisco from the top of the Coit Tower"")

**A view of San Francisco from the top of the Coit Tower**

![A view of the Golden Gate Bridge and Marin County from the top of the Coit Tower](http://farm8.staticflickr.com/7234/7168829319_911e1e38b9.jpg ""A view of the Golden Gate Bridge and Marin County from the top of the Coit Tower"")

**A view of the Golden Gate Bridge and Marin County from the top of the Coit Tower**

We walked back down Telegraph Hill via [Filbert Street](https://en.wikipedia.org/wiki/Filbert_Steps), a stepped walkway down the hill with lots of lovely houses set apart from the city, and found ourselves in Levi's Plaza, home to the world headquarters of Levi Strauss Jeans.  

![A cottage on Filbert Street](http://farm8.staticflickr.com/7094/7168829563_d8ae53b8aa.jpg ""A cottage on Filbert Street"")

**A cottage on Filbert Street**

![Levi Strauss & Co. Head Office](http://farm8.staticflickr.com/7092/7354042088_aac0aa98a7.jpg ""Levi Strauss & Co. Head Office"")

**Levi Strauss &amp; Co. Head Office**

Levi's Plaza was a really nice green space, with a stream and seating and lots of lovely sunshine, and the office buildings were interesting to look at.  It must be great working there.

![A San Francisco Street Car](http://farm8.staticflickr.com/7226/7354042334_a713cf25ef.jpg ""A San Francisco Street Car"")

**A San Francisco Street Car**

We caught a street car back to Fisherman's Wharf for a spot of lunch at Boudin, a San Francisco Sourdough bread-maker. They have a really interesting museum on-site that delves into the history of the area, as well as the history of the Boudin company and the sourdough bread.  

Sourdough bread is really tasty; it has a slight tang to the taste (hence the sourdough name), and is often baked in the French stick (baguette) and bap styles. Boudin's signature sandwich is a clam chowder bowl, where the bowl is made from sourdough bread. 

Running a bit late, we rushed back to the hotel on another street car (as much as one *can* rush on those things!), and got ready for the wedding.  Soon we found ourselves outside the beautiful San Francisco City Hall waiting for the rest of the wedding party to arrive.  

![San Francisco City Hall front entrance](http://farm8.staticflickr.com/7236/7168878569_1c87d72361.jpg ""San Francisco City Hall front entrance"")

**San Francisco City Hall front entrance**

We headed indoors to wait for them, and were again stunned by how beautiful the building was.  A number of other weddings were running at various points around the building.

![Another wedding party in the foyer, outside the Board of Supervisor's office](http://farm9.staticflickr.com/8162/7354090644_ca2b73a31f.jpg ""Another wedding party in the foyer"")

**Another wedding party in the foyer**

![Part of the foyer, looking towards the North Light Room](http://farm8.staticflickr.com/7091/7168879111_61ace72ed8.jpg ""Part of the foyer, looking towards the North Light Room"")

**Part of the foyer, looking towards the North Light Room**

To the north and south of the central foyer are two Light Rooms, with frosted glass ceilings. The light in here is much brighter than elsewhere in City Hall, and is very warm as well.  The North Light Room is home to the Hall's caf&eacute;, whilst the South Light Room houses some memorials to the architect, the city, and the nation.

With the rest of the wedding party now arrived, we headed up to the fourth floor for some photographs and the ceremony.  The ceremony was over surprisingly quickly: we didn't have any of the hymns, readings, or other items you get with a church wedding, so there was little more to it than the vows. However short it was, though, it was a very sweet and fitting ceremony for Toby and Sera, and I'm hugely proud of my brother :-)

![The ceremony: short, but very sweet](http://farm8.staticflickr.com/7211/7354091218_458e59ecc1.jpg ""The ceremony: short, but very sweet"")

**The ceremony: short, but very sweet**

After the ceremony, the photographer took the happy couple off for some photos and the rest of us went in search of a drink and headed outside to flag down the wedding transport.  The bus took us on a bit of a tour around the city, through Alamo Square and Haight Ashbury amongst other places, and finishing up in Golden Gate Park outside the de Young museum and the California Academy of Sciences at the eastern end of the park.  The journey was accompanied by a soundtrack of 1960s music, a bottle of Francis Ford Coppola's Blancs de Blancs [sparkling wine, Sofia](http://www.franciscoppolawinery.com/wine/sofia), and some delicious [cake pops](http://duckduckgo.com/?q=cake+pops) (mine was [red velvet](https://en.wikipedia.org/wiki/Red_velvet_cake), nom!).

![The wedding transport: a cable car-style bus!](http://farm8.staticflickr.com/7105/7168880075_d6d1e91f61.jpg ""The wedding transport: a cable car-style bus!"")

**The wedding transport: a cable car-style bus!**

The photographer took Toby and Sera off for some more photos around the park, and Mum and I went to see if we could get into the Jean Paul Gaultier exhibition at the de Young museum; unfortunately it was too late in the day, so I went for a wander around the immediate area before catching a cab back to the hotel with my parents and my aunt.

![A fountain in the plaza between the de Young Museum and the California Academy of Sciences](http://farm8.staticflickr.com/7222/7168830485_978c2f041a.jpg ""A fountain in the plaza between the de Young Museum and the California Academy of Sciences"")

**A fountain in the plaza between the de Young Museum and the California Academy of Sciences**

![The de Young museum, framed by a palm tree](http://farm8.staticflickr.com/7222/7354042606_37aa7f845d.jpg ""The de Young museum"")

**The de Young museum had on an exhibition of Jean Paul Gaultier whilst we were there**

The wedding reception was held in one of the function areas at the wonderful [Foreign Cinema](http://www.foreigncinema.com/) in [the Mission district](https://maps.google.com/maps?q=Foreign+Cinema,+Mission+Street,+San+Francisco,+CA,+United+States&hl=en&sll=37.768985,-122.468204&sspn=0.021474,0.045447&oq=foreign+cin&hq=Foreign+Cinema,+Mission+Street,+San+Francisco,+CA,+United+States&t=m&z=15) of San Francisco.  The restaurant's entrance is styled as a classic cinema, and the lobby area is a long corridor decked out with a red carpet, and as the sun goes down, the outside dining area at the back of the restaurant is treated to a projection of a foreign film to accompany their dinner. Seated at a large table on the upper floor, we were next to the projectors used for the films so enjoyed the fantastic atmosphere and the films at the same time!  The food was really excellent too, so I highly recommend it if you find yourself in San Francisco.

## 5 May: California Academy of Sciences

On our final day in San Francisco, we returned to the eastern end of Golden Gate Park to visit the [California Academy of Sciences](http://www.calacademy.org/) (CAS). This was a full day out and more than worth the visit!  Our day at CAS started with a visit to the [Morrison Planetarium](http://www.calacademy.org/academy/exhibits/planetarium/), as a show was beginning about 15-20 minutes after we arrived.  This is the largest all-digital planetarium in the world, and runs an interesting program of films; the one we saw was *Life: A Cosmic Story* ([trailer](https://www.youtube.com/watch?v=I4LpmWe1YA4)), narrated by Jodie Foster.  

CAS is divided into a number of sections, based on geography and scientific subject. They had penguins!

![Penguins at CAS](http://farm9.staticflickr.com/8299/7864264742_4cd929af65_z.jpg ""Penguins at CAS"")

**Penguins at CAS**

The basement of CAS is given over to an enormous aquarium that I sadly didn't get to peruse as completely as I would have liked (although, having visited The Aquarium of the Bay just a couple of days earlier, I was not as disappointed as I might otherwise have been). One of the best exhibits in the aquarium was this display of Moon Jellyfish:

<object width=""360"" height=""640""><param name=""allowfullscreen"" value=""true""></param><param name=""movie"" value=""https://www.facebook.com/v/10150858200385267""></param><embed src=""https://www.facebook.com/v/10150858200385267"" type=""application/x-shockwave-flash"" allowfullscreen=""1"" width=""360"" height=""640""></embed></object>

And they had a huge tropical fish tank as well, with a large curved wall given over to displaying them:

![The tropical fish tank at CAS](http://farm9.staticflickr.com/8291/7864091044_fb1ac3fa90_z.jpg ""The tropical fish tank at CAS"")

**The tropical fish tank at CAS**

One of the main exhibits at CAS is a large (i.e., three-storey) biodome, which mirrors the makeup of rainforests around the world. As you walk up the ramp and through the levels of the biodome, you experience the flora and fauna at each of the levels of a rainforest. 

![The biodome at CAS](http://farm8.staticflickr.com/7246/7864089298_a04e2a3236_z.jpg ""The biodome at CAS"")

**The biodome at CAS**

![Lizard!](http://farm9.staticflickr.com/8304/7864090210_0fd7d8b8cc_z.jpg ""Lizard!"")

**Lizard!**

We rounded off the day with a bite to eat at the Top of the Mark, a bar on the top floor of the Mark Hopkins Intercontinental hotel.  The hotel is 17 stories high, and is sited at the top of the hill in Downtown San Francisco.  You get some fantastic views over the city from that height, the food was good too!

![The entrance to the Mark Hopkins Intercontinental](http://farm9.staticflickr.com/8440/7864091830_ea58db1e6a_z.jpg ""The entrance to the Mark Hopkins Intercontinental"")

**The entrance to the Mark Hopkins Intercontinental**

![The front of the Mark Hopkins Intercontinental](http://farm9.staticflickr.com/8423/7864092866_bf7866e45e_z.jpg ""The front of the Mark Hopkins Intercontinental"")

**The front of the Mark Hopkins Intercontinental**

![A trio of croque messieurs](http://farm9.staticflickr.com/8291/7864093770_dd6611dfc2_z.jpg ""A trio of croque messieurs"")

**A trio of croque messieurs**

![Green tea tiramisu](http://farm9.staticflickr.com/8423/7864094734_e5e17f8f66_z.jpg ""Green tea tiramisu"")

**Green tea and mango tiramisu**

![The view out to the Bay](http://farm9.staticflickr.com/8429/7864095654_5b71696995_z.jpg ""The view out to the Bay"")

**The view out to the Bay**

## Closing thoughts

I was initially a bit ambivalent towards San Francisco, but it grew on me and I came to appreciate it for the fantastic city it is; I certainly left wanting to go back. The people are so very friendly, and the city is a really wonderful place to be. It has a few run-down areas (the bit of the Mission district I saw was one, and the Tenderloin district is supposed to be the rough bit of town), and a huge problem with homelessness, but it generally feels like quite a safe city, and is a great place to spend some time."
Cambridge Software Craftsmanship Community: the next phase!,,"Three months ago, [I founded the Cambridge Software Craftsmanship Community](http://codebork.com/2012/07/04/cambridge-software-craftsmanship-community.html), and I am pleased to say that it has grown well over that period: we now have nearly 60 practitioners in our community! On Tuesday, we will be holding our [third monthly Round-Table Discussion](http://www.meetup.com/Cambridge-Software-Craftsmanship/events/76838042/) at Granta Design, but this is not what I want to talk about now.  I want to talk about the next phase of the community's development.  

As I laid out in my [introductory post](http://codebork.com/2012/07/04/cambridge-software-craftsmanship-community.html), phase two of my plan is to introduce the hands-on sessions that are so popular at the [London Software Craftsmanship Community](http://www.meetup.com/London-Software-Craftsmanship/), and this phase begins this September.  On 18 September, we will hold our first hands-on session at Granta Design.  This will take the form of an introduction to TDD, with the intention of providing a mentoring component as part of the exercise.  Craftsmanship exercises are paired, and implemented using the practice of TDD, so it is my hope that this session will provide an introduction to the format for future sessions, as well as in some cases introducing people to a new skill.  We will pair experienced TDD-ers with less-experienced TDD-ers and work through a relatively simple kata, such as the Roman Numerals kata.  

I am also very pleased to announce that our second hands-on session will be delivered by [Sandro Mancuso](http://craftedsw.blogspot.co.uk/), founder of the London Software Craftsmanship Community, at the end of October.  The details of the session are still to be finalised and announced, so keep an eye on Twitter (under the [#camswcraft](https://twitter.com/#!/search/%23camswcraft) hashtag) and the [CSCC's Meetup site](http://www.meetup.com/Cambridge-Software-Craftsmanship/) for updates.

It's a really exciting time for the Cambridge Software Craftsmanship Community. I feel really privileged to be a part of it, and continue to be humbled by the feedback that I get. If you're looking for a group to discuss and improve software development with other developers who want the same, check us out.  We think you'll like it.  "
Automating TeamCity (Part 1 of n),,"Recently at work I have been working on a project to build out a new TeamCity installation on a small farm of servers.  Having drawn some inspiration from [Paul Stack](http://www.twitter.com/stack72), I knew that leveraging a virtualised environment could buy us some big wins in automating many aspects of the new TeamCity environment.  This post begins a series of posts that will describe in some detail what I set out to achieve, why, and how I did so.  

## Motivation

At work, we currently have a number of TeamCity installations, ranging from the very small to somewhat larger: our smallest installation consists of a TeamCity server and a build agent all installed on a single machine, whilst the largest consists of a TeamCity server on one machine and a mixture of physical and virtualised buildagents to a total of eight agents.  The initial motivation for the project was to consolidate these environments into a single larger environment to reduce administration overheads and remove silos of builds and their artefacts.  

I had previously done some work to create a Virtual Machine (VM) template for the build agents in our main TeamCity installation, with some success: setting up a new bulid agent from scratch used to be a day's work, and with the VM template that time initially came down to 10 minutes or so.  Unfortunately, the template was a little fragile in the way it had been set up: for example., a PowerShell script that ran on boot would not always run, and the use of an answer file for Windows setup turned out to be a poor choice. Furthermore, over time the template only became more out of date as we made changes to our product and infrastructure, particularly in moving from Subversion to Git.  It was still much faster to create new build agents than it used to be, however, not least because we didn't have to wait for an old developer machine to become available, and when we bought another five build agent licences a couple of months ago, I was able to get the new VMs set up from this template in a few days. 

We also wanted to set up a new domain on an isolated subnet for the new TeamCity infrastructure.  We have approximately 25000 tests in our main product, and sadly many of these are poor tests: in particular, there is a large number of tests that hit the underlying database for our product, and some hit the company's Active Directory.  Coupled with the network problems we have historically suffered (thankfully now mostly resolved!), it is sadly not uncommon to see spurious test failures because some component of these system tests could not be contacted. By placing TeamCity on its own domain, isolated from high traffic of the main network, it is hoped that these tests will become more reliable.

After talking with Paul months ago, and in feeling some of this pain, I realised that putting everything into the template was not a sensible decision, and something more configurable was required.  Paul had described how he had scripted the configuration of his build agents, to the extent that he could click ""Run&hellip;"" on a particular build configuration in TeamCity, and have a new build agent ready a few minutes later.  This sounded like a really cool thing to be able to do, and coupled with a requirement from my colleague [Adrian](http://www.twitter.com/adrianbanks), namely that we be able to regenerate the build agents on a weekly or monthly basis to keep them clean and free of cruft, Enter the power of automation.  

## The Plan

A plan was forming in my mind.  Onto each of the five new servers we had bought for this purpose, I would install Citrix XenServer, an Enterprise-class Server Virtualisation product I was familiar with from my time working at Citrix.  The servers would be joined together into a pool so that VMs could freely move around them as necessary.  We would need some form of shared storage for the XenServer pool to store its VM templates.

The VM templates themselves would be very slim: the minimum required software would be installed.  For a basic build agent, this meant Windows Server 2012 plus the TeamCity Build Agent software.  Further templates would be required for .NET 4.0 build agents and .NET 4.5 build agents, so that they could include the appropriate Windows and .NET SDKs.  I would be very strict about what could be installed onto a build agent template and what could not, and pretty much everything could not.  

Using the might of PowerShell, we would script the installation of dependent software onto a new build agent VM; Chocolatey (built on the popular NuGet package manager) would be of use here.  Ideally, we would script the configuration of the build agent machine as well: set the computer name, join it to the domain, and more.  

The final piece of the puzzle was TeamCity itself. We would need to set up a set of build configurations to automatically delete and re-create all the build agents currently running, and to allow for the creation of a single new build agent.  We would also use this as the NuGet package source for our custom Chocolatey packages. 

### List of Tools Used

  * Citrix XenServer 6.1 (free edition)
  * FreeNAS (shared templates over iSCSI)
  * Windows Server 2012
  * PowerShell 3.0
  * PowerShell SDK for Citrix XenServer 6.1
  * Chocolatey 
  * and, of course, TeamCity

## Part 1 Wrap-up

Hopefully this post has made clear the motivation for this project, and some of the requirements involved.  When we set out, I was confident that, although the end result sounded more than a little utopian, the goals could be achieved.  I'm glad to say that with a lot of research and tinkering, and hard work to get the bits working in production, it *is* possible to achieve that utopian state, and it's pretty cool to see it happening.  The detail of the solution will be covered in future posts.  "
Automating TeamCity (Part 2 of n): Automating XenServer,,"Recently at work I have been working on a project to build out a new TeamCity installation on a small farm of servers.  Having drawn some inspiration from [Paul Stack](http://www.twitter.com/stack72), I knew that leveraging a virtualised environment could buy us some big wins in automating many aspects of the new TeamCity environment.  This post continues a series of posts that will describe in some detail what I set out to achieve, why, and how I did so.  

Read previous posts in this series:

  * [Automating TeamCity, Part 1: Introduction and Motivation](http://codebork.com/2012/11/25/automating-teamcity-part-1-n.html)

<!--break-->

## The XenServer PowerShell SDK

First, a little bit of context.  Citrix XenServer is an Enterprise-class server virtualisation product, aimed at the corporate data centre.  I've been using it for about five years now, having been introduced to it at Citrix shortly after they acquired XenSource.  It's a direct competitor to VMware's ESXi, and, to a lesser extent, Microsoft's Hyper-V.  XenServer has the advantage of providing [a free version](http://www.citrix.com/products/xenserver/try.html) (account required for download), with a number of limitations, but those seem to be reduced with each successive release: it will support pooling servers to share resources, live migration of VMs across machines in a pool, and more. This is plenty sufficient for our needs, although some of the high-availability and dynamic workload balancing features in the paid-for editions may yet prove useful in future.  

All three of the main server virtualisation products provide a PowerShell SDK for administering and automating aspects of the environment.  [The XenServer PowerShell SDK](http://www.community.citrix.com/cdn/xs/sdks) is given away free (account required), and works with all editions of XenServer.  There are a couple of unfortunate design choices to this tool, sadly, and those are:

  * **It does not follow PowerShell conventions.** Although better than the older preview version of [the XenServer PowerShell cmdlets](http://codebork.com/2012/11/25/automating-teamcity-part-1-n.html), it uses an odd naming convention that does not fully comply with the PowerShell Verb-Noun naming convention, and subsequently breaks tab completion, an important discovery mechanism for any new set of cmdlets.  
  * **The cmdlets do not support the PowerShell pipeline.**  You can't simply take the output of a `Get-VM`-type operation and pipe it into another cmdlet; everything is done using `XenRef` objects, which in practice seems to mean strings. 
  * Both of the above are a direct result of the fact that **the cmdlets provided are a very thin abstraction over the XenServer API.**  Furthermore, the cmdlets are very fine-grained, which means there are no helpful high-level cmdlets for, e.g., creating a VM from a template, or deleting a VM.  Instead, everything is a multi-step operation.
  * **The documentation is terrible.** In most cases, the help on the cmdlets says little more than ""refer to the API documentation"", but the PowerShell SDK does not ship the API documentation!  Furthermore, the API documentation is exactly what you'd expect from something with that title: a brief summary of the method calls, types, enumerations, etc.
 
## Removing existing build agent VMs

We have a couple of templates for our build agents: one for .NET 4.0 (called dotnet40) and one for .NET 4.5 (called dotnet45).  I decided to use the convention [templatename]-[index] to name the virtual machines: the first .NET 4.0 build agent VM would be called dotnet40-1, the second would be dotnet40-2, etc.  I decided that as part of *adding* a build agent, it should have a tag associated with it so that the active build agents can be easily identified for deletion.  Furthermore, we should be careful not to delete the build agent running this job!  

The script to achieve this looks rather like the following.  We enumerate all the VMs on the XenServer pool, filter them by tag (to get the active build agents) and then by name (to remove the build agent running this job), then perform a clean shutdown on each of the VMs and delete it.  

[gist:4143207]

Note that I've imported a couple of scripts here, too. `credentials.ps1` contains a set of functions to provide credentials for various services, such as the XenServer pool; I'll talk more about this script later in this post.  `teamcity.ps1` is the TeamCity module from [psake-contrib](https://github.com/psake/psake-contrib), renamed to make it an executable script file rather than a PowerShell module.  This module provides a set of functions for writing out [TeamCity service messages](http://confluence.jetbrains.net/display/TCD7/Build+Script+Interaction+with+TeamCity#BuildScriptInteractionwithTeamCity-ServiceMessages), which I use here to output build progress information.  

## Spawning new build agent VMs

This script turned out to be rather more complicated to achieve, due to the fact that the cmdlets are so fine-grained.  

First, I clone the template.  This uses a fast-clone of the virtual disk, which uses the virtual hard drive (VHD) file in the template as a base-line for the new VM, and creates a new, linked, VHD file to store the changes to the disk.  This makes cloning a template super-fast (in my research for this blog post, I came across [this blog post](http://virtualfuture.info/2010/10/xenserver-fast-clone-a-vm-120-times-in-90-seconds/) in which the author claims that he was able to fast-clone a template 120 times in 90 seconds).  However, there are some issues with this approach, namely that [it can lead to an explosion of linked VHDs](http://www.danieletosatto.com/2012/05/04/creating-vms-from-templates-in-xenserver-creates-a-fast-clone/) if, as I was, you re-create your template over and over again from a clone each time.  This is easily resolved by doing a full copy of the template, however, which copies out all the delta VHD files as well as the base into a fresh VHD file ready for use by the new copy of the template.  Be warned that this can take a very long time!

Next, I set the tags on the VM.  This is slightly clunky: we get all the tags, add the desired new tag to that collection, and set them all back on the VM again.  It would be nice if this were to conform more to the PowerShell conventions: i.e., for the SDK to provide `Add-Tag` and `Remove-Tag` cmdlets.

So far, we have a clone of the desired template, with a tag added; we can't yet boot it, because it's still a template and not a runnable VM.  The third step, then, is to make the VM runnable.  Most of my research indicated that it was just a case of setting the `is_a_template` property of the VM to `false`, but this seemed a bit hacky.  Investigating the cmdlets available in the SDK, I found `Invoke-XenServer:VM.Provision` which has the same effect; whether or not it does anything beyond this I don't know, but it seemed better to use this cmdlet rather than fiddling the properties of the VM directly.  

We now have a VM we can boot up, so the final step is to do this.  I return the build agent's name from this helper function so that it can be used in future calls.  Note that in the body of the script, I obtain this value by using the `-1` array indexer on the result of the function call.  This is because the TeamCity module used to report build progress reports the service messages using `Write-Output`, which is equivalent to `return` in PowerShell.  As a result, this function returns multiple values, as an array.  The `-1` array indexer is simply a short-hand for saying ""get the last item in the array"".  

With the VM up and running, we now need to know when it's available for use, and more specifically, what its IP address is.  This took a bit of searching around the internet, but eventually I turned up [this handy cheatsheet for the XenServer API](http://blog.gigaspaces.com/wp-content/uploads/cloud/Xenserver_API_CheatSheet.pdf) (PDF), which provided an answer in C# using the .NET SDK that I could translate into the PowerShell SDK fairly easily.  Using the ""Guest Metrics"" of the VM, I can get the network information on the VM, and subsequently the IP address(es) of the VM.  

[gist:4143356]

## A note about storing credentials for use in PowerShell scripts

As I mentioned earlier, I have a couple of functions to retrieve stored credentials for various things, such as connecting to XenServer.  Initially, I used a technique I had picked up in some previous automation work: enter the credentials using `Get-Credential`, then use `ConvertFrom-SecureString` to get a string representation of the encrypted password that could be safely stored in the script file for later use.  This turned out to have a fairly major problem with it, however, namely that by default the encryption algorithm for `SecureString`s includes a machine-specific component *and* a user-specific component.  I discovered this when running the scripts on a build agent, after successfully trialling them on my development machine, and found that my calls to `ConvertTo-SecureString`, to turn the encrypted password string back into a secure string for use in a `PSCredential` object, kept returning the error ""Key not valid for use in specified state.""

What I had to do instead was generate an encryption key, which must be of a specific length (128, 192, or 256 bits, or 8, 12, or 16 Unicode characters), save this to a text file in Unicode encoding, **set appropriate read permissions on this file,** and read it in from the file as part of the script.  I then re-generated the encrypted password string using `Get-Credential` and `ConvertFrom-SecureString`, this time passing the `-Key` argument to the latter. Annoyingly, the Key can only be provided as a byte array or a SecureString (using the `-SecureKey` argument), so I had to convert the key to a byte array before using it.  My `credentials.ps1` script therefore looks a lot like the following:

[gist:4143393]

## Part 2 Wrap-Up

This part of the series has covered the approach that I took in automating the virtual machine aspects of our new TeamCity infrastructure, including some of the problems that I ran into both with the XenServer PowerShell SDK, and the practice of storing credentials such that the password is not written to the script *and* is usable across machines.  

I would like to see more time and effort invested by Citrix in the XenServer PowerShell SDK to make it less of a thin abstraction over the XenServer API and more of a discoverable set of PowerShell cmdlets, in line with the guiding design principles underpinning all of PowerShell itself.  Well-defined cmdlets that don't try to do too much, such as `New-VMFromTemplate` and `Get-VMIPAddress`, would have made this a much nicer SDK to work with, and implementing proper pipeline support would make the SDK immensely more powerful.  

Stay tuned for Part 3: running configuration scripts remotely!"
Speaking at NxtGenUG this month,"I am speaking at [NxtGenUG Cambridge](http://www.nxtgenug.net/) this month, delivering a talk on Dependency Injection.",
SoCraTes UK 2013,"This last weekend I attended the [SoCraTes UK conference](http://socratesuk.org) at the Farncombe Estate in the Cotswolds, near Moreton-in-Marsh. It was a fantastic weekend of mixing with, talking to, and learning from other software craftsman from around the country and across Europe.","This last weekend I attended the [SoCraTes UK conference](http://socratesuk.org) at the Farncombe Estate in the Cotswolds, near Moreton-in-Marsh. It was a fantastic weekend of mixing with, talking to, and learning from other software craftsman from around the country and across Europe.

Organised by Sandro Mancuso, Samir Talwar, Gonçalo Silva and Mashooq Badar (apologies if I missed anyone), SoCraTes took the form of an open space or ""unconference"" with no formal structure or agenda. Such a format requires good facilitation, and we had one of the best in the business: Rachel Davies of Unruly Media, and co-author of the first book on Agile Coaching.

SoCraTes started on Thursday evening with a series of lightning talks from attendees, followed by a fishbowl-format discussion session after dinner. The highlights of the lightning talks for me were @sleepyfox's talk on *Why Gödel's Incompleteness Theorem Renders Estimation Meaningless* (apparently delivered completely off the cuff!), whether personal responsibility should be common knowledge for all software craftsman (delivered by Erik Talboom), and Chris Whitworth's talk titled *Naming things is hard but that's no excuse*. I'm also looking forward to trying out on a couple of codebases [Dmitry Kandalov's tool for code history mining](http://dkandalov.github.io/code-history-mining/junit.html).

The [fishbowl discussion](http://en.wikipedia.org/wiki/Fishbowl_(conversation)) was a new discussion format to me, but it turned out to be quite similar to the park bench-format discussions sometimes held at Skills Matter. It started off with a panel of four people, who were tasked with discussing, and attempting to answer, three questions about software craftsmanship. Any time someone from the audience wanted to contribute they had to take the spare seat on the panel, and someone on the panel had to vacate their seat. As the discussion began to up, I started to see how the format got its name: the people in the room were moving around the room like fish in a bowl. I thought the format worked well, although it was unfortunate that only a fraction of people in the room contributed their thoughts.

The open space itself ran over Friday and Saturday. Talking with Rachel at the end of the event on Sunday, it was interesting to learn about the origins of the format. The person who came up with it was running a lot of events and found them a lot of work to put together, particularly around the selection of sessions; additionally the feedback that he got strongly suggested that the coffee breaks between sessions were the bits the attendees found most valuable. As as a result, he devised a format that removed the work of putting together a schedule and put the focus on collaboration, conversation and communication with other attendees. 

We began the open space on each day with a plenary-type session where people interested in proposing sessions wrote their idea down on a big (A5-sized!) sticky note. We had 9 rooms available over 6 time slots (three before lunch, three after lunch), so everyone who proposed a session got a chance to deliver it to at least a handful of people. The great thing about the open space format is [the set of guidelines (and one law)](http://en.wikipedia.org/wiki/Open_Space_Technology#Guiding_principles_and_one_law) that define it:

* The Law of Two Feet: if the session isn't turning out the way you'd expected and there is something else you'd like to try, just move on to it. Your two feet are your foot of passion and your foot of responsibility, so stand up for your passion as well.
* Whoever comes are the right people
* Whatever happens is the only thing that could have
* Wherever it happens is the right place
* Whenever it starts is the right time
* When it's over, it's over

Earlier sessions spawned new sessions later on, some sessions spilled over into one or more later slots and moved to a new space, and everyone had invigorating discussions.

I attended three sessions on the first day: refactoring the 13 rules of TDD into a more manageable list, WTF Code (with some great examples of how not to use `int`s and `Integer`s in Java!) and Dealing with Legacy People. This last was a fun discussion, if perhaps not hugely productive, of ways of dealing with people in positions of authority (e.g. team lead, architect) that haven't kept their skills and knowledge up-to-date and aren't willing to accept constructive criticism of their work. I took a break before that session to do a spot of hands-on coding: there's something about being amongst that kind of people that brings out my passion for coding. 

I had fuller day on the second day. I spent the first two sessions in Marcin Gryszko's DDD Practice session, then went to a [Cyber-Dojo](http://www.cyber-dojo.com/) before lunch. I took the first session after lunch off, and then joined Erik and Ian Russell's discussion session on Personal Responsibility, which overran into the final slot of the day. 

Marcin's DDD session I found invaluable; it was like the [Architectural Katas](http://blogs.tedneward.com/2010/06/17/Architectural+Katas.aspx) devised by Ted Neward, but for Domain-Driven Design. I learned from Sandro that the key thing is to start with your application's services (which, in the exercise in this session, were loosely tied to use cases) and identify the entities and values from there; previously I'd been working the other way around. This makes a lot more sense to me now, particularly after someone else described it as working outside-in. 

The final day of SoCraTes UK was a lovely walk around the Cotswolds, led by Rachel. It was exactly what I needed in my slightly-hungover-from-Saturday-night state!

SoCraTes UK was also my first experience of speaking at a conference, and in many respects I think I chose a good event for my first time: the audience were friendly and supportive, and interested and engaged, not least because everyone was there for the simple fact that they wanted to be! The format of my session (a prepared talk with slides and everything) didn't really fit the format of the conference, though, so I will remember to keep that in mind when preparing sessions in future. I will blog about my session separately to keep this post down to a vaguely manageable length.

The most interesting thing for me over the course of the conference was the shared vocabulary that emerged during the weekend: ""personal responsibility"" and ""mindfulness"" being two repeating themes. The  most valuable thing for me was the opportunity to reflect on our principles, values and practices with like-minded people, who invited open discussion and debate on those very things: ""strong opinions, weakly held"" was the line from Samir that summed it up best for me. 

The most unexpected thing for me was the opportunity to chat with leaders in the Software Craftsmanship movement across Europe as equals. Occasionally it has been the case at other conferences and events (particularly the more formal and structured ones) that the speakers are set apart from the attendees, which can make it more difficult to interact with them. At SoCraTes, it was the complete opposite: it was just a bunch of people in a room. 

A little surprising for me was the interest other people had in the [Cambridge Software Craftsmanship Community](http://www.camswcraft.org.uk/) and how it was progressing. I must have had conversations along these lines nearly 10 times over the course of the weekend, which was ace. Having stepped back from the London Software Craftsmanship Community in the last year or so, not least to focus on getting CSCC up and running, I now realise that we'd become a little bit isolated up here in Cambridge from the rest of the Software Craftsmanship community. I intend to fix that by starting to attend the LSCC meet-ups again from next month, and encouraging more inter-community collaboration, through things like Craftsman Swaps, workshops and joint code retreats; other ideas welcome, of course! I'm hugely looking forward to seeing more CSCC attendees at SoCraTes UK 2014. 

I'd like to say a note of thanks to everyone that helped out with the organisation of SoCraTes UK, to everyone that delivered sessions, to Rachel for her top-notch facilitation of the event, and to the following people for the varied, wide-ranging, detailed, and down right funny discussions I had with you over the course of the weekend: Ian Russell, Teo Danciu, Nicole, Andreas, James Lewis, Raphael Ackermann, Dmitry Kandalov Erik, Sandro, Mani, Rachel, Emanuele Blanco, @sleepyfox, and Chris Whitworth. It was a pleasure meeting or catching up with you all, and I'm looking forward to seeing you again soon!"
A Plurality of Ideas: Contrasting Perspectives of Software Craftsmanship,"Attending SoCraTes UK over the weekend got me thinking about Software Craftsmanship in general, and the different perspectives within the movement was one of those things, particularly the differences, as I perceive them, between the North American and the European schools. ","Attending SoCraTes UK over the weekend got me thinking about Software Craftsmanship in general, and the different perspectives within the movement was one of those things, particularly the differences, as I perceive them, between the North American<a name=""footnote1-ref"" href=""#footnote1"">*</a> and the European schools. 

<!--break-->

Software Craftsmanship started off in the US about 5-6 years ago, as something of a reaction against the sea change in the agile movement away from technical practices. I first came across it myself in 2010, and it looked right up my street, so I promptly booked myself onto that year's Software Craftsmanship UK conference at Bletchley Park. I liked the technical focus of the conference, particularly the hands-on nature of it, and I've been every year since (hopefully see you there in October!). Reading more about the movement, I saw a great deal of focus on practices, particularly those inherited from XP like pair programming and Test-Driven Development.

A couple of years later, I started attending the London Software Craftsmanship Community meet-ups. Initially, I went along to their workshops to learn something new about the act of developing software, a new technique to try out, practice my TDD, etc. A few months later, I started attending their Round Table discussions as well, and found the opportunity to talk to other craftsmen invaluable. Discussions around Domain-Driven Design, corporate cultures, motivation, and hiring good developers, were just some of the highlights of the conversations there.

Clearly on display at SoCraTes UK this weekend was a plurality of ideas, and people willing to reflect on their craft and develop their understanding through the challenging of their principles, practices, ideas, and beliefs. I saw this across the group of 70 people attending from at least 5 countries in Europe. It could easily have been an exercise in navel-gazing, but what we had instead were productive discussions that helped us form a clearer collective picture of the concept of Software Craftsmanship.

Something we discussed in the fish bowl at the start of the conference was whether Software Craftsmanship was becoming a religion, particularly in the way we share our values with other people. @sleepyfox made the point that we shouldn't be trying to bring our values to other people, because it's too much like a religious war, or a crusade. That way lies dogmatism, the antithesis of what lies at the heart of Software Craftsmanship. I fear there are some elements of Software Craftsmanship that are becoming religious and dogmatic, and that as a result the movement as a whole is starting to lose some of its pragmatism. 

What I have thought for a long time now is that Software Craftsmanship is really about more than just the technical practices. The [manifesto](http://manifesto.softwarecraftsmanship.org) talks about ""a community of professionals"" and ""productive partnerships"", for example, as well as ""well-crafted software"". The [further reading page](http://manifesto.softwarecraftsmanship.org/#/en/reading) on the Software Craftsmanship website includes links to blog posts, including one asking ""Is Craftsmanship all about code?""

Unfortunately, it seems to me that the North American school of Software Craftsmanship *is* all about code. Corey Haines brought the world the Code Retreat, and JB Rainsberger the Legacy Code Retreat. Uncle Bob travels the world talking about Clean Code, TDD, the SOLID principles, and more. I'm enormously grateful to these people for providing such important and valuable formats for learning, and for sharing their knowledge with the world to better the technical state of the industry. When I look at the North American Software Craftsmanship movement, however, it feels to me like there's something missing.

Practices come and go, and those that tie themselves to their practices risk irrelevance. I firmly believe that Craftsmanship practices (or XP practices), such as pair programming and Test-Driven Development, have inherent value to the extent that *everyone should learn them*. Learning them is different from using them all the time, however: there situations where TDD is a difficult practice to apply, such as when working with a large body of legacy code. While you can use TDD in some cases in this context (e.g. when adding new classes and functionality), in the main you have to adopt a test-last approach for the simple reason that **the code has already been written, the design has already been decided**. Other techniques, such as the Golden Master and incremental refactoring, will keep you more productive than TDD in this context.

The North American school's focus on technical practices opens us up to criticism along the lines that we have seen: ""you're TDD Nazis"", ""you're too focussed on code"", and ""you're elitist"". This last one stings the most for me: yes, we are actively trying to improve our craft, and yes we want to improve the industry, but we want to do that in as an inclusive way as possible. It's not a case of converting people to the Software Craftsmanship religion, it's a case of inspiring people of all abilities to seek to better themselves through our ethos of mentoring and peer learning. 

What I see in the European school of Software Craftsmanship is a superset of the North American school: not just technical practices, but discussion, collaboration, learning and mentoring. It feels more fully-formed to me as a result, more holistic, more inclusive and welcoming. Perhaps it's a difference between European and North American cultures being expressed in the equivalent parts of our community, or maybe it's that the European school is an evolution of the North American school; I don't know. What I do know is that, while I see value in the technical focus of the North American school, the European school is the idea of Software Craftsmanship I identify with the most, and the one I will continue to promote. 

---

<a name=""footnote1"">*</a> It might be unfair to generalise the North American movement as such. It's entirely possible it would be more accurate for my comments on the ""North American school"" to instead be addressed to the ""Chicago school"". I would be interested in hearing from someone with more knowledge of the movement that side of the Atlantic. [Return to article](#footnote1-ref)"
"Software Craftsmanship, and Professionalism in Software",,"Another thing to come out of my reflections on Software Craftsmanship at SoCraTes UK this weekend was a clearer view on what professionalism means to me for software craftsmen. I believe there are three pillars of professionalism that need to be considered:

* A mindset of taking a methodical, deliberate, and considered approach to your work;
* Leaning on your tools, but not being dependent upon them; and
* Taking personal responsibility for your decisions and actions

<!--break-->

## The First Pillar: Mindset

The first pillar, that of the approach one takes to their work, is what allows us to deliver quality software. It is also, perhaps, one of the things we would like to be judged on when sharing our work with others. It is about being careful (not, in this context, in the ""cautious"" sense of the word) in our implementations to ensure that what we deliver meets our standards. Let's delve into each of the words I used a bit further.

### Methodical

A methodical approach is one that is systematic and orderly. For example, the Test-Driven Development process takes the form of three steps that are repeated in a loop: red, green, refactor; red, green, refactor; red, green, refactor; etc. This process is both systematic - it encompasses a system that is defined as ""implement software incrementally, by writing your tests first, making the tests pass, and making incremental improvements to the tests and implementation"" - and ordered, in that the steps have to be completed in the order listed: you can't refactor code when you have failing tests, and if you end up with failing tests after making another test pass (or during a refactoring), you have broken something.

It goes further than this, however: for example, seeing the test fail *for the right reason*. That is, if your test passes because your code already covers the case you're now testing, you go back and change your implementation code to make that new test fail. If you see the test fail because of a compilation error, or because of an exception thrown, you go back and make sure the assertion itself fails when the behaviour it is testing fails.

### Deliberate

A deliberate approach is one where each of the steps are there for a reason, they have a specific intent. Taking again the Test-Driven Development process of red-green-refactor as our example, we can see that each of the steps in the process have a purpose, and in the order described. By writing our test first, we commit to a small unit of behaviour that we specify with the test. In the green phase, we implement the behaviour specified. Finally, in the refactor stage, we incrementally improve the design of our code to remove duplication and other smells. 

Again, this also ties back to seeing the test fail for the right reason. Each of the three steps are there for a purpose, so we don't skip over any of them even when it looks like we've met the high-level goal of making bar go green or red; we dig into it further to ensure that we've achieved that goal for the right reason, that the purpose of the current step has been satisfied.

### Considered

A considered approach is one that you have thought about, one that you can defend for your own reasons when challenged on it. You have made the decisions you have, and taken the approach that you have, as the result of thinking about what it is you are doing; not simply following orders, or naively following a process, or being a passive participant in your team's activities.

When I first picked up TDD, I did so without contemplating in much detail the approach it defines. As a result, when I came to introduce the concept to my team at work, I had some tough questions to answer from them about the value of the approach and the detail of the process that I wasn't able to satisfactorily address in some cases. 

### The Mindset: Mindfulness

Mindfulness is perhaps most easily understood as the very opposite of mindlessness, a word many more people are familiar with: we can all picture a person working as an automaton, completing a task they've done hundreds or thousands of times before, doing it without thinking, without engaging with the task at all. We've all had the experience of making a journey somewhere, perhaps on foot, by bike, or in a car, and arriving at our destination without much recollection of the journey there. Mindfulness is about engaging with the task you're undertaking, being an active participant. 

Taking a methodical, deliberate and considered approach to your work instills a mindset of mindfulness in your work, which I believe is important to mastering your craft. Being mindful of your work provides you with greater opportunities for learning from your mistakes, the situations you encounter at work, and the context around you. It means challenging your practices and principles, and accepting that they will be challenged by others. It means thinking about what you are doing and justifying your approach to yourself.

You might be reading this and thinking that what I'm suggesting sounds like perpetual self-doubt; this is not the case. Self-doubt comes at this process from a negative position, where you're questioning your motives and decisions for the simple reason that you made them. Mindfulness comes from a much more positive position, that of curiosity: you question your approach with the intention of learning from the decision, you're curious as to whether there is a better approach that can be taken, a more efficient practice that can be learned, etc.

## The Second Pillar: Tools

Every craftsman has his or her tools: the carpenter has saws, hammers, lathes and more; the surgeon has a scalpel, an endoscope, and sutures; the software developer has their editor or IDE, refactoring tools, test runners, version control system, and more. Tools are an important part of any craftsman's job, and knowing your tools well can be the difference between doing a good job and bad job, producing quality product over poor product, creating more product over less product, etc.

A craftsman should not be dependent upon their tools, however. Being prepared to put a tool to one side to try a new one out is just part of this: how will you know if something better is available if you never try it? For example, a large number of C# developers have a copy of ReSharper at work, and many have their own copy at home as well. This tool came about in the first place because of poor support within Visual Studio for basic refactoring operations like rename and extract method. However, there is a manual approach, described by Martin Fowler in his [original text on refactoring](http://www.amazon.co.uk/Refactoring-Improving-Design-Existing-Technology/dp/0201485672/): add-update-remove. It is a methodical, deliberate, and considered process that allows you to complete a refactoring manually without seeing a compilation error; for example, if adding a parameter to a method:

1. Add a new overload of the method that includes the new parameter, and make it call the existing overload.
2. Update all usages of the existing overload of the method to point to the new one, supplying the new argument as appropriate to that context.
3. Remove the old overload that is now unused everywhere, migrating the implementation to the new overload in the process.
4. Make the changes needed to the implementation to use the new parameter.

If you're using a modern version control system like Git, then you can commit after each of these stages as well, so you get a set of handy staging posts you can roll back to if necessary.

Here are some of the tools I consider essential for any software craftsman, no matter what language they are working in:

* **The best-available editor for your language** (defined on your own terms). Perhaps it's Visual Studio for C# developers, IntelliJ IDEA for Java Developers, vim/emacs for Ruby/Python/JavaScript developers; perhaps you prefer SharpDevelop, Eclipse, or PyCharm.
* **A plain-text editor.** Often we need to make a small change to a file, or write a commit message, or edit a supporting file. Firing up a complete IDE is overkill for this kind of scenario, so it is worth keeping around a plain-text editor for these situations, and one a little less basic than Notepad at that. I've tried a few over the last 10 or so years, and have recently come to settle on Sublime Text 2, and vim.
* **A scripting environment.** Linux users have had Bash and other shells available since the beginning of time (1 Jan 1970, as we all know). Windows now has PowerShell, and if you're a developer working in Windows regularly, you owe it to yourself to learn PowerShell well. Scripting environments put the power of automation at your finger-tips, which can be a huge time-saver. 
* **An automated refactoring tool** (if available). Some IDEs have these built in, particularly JetBrains' offerings. Visual Studio has a number of ""productivity extensions"" including ReSharper, CodeRush and JustCode that bring the full weight of this functionality to bear over VS' limited offering. 
* **A Distributed Version Control System**, such as Git or Mercurial. Having the ability to commit and roll-back changes locally before going anywhere near a central repository is an incredible advantage: with a bit of discipline in committing, you can mess up your working copy as much as you like and always have a recent checkpoint to revert to. Git is my weapon of choice in this arena.
* **An automated test runner, ideally with a continuous mode.** I started using [NCrunch](http://www.ncrunch.net/), a continuous test runner for Visual Studio, about 18 months ago. On projects with proper unit tests, it provides a fantastic productivity boost just having the tests running all the time in the background on each change. JavaScript developers have karma (amongst other options, I'm sure), and there are similar things available for other languages too. At work, I use ReSharper's test runner because it integrates very nicely with my IDE and our tests at work don't play nicely with NCrunch. 

Each of these tools are integral to my development process, and serve a very specific function, in exactly the same way that a lathe, a saw and sandpaper are to a carpenter's wood-working process.

## The Third Pillar: Personal Responsibility

Erik Talboom's Lightning Talk on Personal Responsibility at SoCraTes UK evidently struck a chord with everyone in the room: it was a recurring theme in sessions and conversations throughout the weekend.

Erik explained well Christopher Avery's [Responsibility Process](http://www.christopheravery.com/responsibility-process) in terms of finding and fixing a bug:

* **Denial.** First you cannot believe there is a bug. ""There's no way that could happen!"", you tell yourself.
* **Blame.** Next you start looking around for someone to blame for the bug. ""This is Joey's code, it's his fault!"", you tell yourself.
* **Justification.** Third, you look for excuses for the bug being there. ""Well, if Joey hadn't used null values in his code, there would be no bug!"", you tell yourself.
* **Shame.** Next, you start to lay blame yourself. ""I'm such an idiot. I saw Joey working on this just last week and I didn't say anything."", you tell yourself.
* **Obligation.** Now the context and environment are at fault. ""There's no way we could have done anything different, we're under such pressure to deliver on time that quality goes out the window."", you tell yourself.
* **Responsibility.** You realise there's something you can do about it. ""I'll have a chat with Joey to explain why nulls should be avoided, and start trialling code reviews as part of our development process.""

The alternative to responsibility, of course, is abdicating it: quitting the situation to avoid the pain associated with shame and obligation.

A key part of taking personal responsibility is mindfulness: being aware of when you are in the mental states of denial, blame, justification, etc. This awareness can help you move through the remaining stages more quickly (or skip them entirely) to reach a productive conclusion earlier.

## Conclusion
Mindfulness and Personal Responsibility are the two key ""psychological"" components of professionalism to me; the second pillar, Tools, provides a way of expressing those through our practices. Any amateur can use a professional's tools, but it is the approach we take to our work that indicates professionalism and denotes craftsmanship. "
Introducing Bob,"Test Data Builders are awesome and you should use them to tidy up your test code (read about them in GOOS Chapter 22). I'm introducing a new library called [Bob](http://github.com/alastairs/BobTheBuilder) which replaces the need to write your own hand-rolled Test Data Builders with a generic solution that preserves the fluent syntax suggested by GOOS. 
","## TL;DR

Test Data Builders are awesome and you should use them to tidy up your test code (read about them in GOOS Chapter 22). I'm introducing a new library called [Bob](http://github.com/alastairs/BobTheBuilder) which replaces the need to write your own hand-rolled Test Data Builders with a generic solution that preserves the fluent syntax suggested by GOOS. 

## Test Data Builders

One of the most influential books on software development practice in recent years is *Growing Object Oriented Software, Guided by Tests*, or GOOS for short. It describes an approach to application development based on Test-Driven Development, but demonstrates how to effectively use Mocks, Stubs and Fakes to drive your application's design from the outside and model it around the communications between collaborators, rather than stored state. 

A key recommendation GOOS makes for keeping your test code clean is to use a technique called Test Data Builders (Chapter 22, p257). This approach leans on the Builder pattern from the Gang of Four (GoF) to abstract away the construction of objects on which your tests depend, but do not necessarily care about. For example, say you are developing an online shop, which models customers and orders. You might write your tests like this:

    [Fact]
    public void Placing_An_Order_Adds_The_Order_To_The_Customers_Account()
    {
        // Arrange
        var customer = new Customer(1, // Id
                                    ""Joe"", ""Bloggs"", // Name
                                    ""10 City Road"", ""Staines"", ""Middlesex"", ""AB1 2CD"" // Address
                                    );
        var order = new Order(1, customer);

        // Act, Assert: not interesting for this example
    }

After a while and a couple of tests, you realise you've got some duplicated code that you could factor out, so you introduce a couple of factory methods. Maybe you even include default parameter values to allow you to reuse the same factory method:

    [Fact]
    public void Placing_An_Order_Adds_The_Order_To_The_Customers_Account()
    {
        // Arrange
        var customer = CreateCustomer();
        var order = new Order(1, customer);

        // ...
    }

    private Customer CreateCustomer(int id = 1,
                                    string givenName = ""Joe"",
                                    string familyName = ""Bloggs"",
                                    string addressLine1 = ""10 City Road"",
                                    string addressLine2 = ""Staines"",
                                    string county = ""Middlesex"",
                                    string postCode = ""AB1 2CD"") 
    {
        return new Customer(id, givenName, familyName, addressLine1, AddressLine2, county, postCode);
    }

    private Order CreateOrder(Customer customer, int id = 1)
    {
        return new Order(id, customer);
    }

But time goes on, and you find this approach isn't really working for you either. Perhaps you have somehow ended up with three versions of `CreateCustomer()` that take different dependencies, or some abstraction is leaking all over your tests in spite of your best efforts. This is where the Test Data Builder pattern comes in. 

The Test Data Builder pattern is really just an implementation of the Builder pattern from GoF. This is a creational pattern, like the more common Factory Method and Abstract Factory patterns, and while it is more complicated than either factory pattern, it provides more flexibility too. It achieves this by separating the construction of the object from the object's representation. As defined in GoF, it is a fairly complex pattern, but GOOS simplifies it somewhat. 

We start by defining a Builder class for the type we need to construct, which defines a Build() method returning the type we need:

    internal class CustomerBuilder
    {
        public Customer Build()
        {
            return new Customer(1, // Id
                                ""Joe"", ""Bloggs"", // Name
                                ""10 City Road"", ""Staines"", ""Middlesex"", ""AB1 2CD"" // Address
                                    );
        }
    }

So far, so uninteresting. Next we start adding methods to define how we want the built `Customer` to look:

    internal class CustomerBuilder
    {
        private string givenName;
        private string familyName;

        public CustomerBuilder WithGivenName(string newGivenName)
        {
            givenName = newGivenName;
            return this;
        }

        public CustomerBuilder WithFamilyName(string newFamilyName)
        {
            familyName = newFamilyName;
            return this;
        }

        public Customer Build()
        {
            return new Customer(1, // Id
                                givenName, familyName
                                ""10 City Road"", ""Staines"", ""Middlesex"", ""AB1 2CD"" // Address
                                );

        }
    }

There are two things to notice from the above code sample. First, we get to remove those pesky comments because the code now better reveals its intent: it is self-documenting, which is the dream. 

The second thing to notice is the builder methods return the current instance of `CustomerBuilder`. This allows us to chain calls to the builder methods together to form a nice fluent interface:

    [Fact]
    public void Placing_An_Order_Adds_The_Order_To_The_Customers_Account()
    {
        // Arrange
        var customer = new CustomerBuilder()
                               .WithGivenName(""Joe"")
                               .WithFamilyName(""Bloggs"")
                               .Build();
        var order = new Order(1, customer);

        // ...
    }

Much nicer! This can be combined with a Factory Method in your test fixture to create the `CustomerBuilder`, which can make the code fully fluent:

    [Fact]
    public void Placing_An_Order_Adds_The_Order_To_The_Customers_Account()
    {
        // Arrange
        var customer = ACustomer()
                           .WithGivenName(""Joe"")
                           .WithFamilyName(""Bloggs"")
                           .Build();
        // ...
    }

    private CustomerBuilder ACustomer()
    {
        return new CustomerBuilder();
    }


Furthermore, you can add methods to your Builder along the lines of `WithNoFamilyName()` to make explicit situations where that part of the object should not be set. For example an Address object will have optional information such as `AddressLine2`, or you may wish to test what happens when no post code is provided as part of the address.

[Mark Seemann](http://blog.ploeh.dk/) proposed a couple of extensions to the original GOOS Test Data Builder, which are quite nice. The first is to use the constructor of the Builder to define any default values that must be provided to the object being built. The second is to define an implicit cast from the Builder to the built type, to eliminate the noise of explicitly calling `Build()` all over the place:

    internal class CustomerBuilder
    {
        // ...

        public static implicit operator Customer(CustomerBuilder builder)
        {
            return builder.Build();
        }
    }

## Introducing Bob

I've been writing Test Data Builders for a good number of months now, and as useful as the pattern is I find myself feeling frustrated at the amount of boilerplate code it demands: the builder methods in particular are quite annoying as they are so similar. I started off by trying to reduce the amount of typing I had to do by using [ReSharper](http://www.jetbrains.com/resharper) templates to stub out the different facets of the pattern such as the builder class, the builder methods, etc., but then I moved jobs and lost them all. I haven't yet got around to reproducing the templates in my new dev environment. 

Then a couple of days ago I had an idea. [Simple.Data](http://github.com/markrendle/Simple.Data) uses the Dynamic Language Runtime and the dynamic dispatch features of C# to offer methods  that represent columns in your database tables (e.g. `pets.FindById(256);`, `pets.FindByType(""Dog"");`, etc.). Perhaps the Test Data Builder pattern implementation can be generalised using the techniques? 

I spent today spiking a new library to do this, [Bob](http://github.com/alastairs/BobTheBuilder). You use it like this:

    [Fact]
    public void Placing_An_Order_Adds_The_Order_To_The_Customers_Account()
    {
        // Arrange
        var customer = A.BuilderFor<Customer>()
                           .WithGivenName(""Joe"")
                           .WithFamilyName(""Bloggs"")
                           .Build();
        // ...
    }

It also implements Mark's second extension to the pattern, whereby you can implicitly cast from the Builder returned by `BuilderFor<T>()` to `T` and have it invoke the `Build()` method to complete the conversion.

It is, admittedly pretty limited in functionality at the moment because it's a proof of concept. It will only build instances of types that have a default (i.e. parameterless) constructor and that provide public setters on properties for the things that are to be set. I will build out the functionality as I need it, and [I have published a Task list](https://github.com/alastairs/BobTheBuilder/#task-list) of immediately-forthcoming functionality. 

Check it out and let me know what you think!"
"Updates to Bob, v0.2 released!","Recently [I introduced a new library](http://codebork.com/2014/03/23/introducing-bob.html) to aid testing in C#, [Bob](https://github.com/alastairs/bobthebuilder). I've made a couple of updates to it recently, one small, and one a bit larger. The latest release tackles a few robustness issues and sees it move out of alpha phase and up to version 0.2!

[Updated: 2014-04-12 16:00 GMT]","Recently [I introduced a new library](http://codebork.com/2014/03/23/introducing-bob.html) to aid testing in C#, [Bob](https://github.com/alastairs/bobthebuilder). I've made a couple of updates to it recently, one small, and one a bit larger. The latest release tackles a few robustness issues and sees it move out of alpha phase and up to version 0.2!

# Support for Complex Types

The first new feature is support for complex types with parameterless constructors. You can now say `A.BuilderFor<Customer>().WithAddress(new Address())`, for example. As it turned out, Bob already supported this as, of course, types with parameterless constructors are easily constructed by the reflection API. There are now tests around this use case, however, so you can be sure it will continue to work!

# Support for Named Arguments syntax

The second new feature is a little larger: supporting a different syntax taking advantage of C# 4's support for named arguments. You can now say `A.BuilderFor<Customer>().With(customerId: 43)`, for example, and you can supply multiple properties to the same call, such as 

<pre>
A.BuilderFor&lt;Customer&gt;()
    .With(customerId: 43,
          givenName: ""John"",
          familyName: ""Doe""
    ).Build();
</pre>

This might be useful if you have a set of properties that always travel together (and so might help you identify where a new class could be introduced). 

As before, you can skip the call to `Build()` if the compiler can work out that you're after an instance of the built type.

# Robustness

I've done a bit of redesign which threw up some places where the library wouldn't work quite as intended (e.g., see what happens if you call a builder method that doesn't start with the word 'With'). I now have a design I'm happier with, and have ironed out these peculiarities of functionality, and now feel that it should be more solid than previously. As such, I'm very happy to announce that I've removed the ""alpha"" tag from the package version and have bumped it to v0.2!

# Get Bob v0.2

All of these changes are available from NuGet in the 0.2 version of the package, just `install-package BobTheBuilder`!

Happy building!"
Desire Lines in software architecture: what can we learn from landscape architecture?,,"Before Christmas I was talking with <a href=""https://twitter.com/theagilepirate"">Simon</a> about an architectural approach we’d taken on a recent project. The aim of the project is to replace an existing WinForms user interface with a shiny new HTML and JavaScript version. Part of this involves making HTTP requests back to the “engine” of the product, a .NET application, and of course our chosen data format is JSON. To protect the HTML UI from changes made in the Engine, we decided to keep a separation between the models we transferred over HTTP (what we termed Data Models), and the models we used in the application (or, “Application Models”). The approach that we took had similarities with the concept of Desire Lines from landscape architecture, as described in <a href=""http://www.amazon.co.uk/Practices-Scaling-Lean-Agile-Development/dp/0321636406""><em>Practices for Scaling Lean and Agile Development</em> by Larman and Vodde</a>. 
<!--break-->
A desire line (or desire path) is a path that evolves over time, as a result of people walking or cycling from where they are to where they want to be, and usually represents the shortest (or easiest) route between those points. These desire paths provide a key indication of the routes people take to reach a destination, and many desire paths become fully managed “official” paths, upgraded to gravel or tarmac. A good example from Cambridge is the green open space called Parker’s Piece<a href=""#footnote-asterisk""><sup>*</sup></a>. Bounded on all sides by roads, it has two main paths, running roughly North-South and East-West<a href=""#footnote-dagger""><sup>&dagger;</sup></a>:
 
<a href=""http://codebork.com/content/parkers-piece-sketchmap""><img src=""http://codebork.com/sites/default/files/Parkers%20Piece.png"" alt=""Parkers Piece"" width=""690"" /></a>

Even the briefest of glances at that deliberately rough sketch map provides a clear idea of where the points of interest lie outside of Parker’s Piece. In the top corners, we have obstacles – to the left, the public toilets; to the right, the University Arms hotel – and in the bottom left corner, we have objects of desire: the gym and swimming pool, a skate park, and a pedestrian crossing. I find it interesting that not only do desire paths represent the shortest or easiest path to an object of desire, but that they also represent the shortest path around an obstacle. 

<strong>Ok, ok, but how does this apply to software architecture?</strong> 

Well, the overall concept of desire lines has similarities with both the “outside-in” development approach described by books such as <a href=""http://www.growing-object-oriented-software.com/""><em>Growing Object-Oriented Software, Guided by Tests</em></a> and the Lean and Agile principles of a <a href=""http://theleanstartup.com/principles"">minimum viable product (MVP)</a> and incremental development. The MVP can be considered the diagonal paths in the sketch above, the basic routes from one side of the grass to the other; the desire lines represent the incremental development, added as and when they become needed – for example, I gather the skate park didn’t exist in the 16th century when Parker’s Piece was first established!

A more concrete example is in communication between disparate systems. Remember in the opening paragraph, I mentioned the HTML UI communicates with the product’s engine via JSON? In this sort of scenario, you can start with a minimum viable data contract and add to it as more of the application is built. Or to put it another way, working from the outside in, you let the consumer define the data format and, as you build this consumer, so you build the data format (this applies to behaviour as much as data, of course, but that’s another blog post.) Taking the ubiquitous to-do list app example, you might build up your data contract as follows:

<a href=""http://codebork.com/content/mvp-phases""><img src=""http://codebork.com/sites/default/files/MVP%20phases.png"" width=""690"" /></a>

An alternative approach, which we took on this project for reasons of practicality, is to provide a very loose data contract and then refine it as you build. This roughly translates to throwing everything and the kitchen sink over the wire for every request, and then refactoring the data contracts as you build the client. We took this approach in our project because the HTTP endpoints were developed independently of the user interface up front, and that team didn’t know at the time what the UI would need. They developed a proof-of-concept UI along the way which consumed this data, and the production-quality UI is refactoring those data contracts along the way. In honesty, I wouldn’t recommend taking this approach unless your hands are tied, as it’s analogous to refactoring a God Object. To stretch the desire lines metaphor to the point of breaking, it’s a bit like covering the proposed green space with gravel and grass seed, and letting the grass grow through the gravel where people don’t walk, then scooping up the gravel in those areas and properly turfing them. Basically, a massive faff, and one that is likely to produce less well-defined results than the true desire lines approach. 

I’ve had a few musings around the MVP concept over the last few weeks, and this latest addition was sufficiently interesting to share. Certainly I will be keeping it in mind in the coming weeks and months as we start on new development work.

<span id=""footnote-asterisk"" style=""font-size: x-small""><sup>*</sup>Factoid: Parker’s Piece is considered <a href=""http://www.bbc.co.uk/cambridgeshire/content/articles/2006/06/09/cambridge_football_rules_parkers_piece_feature.shtml"">the birthplace of the rules of Association Football</a>!</span><br />
<span id=""footnote-dagger"" style=""font-size: x-small""><sup>&dagger;</sup>The elaborate lamppost at the crossing in the middle is known as ""the Reality Checkpoint""</span>
"
